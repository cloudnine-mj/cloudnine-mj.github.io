---
key: jekyll-text-theme
title: 'Airflowì—ì„œ ì™¸ë¶€ ì´ë²¤íŠ¸ ê°ì§€í•˜ê¸° - External Sensor'
excerpt: ' Airflowì—ì„œ External Sensorë¡œ ì™¸ë¶€ ì´ë²¤íŠ¸ ê°ì§€í•˜ê¸° ğŸ˜'
tags: [Airflow]
---

## ê°œë…

* SensorëŠ” íŠ¹ì • ì¡°ê±´ì´ ë§Œì¡±ë  ë•Œê¹Œì§€ ëŒ€ê¸°í•˜ëŠ” íŠ¹ìˆ˜í•œ Operator
* External SensorëŠ” ë‹¤ë¥¸ DAGì˜ Task ì™„ë£Œ, íŒŒì¼ ìƒì„±, API ì‘ë‹µ ë“± ì™¸ë¶€ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì¡°ìœ¨í•  ìˆ˜ ìˆìŒ.

## ì„¤ì¹˜


```bash
# SensorëŠ” Airflow ê¸°ë³¸ íŒ¨í‚¤ì§€ì— í¬í•¨
# ì¶”ê°€ Provider íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìš”ì‹œ)
pip install apache-airflow-providers-amazon  # S3 Sensor
pip install apache-airflow-providers-http    # HTTP Sensor
pip install apache-airflow-providers-google  # GCS Sensor
```

## ì›ë¦¬

1. SensorëŠ” **poke ëª¨ë“œ** ë˜ëŠ” **reschedule ëª¨ë“œ**ë¡œ ë™ì‘í•¨.
2. **Poke ëª¨ë“œ**: Worker ìŠ¬ë¡¯ì„ ì ìœ í•œ ì±„ë¡œ ì£¼ê¸°ì ìœ¼ë¡œ ì¡°ê±´ì„ í™•ì¸í•¨.
3. **Reschedule ëª¨ë“œ**: ì¡°ê±´ í™•ì¸ í›„ ìŠ¬ë¡¯ì„ í•´ì œí•˜ê³ , ë‹¤ìŒ í™•ì¸ ì‹œê°„ì— ë‹¤ì‹œ ìŠ¤ì¼€ì¤„ë¨.
4. ì¡°ê±´ì´ ë§Œì¡±ë˜ë©´ ë‹¤ìŒ Taskë¡œ ì§„í–‰
5. **Timeout** ì‹œê°„ ë‚´ì— ì¡°ê±´ì´ ë§Œì¡±ë˜ì§€ ì•Šìœ¼ë©´ ì‹¤íŒ¨í•¨.

### ì½”ë“œ

* ì‹¤ë¬´ì—ì„œ í™œìš©í–ˆë˜ ì½”ë“œ

### 1. ExternalTaskSensor - ë‹¤ë¥¸ DAG ì™„ë£Œ ëŒ€ê¸°


```python
from airflow import DAG
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# ì—…ìŠ¤íŠ¸ë¦¼ DAG (ë°ì´í„° ìˆ˜ì§‘)
upstream_dag = DAG(
    'data_collection',
    start_date=datetime(2024, 1, 1),
    schedule_interval='0 1 * * *',  # ë§¤ì¼ 01:00
    catchup=False,
)

collect_data = PythonOperator(
    task_id='collect_data',
    python_callable=lambda: print("Data collected"),
    dag=upstream_dag,
)

# ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ DAG (ë°ì´í„° ì²˜ë¦¬)
downstream_dag = DAG(
    'data_processing',
    start_date=datetime(2024, 1, 1),
    schedule_interval='0 2 * * *',  # ë§¤ì¼ 02:00
    catchup=False,
)

# ì—…ìŠ¤íŠ¸ë¦¼ DAGì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼
wait_for_collection = ExternalTaskSensor(
    task_id='wait_for_collection',
    external_dag_id='data_collection',
    external_task_id='collect_data',
    # ì‹¤í–‰ ì‹œê°„ ì°¨ì´ ì„¤ì • (1ì‹œê°„ ì „ ì‹¤í–‰ëœ DAGë¥¼ ê¸°ë‹¤ë¦¼)
    execution_delta=timedelta(hours=1),
    # Reschedule ëª¨ë“œ ì‚¬ìš© (Worker ìŠ¬ë¡¯ ì ˆì•½)
    mode='reschedule',
    # 30ë¶„ë§ˆë‹¤ ì²´í¬
    poke_interval=60 * 30,
    # ìµœëŒ€ 2ì‹œê°„ ëŒ€ê¸°
    timeout=60 * 60 * 2,
    dag=downstream_dag,
)

process_data = PythonOperator(
    task_id='process_data',
    python_callable=lambda: print("Data processed"),
    dag=downstream_dag,
)

wait_for_collection >> process_data
```

### 2. S3KeySensor - S3 íŒŒì¼ ìƒì„± ëŒ€ê¸°


```python
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.amazon.aws.operators.s3 import S3CopyObjectOperator
from datetime import datetime

dag = DAG(
    's3_file_processing',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@hourly',
    catchup=False,
)

# S3ì— íŒŒì¼ì´ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
wait_for_file = S3KeySensor(
    task_id='wait_for_file',
    bucket_name='my-data-bucket',
    bucket_key='incoming/data_{{ ds }}.csv',  # í…œí”Œë¦¿ ì‚¬ìš©
    aws_conn_id='aws_default',
    # Wildcard ì‚¬ìš©
    wildcard_match=True,  # data_*.csv ë§¤ì¹­
    # 5ë¶„ë§ˆë‹¤ ì²´í¬
    poke_interval=300,
    # ìµœëŒ€ 1ì‹œê°„ ëŒ€ê¸°
    timeout=3600,
    mode='reschedule',
    dag=dag,
)

# íŒŒì¼ ì²˜ë¦¬
process_file = S3CopyObjectOperator(
    task_id='process_file',
    source_bucket_name='my-data-bucket',
    source_bucket_key='incoming/data_{{ ds }}.csv',
    dest_bucket_name='my-data-bucket',
    dest_bucket_key='processed/data_{{ ds }}.csv',
    aws_conn_id='aws_default',
    dag=dag,
)

wait_for_file >> process_file
```

### 3. HttpSensor - API ì‘ë‹µ ëŒ€ê¸°

```python
from airflow import DAG
from airflow.providers.http.sensors.http import HttpSensor
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'api_dependent_pipeline',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
)

# APIê°€ ì •ìƒ ì‘ë‹µí•  ë•Œê¹Œì§€ ëŒ€ê¸°
wait_for_api = HttpSensor(
    task_id='wait_for_api',
    http_conn_id='external_api',
    endpoint='/health',
    # ì‘ë‹µ ê²€ì¦ í•¨ìˆ˜
    response_check=lambda response: response.json()['status'] == 'healthy',
    poke_interval=60,  # 1ë¶„ë§ˆë‹¤ ì²´í¬
    timeout=1800,  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
    mode='reschedule',
    dag=dag,
)

# íŠ¹ì • ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ëŒ€ê¸°
wait_for_data_ready = HttpSensor(
    task_id='wait_for_data_ready',
    http_conn_id='external_api',
    endpoint='/data/status/{{ ds }}',
    response_check=lambda response: response.json()['is_ready'] == True,
    poke_interval=300,
    timeout=3600,
    mode='reschedule',
    dag=dag,
)

def fetch_and_process_data(**context):
    """APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬"""
    import requests
    response = requests.get('http://external-api/data/{{ ds }}')
    data = response.json()
    # ë°ì´í„° ì²˜ë¦¬ ë¡œì§
    print(f"Processed {len(data)} records")

process_data = PythonOperator(
    task_id='process_data',
    python_callable=fetch_and_process_data,
    dag=dag,
)

wait_for_api >> wait_for_data_ready >> process_data
```

### 4. Custom Sensor êµ¬í˜„

```python
from airflow.sensors.base import BaseSensorOperator
from airflow.utils.decorators import apply_defaults
import requests

class CustomDatabaseSensor(BaseSensorOperator):
    """ë°ì´í„°ë² ì´ìŠ¤ íŠ¹ì • ë ˆì½”ë“œ ì¡´ì¬ ì—¬ë¶€ ì²´í¬"""
    
    @apply_defaults
    def __init__(
        self,
        database_conn_id,
        table_name,
        condition,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.database_conn_id = database_conn_id
        self.table_name = table_name
        self.condition = condition
    
    def poke(self, context):
        """ì¡°ê±´ í™•ì¸ ë¡œì§"""
        from airflow.hooks.postgres_hook import PostgresHook
        
        hook = PostgresHook(postgres_conn_id=self.database_conn_id)
        
        sql = f"""
            SELECT COUNT(*) 
            FROM {self.table_name} 
            WHERE {self.condition}
        """
        
        result = hook.get_first(sql)
        count = result[0] if result else 0
        
        self.log.info(f"Found {count} records matching condition")
        
        return count > 0

# ì‚¬ìš© ì˜ˆì œ
dag = DAG(
    'custom_sensor_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@hourly',
    catchup=False,
)

wait_for_records = CustomDatabaseSensor(
    task_id='wait_for_records',
    database_conn_id='postgres_default',
    table_name='events',
    condition="event_date = '{{ ds }}' AND processed = false",
    poke_interval=300,
    timeout=3600,
    mode='reschedule',
    dag=dag,
)
```

## ì ìš©í•  ë•Œ ê³ ë ¤í–ˆë˜ ì 

1. **Reschedule ëª¨ë“œ ì‚¬ìš©**: ì¥ì‹œê°„ ëŒ€ê¸°ê°€ í•„ìš”í•œ ê²½ìš° reschedule ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ Worker ìŠ¬ë¡¯ì„ ì ˆì•½í•¨.
2. **ì ì ˆí•œ Timeout ì„¤ì •**: ë¬´í•œ ëŒ€ê¸°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ í•©ë¦¬ì ì¸ timeoutì„ ì„¤ì •í•¨.
3. **Poke Interval ì¡°ì •**: ì²´í¬ ë¹ˆë„ë¥¼ ì ì ˆíˆ ì„¤ì •í•˜ì—¬ ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì„ ì¤„ì„.
4. **ì‹¤íŒ¨ ì²˜ë¦¬**: Sensorê°€ íƒ€ì„ì•„ì›ƒë˜ì—ˆì„ ë•Œì˜ í›„ì† ì¡°ì¹˜(ì•Œë¦¼, ì¬ì‹œë„ ë“±)ë¥¼ ê³„íší•¨.
5. **ì˜ì¡´ì„± ì²´ì¸ ìµœì†Œí™”**: Sensorê°€ ë„ˆë¬´ ë§ìœ¼ë©´ íŒŒì´í”„ë¼ì¸ì´ ë³µì¡í•´ì§€ë¯€ë¡œ, ê¼­ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•¨.