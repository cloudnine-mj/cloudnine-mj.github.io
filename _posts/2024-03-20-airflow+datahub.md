---
key: jekyll-text-theme
title: 'Airflow + DataHub'
excerpt: ' Airflow + DataHubë¡œ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬ ìë™í™” ğŸ˜'
tags: [Airflow, Datahub]
---

# Airflow + DataHub

## ê°œë…

* DataHubëŠ” LinkedInì—ì„œ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë©”íƒ€ë°ì´í„° í”Œë«í¼
* Airflowì™€ ì—°ë™í•˜ì—¬ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ Lineage(ê³„ë³´), ìŠ¤í‚¤ë§ˆ, ì†Œìœ ì ì •ë³´ ë“±ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŒ.

## Datahub ì„¤ì¹˜

```bash
# DataHub CLI ì„¤ì¹˜
pip install acryl-datahub

# DataHub ì„œë²„ ì„¤ì¹˜ (Docker Compose)
python3 -m datahub docker quickstart

# Airflowì— DataHub í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜
pip install acryl-datahub[airflow]

# Airflow ì„¤ì •ì— DataHub ì—°ê²° ì¶”ê°€
# airflow.cfg ë˜ëŠ” í™˜ê²½ë³€ìˆ˜
export DATAHUB_GMS_URL=http://localhost:8080
export DATAHUB_GMS_TOKEN=your-access-token
```

## ì›ë¦¬

1. Airflow DAGì— **DataHub Lineage Backend**ë¥¼ ì„¤ì •í•¨.
2. Task ì‹¤í–‰ ì‹œ **Inlet(ì…ë ¥)ê³¼ Outlet(ì¶œë ¥)** ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ê¸°ë¡í•¨.
3. DataHubê°€ ì´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ **ë°ì´í„° ê³„ë³´ ê·¸ë˜í”„**ë¥¼ êµ¬ì„±í•¨.
4. **ìŠ¤í‚¤ë§ˆ ì •ë³´, ì†Œìœ ì, íƒœê·¸** ë“± ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•¨.
5. DataHub UIì—ì„œ **ê²€ìƒ‰, íƒìƒ‰, ì¶”ì **ì´ ê°€ëŠ¥í•¨.

## ì½”ë“œ

* ì‹¤ë¬´ì—ì„œ í™œìš©í–ˆë˜ ì½”ë“œ

### 1. Airflowì— DataHub Lineage Backend ì„¤ì •


```python
# airflow.cfg ìˆ˜ì •
[lineage]
backend = datahub_provider.lineage.datahub.DatahubLineageBackend

[datahub]
datahub_conn_id = datahub_rest_default
```

### 2. Connection ì„¤ì •

```python
# Airflow UIì—ì„œ Connection ì¶”ê°€
# Conn Id: datahub_rest_default
# Conn Type: HTTP
# Host: http://localhost:8080
# Extra: {"token": "your-access-token"}
```

### 3. Lineage ì •ë³´ê°€ í¬í•¨ëœ DAG


```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.lineage.entities import File, Table
from datetime import datetime

dag = DAG(
    'datahub_lineage_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
)

def extract_from_source(**context):
    """ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    # ì‹¤ì œ ì¶”ì¶œ ë¡œì§
    print("Extracting data from source")

# Inletê³¼ Outlet ì •ì˜
extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_from_source,
    # ì…ë ¥: PostgreSQL í…Œì´ë¸”
    inlets=[
        Table(
            database='production',
            cluster='postgres_prod',
            name='users',
        )
    ],
    # ì¶œë ¥: S3 íŒŒì¼
    outlets=[
        File(
            url='s3://my-bucket/raw/users/{{ ds }}.parquet'
        )
    ],
    dag=dag,
)

def transform_data(**context):
    """ë°ì´í„° ë³€í™˜"""
    print("Transforming data")

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    inlets=[
        File(url='s3://my-bucket/raw/users/{{ ds }}.parquet')
    ],
    outlets=[
        File(url='s3://my-bucket/processed/users/{{ ds }}.parquet')
    ],
    dag=dag,
)

def load_to_warehouse(**context):
    """ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì ì¬"""
    print("Loading to warehouse")

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_to_warehouse,
    inlets=[
        File(url='s3://my-bucket/processed/users/{{ ds }}.parquet')
    ],
    outlets=[
        Table(
            database='analytics',
            cluster='snowflake_prod',
            name='dim_users',
        )
    ],
    dag=dag,
)

extract_task >> transform_task >> load_task
```

### 4. DataHub APIë¡œ ë©”íƒ€ë°ì´í„° ì§ì ‘ ë“±ë¡


```python
from datahub.emitter.mce_builder import make_dataset_urn
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import (
    DatasetPropertiesClass,
    OwnerClass,
    OwnershipClass,
    OwnershipTypeClass,
)

def register_dataset_metadata(**context):
    """DataHubì— ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ë“±ë¡"""
    
    emitter = DatahubRestEmitter(
        gms_server='http://localhost:8080',
        token='your-access-token'
    )
    
    # ë°ì´í„°ì…‹ URN ìƒì„±
    dataset_urn = make_dataset_urn(
        platform='snowflake',
        name='analytics.dim_users',
        env='PROD'
    )
    
    # Properties ì„¤ì •
    properties = DatasetPropertiesClass(
        description='ì‚¬ìš©ì ì°¨ì› í…Œì´ë¸”',
        customProperties={
            'data_format': 'parquet',
            'update_frequency': 'daily',
            'retention_days': '365',
        }
    )
    
    # ì†Œìœ ì ì„¤ì •
    ownership = OwnershipClass(
        owners=[
            OwnerClass(
                owner='urn:li:corpuser:data-team',
                type=OwnershipTypeClass.DATAOWNER,
            )
        ]
    )
    
    # ë©”íƒ€ë°ì´í„° ì „ì†¡
    emitter.emit_mcp(
        dataset_urn,
        aspect_name='datasetProperties',
        aspect_value=properties,
    )
    
    emitter.emit_mcp(
        dataset_urn,
        aspect_name='ownership',
        aspect_value=ownership,
    )
    
    print(f"Metadata registered for {dataset_urn}")

metadata_task = PythonOperator(
    task_id='register_metadata',
    python_callable=register_dataset_metadata,
    dag=dag,
)
```

### 5. ìŠ¤í‚¤ë§ˆ ìë™ ìˆ˜ì§‘

```python
from datahub.emitter.mce_builder import make_dataset_urn, make_schema_field_urn
from datahub.metadata.schema_classes import (
    SchemaMetadataClass,
    SchemaFieldClass,
    SchemaFieldDataTypeClass,
    StringTypeClass,
    NumberTypeClass,
)

def update_schema_metadata(**context):
    """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ DataHubì— ì—…ë°ì´íŠ¸"""
    from airflow.hooks.postgres_hook import PostgresHook
    
    hook = PostgresHook(postgres_conn_id='postgres_prod')
    
    # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
    schema_info = hook.get_records("""
        SELECT column_name, data_type 
        FROM information_schema.columns 
        WHERE table_name = 'users'
    """)
    
    # DataHub ìŠ¤í‚¤ë§ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    fields = []
    for col_name, data_type in schema_info:
        field = SchemaFieldClass(
            fieldPath=col_name,
            type=SchemaFieldDataTypeClass(
                type=StringTypeClass() if 'char' in data_type else NumberTypeClass()
            ),
            nativeDataType=data_type,
        )
        fields.append(field)
    
    schema = SchemaMetadataClass(
        schemaName='users',
        platform='urn:li:dataPlatform:postgres',
        version=0,
        fields=fields,
        hash='',
        platformSchema=None,
    )
    
    # DataHubì— ì „ì†¡
    emitter = DatahubRestEmitter(
        gms_server='http://localhost:8080',
        token='your-access-token'
    )
    
    dataset_urn = make_dataset_urn(
        platform='postgres',
        name='production.users',
        env='PROD'
    )
    
    emitter.emit_mcp(
        dataset_urn,
        aspect_name='schemaMetadata',
        aspect_value=schema,
    )

schema_task = PythonOperator(
    task_id='update_schema',
    python_callable=update_schema_metadata,
    dag=dag,
)
```

### 6. DataHubì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ


```python
from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph

def query_datahub_metadata(**context):
    """DataHubì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
    
    graph = DataHubGraph(
        config=DatahubClientConfig(
            server='http://localhost:8080',
            token='your-access-token'
        )
    )
    
    # ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ
    dataset_urn = 'urn:li:dataset:(urn:li:dataPlatform:snowflake,analytics.dim_users,PROD)'
    
    # Properties ì¡°íšŒ
    properties = graph.get_aspect(
        entity_urn=dataset_urn,
        aspect_type=DatasetPropertiesClass,
    )
    print(f"Description: {properties.description}")
    
    # Lineage ì¡°íšŒ (ì—…ìŠ¤íŠ¸ë¦¼)
    lineage = graph.get_lineage(
        entity_urn=dataset_urn,
        direction='UPSTREAM',
        max_hops=3,
    )
    
    print("Upstream datasets:")
    for edge in lineage.edges:
        print(f"  - {edge.sourceUrn}")
    
    # ê²€ìƒ‰
    search_results = graph.search(
        entity_types=['dataset'],
        query='users',
        start=0,
        count=10,
    )
    
    for result in search_results:
        print(f"Found: {result.entity_urn}")

query_task = PythonOperator(
    task_id='query_metadata',
    python_callable=query_datahub_metadata,
    dag=dag,
)
```

## ì ìš©í•  ë•Œ ê³ ë ¤í–ˆë˜ ì 

1. **ì¼ê´€ëœ ë„¤ì´ë°**: ë°ì´í„°ì…‹ ì´ë¦„ì„ ì¼ê´€ë˜ê²Œ ì‚¬ìš©í•˜ì—¬ Lineage ì¶”ì ì„ ì •í™•í•˜ê²Œ í•´ì•¼ í•¨. (ì´ìŠˆ ë°œìƒ...)
2. **ìë™í™”**: ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œ ìë™ìœ¼ë¡œ DataHubì— ë°˜ì˜ë˜ë„ë¡ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•¨.
3. **íƒœê·¸ í™œìš©**: PII, ë¯¼ê°ì •ë³´ ë“±ì˜ íƒœê·¸ë¥¼ ì¶”ê°€í•˜ì—¬ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ë¥¼ ê°•í™”í•´ì•¼ í•¨.
4. **ë¬¸ì„œí™”**: ê° ë°ì´í„°ì…‹ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ë¥¼ descriptionì— ëª…í™•íˆ ê¸°ë¡í•´ì•¼ í•¨.
5. **ì•ŒëŒ ì—°ë™**: ì¤‘ìš” ë°ì´í„°ì…‹ì˜ ìŠ¤í‚¤ë§ˆ ë³€ê²½, ë°ì´í„° í’ˆì§ˆ ì´ìŠˆë¥¼ DataHubì—ì„œ ì¶”ì í•˜ê³  ì•ŒëŒì„ ë°›ë„ë¡ í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.
