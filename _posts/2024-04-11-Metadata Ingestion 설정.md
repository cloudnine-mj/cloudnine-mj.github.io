---
key: jekyll-text-theme
title: 'Metadata Ingestion ì„¤ì •'
excerpt: ' Datahub ì‹œì‘, ì ìš©í•´ë³´ê¸°ğŸ˜'
tags: [Datahub]
---



# Metadata Ingestion ì„¤ì •

## ê°œë…

* DataHubëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” Ingestion Frameworkë¥¼ ì œê³µí•¨.
* ê° ì†ŒìŠ¤ë³„ë¡œ ìµœì í™”ëœ ì„¤ì •ê³¼ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë°©ë²•ì„ ìµíˆë©´ íš¨ê³¼ì ì¸ ë©”íƒ€ë°ì´í„° ê´€ë¦¬ê°€ ê°€ëŠ¥í•¨.

## Ingestion Framework êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Recipe íŒŒì¼ (YAML/JSON)                 â”‚
â”‚          Source â†’ Transformers â†’ Sink               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Source â”‚   â”‚Transform â”‚  â”‚  Sink  â”‚
    â”‚ Config â”‚   â”‚  Config  â”‚  â”‚ Config â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ê¸°ë³¸ Recipe êµ¬ì¡°

```yaml
# basic_recipe.yml
source:
  type: <source_type>
  config:
    # ì†ŒìŠ¤ë³„ ì„¤ì •

# ì„ íƒì  ë³€í™˜
transformers:
  - type: <transformer_type>
    config:
      # ë³€í™˜ ì„¤ì •

sink:
  type: datahub-rest  # ë˜ëŠ” datahub-kafka
  config:
    server: 'http://localhost:8080'
    token: '${DATAHUB_TOKEN}'  # ì„ íƒì 
```

### MySQL/PostgreSQL Ingestion

```yaml
# mysql_recipe.yml
source:
  type: mysql
  config:
    # ì—°ê²° ì •ë³´
    host_port: "mysql.company.com:3306"
    database: "production"
    
    # ì¸ì¦
    username: "${MYSQL_USER}"
    password: "${MYSQL_PASSWORD}"
    
    # ë˜ëŠ” SQLAlchemy URI ì‚¬ìš©
    # sqlalchemy_uri: "mysql+pymysql://user:pass@host:3306/db"
    
    # ìˆ˜ì§‘ ëŒ€ìƒ í•„í„°ë§
    database_pattern:
      allow:
        - "production"
        - "analytics"
      deny:
        - "test.*"
        - "dev.*"
    
    table_pattern:
      allow:
        - "customers"
        - "orders"
        - "products"
      deny:
        - ".*_temp"
        - ".*_backup"
    
    schema_pattern:
      allow:
        - "public"
    
    # ë·° í¬í•¨ ì—¬ë¶€
    include_views: true
    include_tables: true
    
    # í”„ë¡œíŒŒì¼ë§ (í†µê³„ ìˆ˜ì§‘)
    profiling:
      enabled: true
      # ì „ì²´ í…Œì´ë¸” í”„ë¡œíŒŒì¼ë§ (ëŠë¦¼)
      profile_table_level_only: false
      
      # ìƒ˜í”Œë§
      profile_sample_size: 10000
      
      # ìˆ˜ì§‘í•  í†µê³„
      include_field_null_count: true
      include_field_distinct_count: true
      include_field_min_value: true
      include_field_max_value: true
      include_field_mean_value: true
      include_field_median_value: true
      include_field_stddev_value: true
      
      # í”„ë¡œíŒŒì¼ë§ ëŒ€ìƒ í•„í„°
      profile_table_size_limit: 10  # 10GB ì´í•˜ë§Œ
      profile_table_row_limit: 10000000  # 1000ë§Œ í–‰ ì´í•˜ë§Œ
    
    # ì‚¬ìš© í†µê³„ ìˆ˜ì§‘
    stateful_ingestion:
      enabled: true
      
    # ì†Œí”„íŠ¸ ì‚­ì œëœ í…Œì´ë¸” ì²˜ë¦¬
    remove_stale_metadata: true
    
    # ì»¤ìŠ¤í…€ í”„ë¡œí¼í‹°
    domain:
      "production.*": "urn:li:domain:Production"
      "analytics.*": "urn:li:domain:Analytics"

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'

# PostgreSQLì€ ê±°ì˜ ë™ì¼
# source:
#   type: postgres
#   config:
#     host_port: "postgres.company.com:5432"
#     ...
```

## Snowflake Ingestion

```yaml
# snowflake_recipe.yml
source:
  type: snowflake
  config:
    # ê³„ì • ì •ë³´
    account_id: "xy12345.us-east-1"
    warehouse: "COMPUTE_WH"
    
    # ì¸ì¦
    username: "${SNOWFLAKE_USER}"
    password: "${SNOWFLAKE_PASSWORD}"
    # ë˜ëŠ” í‚¤ í˜ì–´ ì¸ì¦
    # private_key_path: "/path/to/private_key.p8"
    # private_key_password: "${PRIVATE_KEY_PASSWORD}"
    
    # ì—­í• 
    role: "ACCOUNTADMIN"
    
    # ìˆ˜ì§‘ ëŒ€ìƒ
    database_pattern:
      allow:
        - "ANALYTICS_DB"
        - "WAREHOUSE_DB"
    
    schema_pattern:
      allow:
        - "PUBLIC"
        - "REPORTING"
      deny:
        - ".*_BACKUP"
    
    table_pattern:
      allow:
        - "FACT_.*"
        - "DIM_.*"
      deny:
        - ".*_TEMP"
    
    # Snowflake íŠ¹í™” ì˜µì…˜
    include_tables: true
    include_views: true
    include_external_tables: true
    include_materialized_views: true
    
    # ì‚¬ìš© í†µê³„ (ì¿¼ë¦¬ ë¡œê·¸)
    include_usage_stats: true
    start_time: "-7 days"  # ìµœê·¼ 7ì¼
    end_time: "now"
    
    # í…Œì´ë¸” Lineage (ì¿¼ë¦¬ íŒŒì‹±)
    include_table_lineage: true
    include_column_lineage: true  # ì»¬ëŸ¼ ë ˆë²¨ ê³„ë³´
    
    # ì¿¼ë¦¬ íƒœê·¸ ê¸°ë°˜ í•„í„°ë§
    email_domain: "company.com"  # ì´ ë„ë©”ì¸ ì‚¬ìš©ìë§Œ
    
    # í”„ë¡œíŒŒì¼ë§
    profiling:
      enabled: true
      profile_table_level_only: true  # í…Œì´ë¸” ë ˆë²¨ë§Œ (ë¹ ë¦„)
      max_workers: 5  # ë³‘ë ¬ ì²˜ë¦¬
    
    # íƒœê·¸ ë° ì£¼ì„ ìˆ˜ì§‘
    extract_tags: true
    
    # ì„±ëŠ¥ ìµœì í™”
    check_role_grants: false

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## BigQuery Ingestion


```yaml
# bigquery_recipe.yml
source:
  type: bigquery
  config:
    # GCP í”„ë¡œì íŠ¸
    project_id: "my-gcp-project"
    
    # ì¸ì¦
    # ë°©ë²• 1: ì„œë¹„ìŠ¤ ê³„ì • JSON íŒŒì¼
    credential:
      project_id: "my-gcp-project"
      private_key_id: "..."
      private_key: "-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\n"
      client_email: "datahub@my-project.iam.gserviceaccount.com"
      client_id: "..."
    
    # ë°©ë²• 2: íŒŒì¼ ê²½ë¡œ
    # credential_path: "/path/to/service-account.json"
    
    # ìˆ˜ì§‘ ëŒ€ìƒ
    project_ids:
      - "my-gcp-project"
      - "another-project"
    
    dataset_pattern:
      allow:
        - "analytics.*"
        - "warehouse.*"
      deny:
        - ".*_backup"
        - "scratch_.*"
    
    table_pattern:
      allow:
        - "fact_.*"
        - "dim_.*"
    
    # BigQuery íŠ¹í™” ì˜µì…˜
    include_tables: true
    include_views: true
    include_external_tables: true
    
    # ì‚¬ìš© í†µê³„ (ì¿¼ë¦¬ ë¡œê·¸)
    include_usage_statistics: true
    usage_lookback_days: 7
    
    # Lineage (ì¿¼ë¦¬ íŒŒì‹±)
    include_table_lineage: true
    use_exported_bigquery_audit_metadata: true  # Audit ë¡œê·¸ ì‚¬ìš©
    
    # í”„ë¡œíŒŒì¼ë§
    profiling:
      enabled: true
      profile_table_level_only: false
      
      # ë¹„ìš© ì ˆê°ì„ ìœ„í•œ ìƒ˜í”Œë§
      use_sampling: true
      sample_size: 10000
      
      # Dry runìœ¼ë¡œ ë¹„ìš© í™•ì¸
      max_workers: 5
    
    # íŒŒí‹°ì…˜ í…Œì´ë¸” ì²˜ë¦¬
    partition_support_enabled: true
    
    # ë ˆì´ë¸” ë° íƒœê·¸ ìˆ˜ì§‘
    extract_policy_tags: true
    extract_labels: true

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## Kafka Ingestion


```yaml
# kafka_recipe.yml
source:
  type: kafka
  config:
    # Kafka ì—°ê²°
    connection:
      bootstrap: "kafka-broker-1:9092,kafka-broker-2:9092"
      schema_registry_url: "http://schema-registry:8081"
      
      # ì¸ì¦
      consumer_config:
        security.protocol: "SASL_SSL"
        sasl.mechanism: "PLAIN"
        sasl.username: "${KAFKA_USER}"
        sasl.password: "${KAFKA_PASSWORD}"
    
    # ìˆ˜ì§‘ ëŒ€ìƒ í† í”½
    topic_patterns:
      allow:
        - "events.*"
        - "logs.*"
      deny:
        - ".*test.*"
        - ".*temp.*"
    
    # ìŠ¤í‚¤ë§ˆ ìˆ˜ì§‘ (Schema Registry)
    platform_instance: "production-kafka"
    
    # ë„ë©”ì¸ ë§¤í•‘
    domain:
      "events.*": "urn:li:domain:Events"
      "logs.*": "urn:li:domain:Logging"

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## S3 / HDFS Ingestion

```yaml
# s3_recipe.yml
source:
  type: s3
  config:
    # AWS ì¸ì¦
    aws_config:
      aws_access_key_id: "${AWS_ACCESS_KEY}"
      aws_secret_access_key: "${AWS_SECRET_KEY}"
      aws_region: "us-east-1"
    
    # S3 ê²½ë¡œ
    path_specs:
      - include: "s3://my-bucket/data-lake/raw/**/*.parquet"
        exclude:
          - "s3://my-bucket/data-lake/raw/**/_*"  # ìˆ¨ê¹€ íŒŒì¼
        
        # í…Œì´ë¸” ì¶”ì¶œ íŒ¨í„´
        table_name: "raw_{table}"  # íŒŒì¼ëª…ì—ì„œ í…Œì´ë¸”ëª… ì¶”ì¶œ
        
        # íŒŒí‹°ì…˜ ì²˜ë¦¬
        partition_pattern: "s3://my-bucket/data-lake/raw/{table}/year={year}/month={month}/day={day}/*.parquet"
        
        # ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ (Parquet/Avro)
        enable_schema_inference: true
        
        # íŒŒì¼ í†µê³„
        profile_patterns:
          allow:
            - ".*\\.parquet"
    
    # í”„ë¡œíŒŒì¼ë§
    profiling:
      enabled: true
      max_file_size: "1GB"

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## Airflow Ingestion

```yaml
# airflow_recipe.yml
source:
  type: airflow
  config:
    # Airflow ë©”íƒ€ë°ì´í„° DB ì—°ê²°
    conn_id: "airflow_db"
    
    # ë˜ëŠ” ì§ì ‘ ì—°ê²°
    host_port: "postgres:5432"
    database: "airflow"
    username: "${AIRFLOW_DB_USER}"
    password: "${AIRFLOW_DB_PASSWORD}"
    
    # Airflow UI URL
    webserver_url: "http://airflow.company.com"
    
    # ìˆ˜ì§‘ ì˜µì…˜
    capture_ownership_info: true
    capture_tags_info: true
    capture_executions: true  # ì‹¤í–‰ ì´ë ¥
    
    # DAG í•„í„°
    dag_pattern:
      allow:
        - "prod_.*"
        - "etl_.*"
      deny:
        - "test_.*"
        - "dev_.*"
    
    # Lineage ìˆ˜ì§‘
    capture_lineage: true
    
    # ìŠ¤ì¼€ì¤„ ì •ë³´
    extract_dag_schedule: true

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## Tableau Ingestion

```yaml
# tableau_recipe.yml
source:
  type: tableau
  config:
    # Tableau Server ì—°ê²°
    connect_uri: "https://tableau.company.com"
    site: "production"  # ì‚¬ì´íŠ¸ëª… (ì—†ìœ¼ë©´ "")
    
    # ì¸ì¦
    username: "${TABLEAU_USER}"
    password: "${TABLEAU_PASSWORD}"
    # ë˜ëŠ” Personal Access Token
    # token_name: "${TABLEAU_TOKEN_NAME}"
    # token_value: "${TABLEAU_TOKEN_VALUE}"
    
    # ìˆ˜ì§‘ ëŒ€ìƒ
    projects:
      - "Sales Analytics"
      - "Marketing Dashboards"
    
    # ì›Œí¬ë¶/ëŒ€ì‹œë³´ë“œ í•„í„°
    workbook_pattern:
      allow:
        - "Production.*"
      deny:
        - ".*Draft.*"
    
    # Lineage ìˆ˜ì§‘ (ëŒ€ì‹œë³´ë“œ â†’ ë°ì´í„°ì†ŒìŠ¤)
    extract_lineage: true
    
    # ì‚¬ìš© í†µê³„
    extract_usage_stats: true
    stateful_ingestion:
      enabled: true
    
    # ì»¤ìŠ¤í…€ ë©”íƒ€ë°ì´í„°
    extract_tags: true
    extract_owners: true

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## Transformers (ë³€í™˜)

```yaml
# recipe_with_transformers.yml
source:
  type: mysql
  config:
    host_port: "localhost:3306"
    database: "production"
    username: "root"
    password: "password"

transformers:
  # 1. íŒ¨í„´ ê¸°ë°˜ íƒœê·¸ ì¶”ê°€
  - type: pattern_add_dataset_tags
    config:
      tag_pattern:
        rules:
          ".*_pii$": ["PII", "Sensitive"]
          ".*_customer.*": ["Customer_Data"]
          "fact_.*": ["Fact_Table"]
          "dim_.*": ["Dimension_Table"]
  
  # 2. ë„ë©”ì¸ í• ë‹¹
  - type: pattern_add_dataset_domain
    config:
      domain_pattern:
        rules:
          "sales.*": "urn:li:domain:Sales"
          "marketing.*": "urn:li:domain:Marketing"
          "finance.*": "urn:li:domain:Finance"
  
  # 3. ì†Œìœ ì ì¶”ê°€
  - type: pattern_add_dataset_ownership
    config:
      owner_pattern:
        rules:
          "sales.*":
            - "urn:li:corpuser:sales-team"
          "marketing.*":
            - "urn:li:corpuser:marketing-team"
  
  # 4. ìš©ì–´ ì‚¬ì „ ë§¤í•‘
  - type: pattern_add_dataset_terms
    config:
      term_pattern:
        rules:
          ".*customer.*":
            - "urn:li:glossaryTerm:Customer"
          ".*revenue.*":
            - "urn:li:glossaryTerm:Revenue"
  
  # 5. ì„¤ëª… ì¶”ê°€
  - type: simple_add_dataset_properties
    config:
      properties:
        description: "Auto-generated from MySQL production database"
  
  # 6. íƒœê·¸ ì œê±°
  - type: simple_remove_dataset_tags
    config:
      tag_pattern: ".*_temp"

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## ë°°ì¹˜ Ingestion ì‹¤í–‰

```bash
# 1. ë‹¨ì¼ ì‹¤í–‰
datahub ingest -c mysql_recipe.yml

# 2. Dry run (ì‹¤ì œ ì „ì†¡ ì•ˆí•¨)
datahub ingest -c mysql_recipe.yml --dry-run

# 3. ë””ë²„ê·¸ ëª¨ë“œ
datahub ingest -c mysql_recipe.yml --debug

# 4. ê²°ê³¼ íŒŒì¼ ì €ì¥
datahub ingest -c mysql_recipe.yml --report-to report.json

# 5. í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
export MYSQL_USER=root
export MYSQL_PASSWORD=password
datahub ingest -c mysql_recipe.yml
```

## ìŠ¤ì¼€ì¤„ë§

### Airflow DAG

```python
# ingestion_dag.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email': ['data-team@company.com'],
}

dag = DAG(
    'datahub_metadata_ingestion',
    default_args=default_args,
    schedule_interval='0 2 * * *',  # ë§¤ì¼ 02:00
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['datahub', 'metadata'],
)

# MySQL Ingestion
mysql_ingest = BashOperator(
    task_id='ingest_mysql',
    bash_command='datahub ingest -c /configs/mysql_recipe.yml',
    dag=dag,
)

# Snowflake Ingestion
snowflake_ingest = BashOperator(
    task_id='ingest_snowflake',
    bash_command='datahub ingest -c /configs/snowflake_recipe.yml',
    dag=dag,
)

# Airflow Ingestion (ìê¸° ìì‹ )
airflow_ingest = BashOperator(
    task_id='ingest_airflow',
    bash_command='datahub ingest -c /configs/airflow_recipe.yml',
    dag=dag,
)

# ë³‘ë ¬ ì‹¤í–‰
[mysql_ingest, snowflake_ingest] >> airflow_ingest
```

### Kubernetes CronJob


```yaml
# datahub-ingestion-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: datahub-mysql-ingestion
  namespace: datahub
spec:
  schedule: "0 2 * * *"  # ë§¤ì¼ 02:00
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: ingestion
            image: acryldata/datahub-ingestion:latest
            command:
            - /bin/sh
            - -c
            - |
              datahub ingest -c /configs/mysql_recipe.yml
            volumeMounts:
            - name: config
              mountPath: /configs
            env:
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  name: mysql-credentials
                  key: username
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-credentials
                  key: password
          volumes:
          - name: config
            configMap:
              name: datahub-recipes
          restartPolicy: OnFailure
```

## ì„±ëŠ¥ ìµœì í™”

### 1. í”„ë¡œíŒŒì¼ë§ ìµœì í™”

```yaml
# í° í…Œì´ë¸” í”„ë¡œíŒŒì¼ë§ ìµœì í™”
profiling:
  enabled: true
  
  # í…Œì´ë¸” ë ˆë²¨ë§Œ (ë¹ ë¦„)
  profile_table_level_only: true
  
  # ìƒ˜í”Œë§
  use_sampling: true
  sample_size: 10000
  
  # í¬ê¸°/í–‰ ìˆ˜ ì œí•œ
  profile_table_size_limit: 10  # GB
  profile_table_row_limit: 10000000
  
  # ë³‘ë ¬ ì²˜ë¦¬
  max_workers: 5
  
  # íŠ¹ì • ì»¬ëŸ¼ ì œì™¸
  exclude_field_patterns:
    - ".*_id$"
    - ".*_timestamp$"
```

### 2. ë³‘ë ¬ ì²˜ë¦¬

```python
# parallel_ingestion.py
from concurrent.futures import ThreadPoolExecutor
import subprocess

recipes = [
    'mysql_recipe.yml',
    'snowflake_recipe.yml',
    'bigquery_recipe.yml',
]

def run_ingestion(recipe):
    """Ingestion ì‹¤í–‰"""
    cmd = f'datahub ingest -c {recipe}'
    result = subprocess.run(cmd, shell=True, capture_output=True)
    
    if result.returncode != 0:
        print(f"Failed: {recipe}")
        print(result.stderr.decode())
    else:
        print(f"Success: {recipe}")

# ë³‘ë ¬ ì‹¤í–‰
with ThreadPoolExecutor(max_workers=3) as executor:
    executor.map(run_ingestion, recipes)
```

### 3. ì¦ë¶„ ìˆ˜ì§‘ (Stateful Ingestion)

```yaml
# incremental_recipe.yml
source:
  type: mysql
  config:
    host_port: "localhost:3306"
    database: "production"
    username: "root"
    password: "password"
    
    # ìƒíƒœ ì €ì¥ í™œì„±í™”
    stateful_ingestion:
      enabled: true
      
      # ìƒíƒœ ì €ì¥ ìœ„ì¹˜
      state_provider:
        type: datahub
        config:
          datahub_api:
            server: "http://localhost:8080"
      
      # ì œê±°ëœ ì—”í‹°í‹° ì²˜ë¦¬
      remove_stale_metadata: true

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. ì—°ê²° ì˜¤ë¥˜

```bash
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
# test_connection.py
from sqlalchemy import create_engine

# MySQL í…ŒìŠ¤íŠ¸
engine = create_engine('mysql+pymysql://user:pass@host:3306/db')
with engine.connect() as conn:
    result = conn.execute("SELECT 1")
    print(f"MySQL OK: {result.fetchone()}")

# Snowflake í…ŒìŠ¤íŠ¸
from snowflake.connector import connect

conn = connect(
    account='xy12345.us-east-1',
    user='username',
    password='password',
    warehouse='COMPUTE_WH'
)
cursor = conn.cursor()
cursor.execute("SELECT CURRENT_VERSION()")
print(f"Snowflake OK: {cursor.fetchone()}")
```

### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±

```yaml
# ë©”ëª¨ë¦¬ ì ˆì•½ ì„¤ì •
source:
  type: mysql
  config:
    # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
    profiling:
      enabled: true
      max_workers: 2  # ë³‘ë ¬ë„ ë‚®ì¶”ê¸°
      profile_sample_size: 1000  # ìƒ˜í”Œ í¬ê¸° ì¤„ì´ê¸°
    
    # í…Œì´ë¸” ìˆ˜ ì œí•œ
    table_pattern:
      allow:
        - "important_table_1"
        - "important_table_2"
```

### 3. ê¶Œí•œ ì˜¤ë¥˜


```sql
-- MySQL ìµœì†Œ ê¶Œí•œ
GRANT SELECT ON database.* TO 'datahub_user'@'%';
GRANT SELECT ON information_schema.* TO 'datahub_user'@'%';

-- Snowflake ìµœì†Œ ê¶Œí•œ
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE datahub_role;
GRANT USAGE ON DATABASE analytics_db TO ROLE datahub_role;
GRANT USAGE ON SCHEMA analytics_db.public TO ROLE datahub_role;
GRANT SELECT ON ALL TABLES IN SCHEMA analytics_db.public TO ROLE datahub_role;
GRANT SELECT ON ALL VIEWS IN SCHEMA analytics_db.public TO ROLE datahub_role;

-- ì‚¬ìš© í†µê³„ ìˆ˜ì§‘ ê¶Œí•œ
GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE datahub_role;
```

## ì ìš©í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ì 

1. **ì ì§„ì  ìˆ˜ì§‘**: ëª¨ë“  í…Œì´ë¸”ì„ í•œ ë²ˆì— ìˆ˜ì§‘í•˜ì§€ ë§ê³  ì¤‘ìš”í•œ ê²ƒë¶€í„°
2. **í”„ë¡œíŒŒì¼ë§ ì„ íƒ**: í•„ìš”í•œ í…Œì´ë¸”ë§Œ í”„ë¡œíŒŒì¼ë§í•˜ì—¬ ë¹„ìš© ì ˆê°
3. **ìŠ¤ì¼€ì¤„ ë¶„ì‚°**: ëª¨ë“  ì†ŒìŠ¤ë¥¼ ë™ì‹œì— ìˆ˜ì§‘í•˜ì§€ ë§ê³  ì‹œê°„ ë¶„ì‚°
4. **ëª¨ë‹ˆí„°ë§**: Ingestion ì„±ê³µ/ì‹¤íŒ¨ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ì•Œë¦¼ ì„¤ì •
5. **ë¬¸ì„œí™”**: ê° Recipeì˜ ëª©ì ê³¼ ìŠ¤ì¼€ì¤„ì„ ë¬¸ì„œí™”
6. **í…ŒìŠ¤íŠ¸**: í”„ë¡œë•ì…˜ ì ìš© ì „ dry-runìœ¼ë¡œ í…ŒìŠ¤íŠ¸