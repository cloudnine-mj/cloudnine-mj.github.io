---
key: jekyll-text-theme
title: 'Hadoop ecosystem'
excerpt: ' Hadoop study ğŸ˜'
tags: [Hadoop]
---



# Hadoop ì—ì½”ì‹œìŠ¤í…œ

Hadoop ì—ì½”ì‹œìŠ¤í…œì€ HDFSì™€ MapReduceë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ ë„êµ¬ë“¤ì´ ê²°í•©ëœ í†µí•© ë¹…ë°ì´í„° í”Œë«í¼ì„. ê° ë„êµ¬ëŠ” íŠ¹ì • ìš©ë„ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ê°•ë ¥í•œ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŒ.

## 1. Hive - SQL ê¸°ë°˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤

**ê°œë…**

HiveëŠ” HDFSì— ì €ì¥ëœ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ SQLë¡œ ì¿¼ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì‹œìŠ¤í…œì„. SQLì„ MapReduce ì‘ì—…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‹¤í–‰í•¨.

**ì£¼ìš” íŠ¹ì§•**

- SQL ë¬¸ë²• ì§€ì› (HiveQL)
- í…Œì´ë¸” ê¸°ë°˜ ë°ì´í„° ê´€ë¦¬
- íŒŒí‹°ì…”ë‹ê³¼ ë²„í‚·íŒ…ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
- UDF(User Defined Function) ì§€ì›

**ì„¤ì¹˜ ë°©ë²•**

```bash
# Hive ë‹¤ìš´ë¡œë“œ
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf apache-hive-3.1.3-bin.tar.gz
sudo mv apache-hive-3.1.3-bin /usr/local/hive

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo 'export HIVE_HOME=/usr/local/hive' >> ~/.bashrc
echo 'export PATH=$PATH:$HIVE_HOME/bin' >> ~/.bashrc
source ~/.bashrc

# Hive ë””ë ‰í† ë¦¬ ìƒì„±
hdfs dfs -mkdir /tmp
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -chmod g+w /tmp
hdfs dfs -chmod g+w /user/hive/warehouse

# Metastore ì´ˆê¸°í™”
schematool -dbType derby -initSchema

# Hive ì‹œì‘
hive
```

**ê¸°ë³¸ ì‚¬ìš©ë²•**

```sql
-- ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
CREATE DATABASE mydb;
USE mydb;

-- í…Œì´ë¸” ìƒì„±
CREATE TABLE employees (
    id INT,
    name STRING,
    age INT,
    department STRING,
    salary DOUBLE
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

-- ë°ì´í„° ë¡œë“œ (HDFSì—ì„œ)
LOAD DATA INPATH '/user/data/employees.csv' INTO TABLE employees;

-- ë°ì´í„° ì¡°íšŒ
SELECT * FROM employees WHERE age > 30;

-- ì§‘ê³„ ì¿¼ë¦¬
SELECT department, AVG(salary) as avg_salary
FROM employees
GROUP BY department;

-- íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„±
CREATE TABLE sales (
    product_id INT,
    amount DOUBLE
)
PARTITIONED BY (year INT, month INT)
STORED AS PARQUET;

-- íŒŒí‹°ì…˜ì— ë°ì´í„° ì‚½ì…
INSERT INTO TABLE sales PARTITION(year=2024, month=12)
SELECT product_id, amount FROM staging_sales
WHERE sale_date LIKE '2024-12%';
```

**ì‹¤ì „ ì˜ˆì‹œ: ë¡œê·¸ ë¶„ì„**

~~~sql
-- ì›¹ ë¡œê·¸ í…Œì´ë¸” ìƒì„±
CREATE EXTERNAL TABLE web_logs (
    ip STRING,
    timestamp STRING,
    method STRING,
    url STRING,
    status INT,
    size INT
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
    "input.regex" = "([^ ]*) - - \\[([^\\]]*)\\] \"([^ ]*) ([^ ]*) [^\"]*\" ([0-9]*) ([0-9]*)"
)
LOCATION '/logs/web/';

-- ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ ë¶„ì„
SELECT 
    HOUR(from_unixtime(unix_timestamp(timestamp, 'dd/MMM/yyyy:HH:mm:ss'))) as hour,
    COUNT(*) as requests
FROM web_logs
GROUP BY HOUR(from_unixtime(unix_timestamp(timestamp, 'dd/MMM/yyyy:HH:mm:ss')))
ORDER BY hour;

-- ê°€ì¥ ë§ì´ ë°©ë¬¸í•œ í˜ì´ì§€ Top 10
SELECT url, COUNT(*) as visits
FROM web_logs
WHERE status = 200
GROUP BY url
ORDER BY visits DESC
LIMIT 10;
```


~~~

## 2. HBase - ë¶„ì‚° NoSQL ë°ì´í„°ë² ì´ìŠ¤

**ê°œë…**

HBaseëŠ” Hadoop ìœ„ì—ì„œ ë™ì‘í•˜ëŠ” ë¶„ì‚° NoSQL ë°ì´í„°ë² ì´ìŠ¤ì„. ìˆ˜ì‹­ì–µ ê°œì˜ í–‰ê³¼ ìˆ˜ë°±ë§Œ ê°œì˜ ì—´ì„ ê°€ì§„ ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì½ê³  ì“¸ ìˆ˜ ìˆìŒ.

**ì£¼ìš” íŠ¹ì§•**

- ì»¬ëŸ¼ ê¸°ë°˜ ì €ì¥ êµ¬ì¡°
- ì‹¤ì‹œê°„ ëœë¤ ì½ê¸°/ì“°ê¸°
- ìë™ ìƒ¤ë”©
- ì„ í˜•ì  í™•ì¥ì„±
- ê°•ë ¥í•œ ì¼ê´€ì„±

**ì•„í‚¤í…ì²˜**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HBase Master                          â”‚
â”‚              (ë©”íƒ€ë°ì´í„° ê´€ë¦¬ & ì¡°ì •)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼             â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚RegionServer 1â”‚ â”‚RegionServer 2â”‚ â”‚RegionServer 3â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  Region A    â”‚ â”‚  Region B    â”‚ â”‚  Region C    â”‚
    â”‚  Region D    â”‚ â”‚  Region E    â”‚ â”‚  Region F    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚             â”‚             â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    HDFS     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì„¤ì¹˜ ë°©ë²•**

```bash
# HBase ë‹¤ìš´ë¡œë“œ
wget https://downloads.apache.org/hbase/2.5.5/hbase-2.5.5-bin.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf hbase-2.5.5-bin.tar.gz
sudo mv hbase-2.5.5 /usr/local/hbase

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo 'export HBASE_HOME=/usr/local/hbase' >> ~/.bashrc
echo 'export PATH=$PATH:$HBASE_HOME/bin' >> ~/.bashrc
source ~/.bashrc

# hbase-env.sh ì„¤ì •
nano $HBASE_HOME/conf/hbase-env.sh
# ë‹¤ìŒ ì¤„ ì¶”ê°€:
# export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
# export HBASE_MANAGES_ZK=true

# hbase-site.xml ì„¤ì •
nano $HBASE_HOME/conf/hbase-site.xml
```

```xml
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://localhost:9000/hbase</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.zookeeper.property.dataDir</name>
        <value>/home/ì‚¬ìš©ìëª…/zookeeper</value>
    </property>
</configuration>
```

**HBase ì‹œì‘**

```bash
# HBase ì‹œì‘
start-hbase.sh

# HBase Shell ì ‘ì†
hbase shell

# ìƒíƒœ í™•ì¸
status
```

**ê¸°ë³¸ ì‚¬ìš©ë²•**

```bash
# í…Œì´ë¸” ìƒì„±
create 'users', 'info', 'contact'

# í…Œì´ë¸” ëª©ë¡ ë³´ê¸°
list

# ë°ì´í„° ì‚½ì…
put 'users', 'user1', 'info:name', 'John Doe'
put 'users', 'user1', 'info:age', '30'
put 'users', 'user1', 'contact:email', 'john@example.com'
put 'users', 'user1', 'contact:phone', '010-1234-5678'

# ë°ì´í„° ì¡°íšŒ
get 'users', 'user1'

# íŠ¹ì • ì»¬ëŸ¼ ì¡°íšŒ
get 'users', 'user1', 'info:name'

# ì „ì²´ ìŠ¤ìº”
scan 'users'

# ì¡°ê±´ë¶€ ìŠ¤ìº”
scan 'users', {FILTER => "SingleColumnValueFilter('info', 'age', >, 'binary:25')"}

# ë°ì´í„° ì‚­ì œ
delete 'users', 'user1', 'info:age'

# í…Œì´ë¸” ì‚­ì œ
disable 'users'
drop 'users'
```

**Pythonìœ¼ë¡œ HBase ì‚¬ìš©**

```python
import happybase

# HBase ì—°ê²°
connection = happybase.Connection('localhost')

# í…Œì´ë¸” ìƒì„±
connection.create_table(
    'users',
    {
        'info': dict(),
        'contact': dict()
    }
)

# í…Œì´ë¸” ì—´ê¸°
table = connection.table('users')

# ë°ì´í„° ì‚½ì…
table.put(b'user1', {
    b'info:name': b'John Doe',
    b'info:age': b'30',
    b'contact:email': b'john@example.com'
})

# ë°ì´í„° ì¡°íšŒ
row = table.row(b'user1')
print(row)

# ë°°ì¹˜ ì‚½ì…
batch = table.batch()
for i in range(1000):
    batch.put(f'user{i}'.encode(), {
        b'info:name': f'User {i}'.encode(),
        b'info:age': str(20 + i % 50).encode()
    })
batch.send()

# ìŠ¤ìº”
for key, data in table.scan():
    print(key, data)

# ì—°ê²° ì¢…ë£Œ
connection.close()
```

## 3. Pig - ë°ì´í„° íë¦„ ìŠ¤í¬ë¦½íŒ…

**ê°œë…**

PigëŠ” MapReduce ì‘ì—…ì„ ê°„ë‹¨í•œ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‘ì„±í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í”Œë«í¼ì„. Pig Latinì´ë¼ëŠ” ê³ ìˆ˜ì¤€ ì–¸ì–´ë¥¼ ì‚¬ìš©í•¨.

**ì„¤ì¹˜ ë°©ë²•**

```bash
# Pig ë‹¤ìš´ë¡œë“œ
wget https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf pig-0.17.0.tar.gz
sudo mv pig-0.17.0 /usr/local/pig

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo 'export PIG_HOME=/usr/local/pig' >> ~/.bashrc
echo 'export PATH=$PATH:$PIG_HOME/bin' >> ~/.bashrc
source ~/.bashrc

# Pig ì‹¤í–‰
pig
```

**ê¸°ë³¸ ì‚¬ìš©ë²•**

```pig
-- ë°ì´í„° ë¡œë“œ
users = LOAD '/data/users.csv' USING PigStorage(',')
    AS (id:int, name:chararray, age:int, city:chararray);

-- í•„í„°ë§
adults = FILTER users BY age >= 18;

-- ê·¸ë£¹í™”
by_city = GROUP adults BY city;

-- ì§‘ê³„
city_counts = FOREACH by_city GENERATE 
    group AS city, 
    COUNT(adults) AS user_count;

-- ì •ë ¬
sorted = ORDER city_counts BY user_count DESC;

-- ìƒìœ„ 10ê°œ ì¶”ì¶œ
top10 = LIMIT sorted 10;

-- ê²°ê³¼ ì €ì¥
STORE top10 INTO '/output/city_stats' USING PigStorage(',');
```

**ì‹¤ì „ ì˜ˆì‹œ: ë¡œê·¸ ë¶„ì„**

```pig
-- ì›¹ ë¡œê·¸ ë¡œë“œ
logs = LOAD '/logs/web/*.log' USING PigStorage(' ')
    AS (ip:chararray, timestamp:chararray, method:chararray, 
        url:chararray, status:int, size:int);

-- ì„±ê³µí•œ ìš”ì²­ë§Œ í•„í„°ë§
successful = FILTER logs BY status == 200;

-- URLë³„ ê·¸ë£¹í™”
by_url = GROUP successful BY url;

-- URLë³„ í†µê³„ ê³„ì‚°
url_stats = FOREACH by_url GENERATE
    group AS url,
    COUNT(successful) AS hits,
    SUM(successful.size) AS total_bytes,
    AVG(successful.size) AS avg_bytes;

-- íˆíŠ¸ ìˆ˜ë¡œ ì •ë ¬
sorted_urls = ORDER url_stats BY hits DESC;

-- ê²°ê³¼ ì €ì¥
STORE sorted_urls INTO '/output/url_stats';
```

## 4. Sqoop - ë°ì´í„° ì „ì†¡ ë„êµ¬

**ê°œë…**

Sqoopì€ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤(MySQL, PostgreSQL, Oracle ë“±)ì™€ Hadoop ê°„ì˜ ë°ì´í„° ì „ì†¡ì„ ìë™í™”í•˜ëŠ” ë„êµ¬ì„.

**ì„¤ì¹˜ ë°©ë²•**

```bash
# Sqoop ë‹¤ìš´ë¡œë“œ
wget https://downloads.apache.org/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.0.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf sqoop-1.4.7.bin__hadoop-2.0.tar.gz
sudo mv sqoop-1.4.7.bin__hadoop-2.0 /usr/local/sqoop

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo 'export SQOOP_HOME=/usr/local/sqoop' >> ~/.bashrc
echo 'export PATH=$PATH:$SQOOP_HOME/bin' >> ~/.bashrc
source ~/.bashrc

# JDBC ë“œë¼ì´ë²„ ë³µì‚¬ (MySQL ì˜ˆì‹œ)
cp mysql-connector-java.jar $SQOOP_HOME/lib/
```

**MySQLì—ì„œ HDFSë¡œ ê°€ì ¸ì˜¤ê¸°**

```bash
# ë‹¨ì¼ í…Œì´ë¸” ê°€ì ¸ì˜¤ê¸°
sqoop import \
  --connect jdbc:mysql://localhost:3306/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --target-dir /user/data/employees \
  --num-mappers 4

# íŠ¹ì • ì»¬ëŸ¼ë§Œ ê°€ì ¸ì˜¤ê¸°
sqoop import \
  --connect jdbc:mysql://localhost:3306/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --columns "id,name,salary" \
  --target-dir /user/data/employees_basic

# WHERE ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§
sqoop import \
  --connect jdbc:mysql://localhost:3306/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --where "salary > 50000" \
  --target-dir /user/data/high_salary_employees

# ì¦ë¶„ ê°€ì ¸ì˜¤ê¸° (ë³€ê²½ëœ ë°ì´í„°ë§Œ)
sqoop import \
  --connect jdbc:mysql://localhost:3306/mydb \
  --username root \
  --password mypassword \
  --table orders \
  --incremental append \
  --check-column order_id \
  --last-value 1000 \
  --target-dir /user/data/orders

# ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
sqoop import-all-tables \
  --connect jdbc:mysql://localhost:3306/mydb \
  --username root \
  --password mypassword \
  --warehouse-dir /user/data/mydb
```

**HDFSì—ì„œ MySQLë¡œ ë‚´ë³´ë‚´ê¸°**

~~~bash
# Hive í…Œì´ë¸”ì„ MySQLë¡œ ë‚´ë³´ë‚´ê¸°
sqoop export \
  --connect jdbc:mysql://localhost:3306/mydb \
  --username root \
  --password mypassword \
  --table employees_export \
  --export-dir /user/hive/warehouse/employees \
  --input-fields-terminated-by ',' \
  --num-mappers 4

# ì—…ë°ì´íŠ¸ ëª¨ë“œë¡œ ë‚´ë³´ë‚´ê¸°
sqoop export \
  --connect jdbc:mysql://localhost:3306/mydb \
  --username root \
  --password mypassword \
  --table employees \
  --update-key id \
  --export-dir /user/data/updated_employees
```


~~~

## 5. Flume - ë¡œê·¸ ìˆ˜ì§‘ ë„êµ¬

**ê°œë…**

Flumeì€ ëŒ€ìš©ëŸ‰ì˜ ë¡œê·¸ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  HDFSë¡œ ì „ì†¡í•˜ëŠ” ë¶„ì‚° ì‹œìŠ¤í…œì„.

**ì•„í‚¤í…ì²˜**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source    â”‚  -->  â”‚   Channel   â”‚  -->  â”‚    Sink     â”‚
â”‚ (ë°ì´í„° ìˆ˜ì§‘)  â”‚       â”‚  (ë²„í¼ë§)    â”‚       â”‚ (ë°ì´í„° ì „ì†¡)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì„¤ì¹˜ ë°©ë²•**

```bash
# Flume ë‹¤ìš´ë¡œë“œ
wget https://downloads.apache.org/flume/1.11.0/apache-flume-1.11.0-bin.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf apache-flume-1.11.0-bin.tar.gz
sudo mv apache-flume-1.11.0-bin /usr/local/flume

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo 'export FLUME_HOME=/usr/local/flume' >> ~/.bashrc
echo 'export PATH=$PATH:$FLUME_HOME/bin' >> ~/.bashrc
source ~/.bashrc
```

**ê¸°ë³¸ ì„¤ì • íŒŒì¼ ì‘ì„±**

**ì›¹ ì„œë²„ ë¡œê·¸ ìˆ˜ì§‘ ì˜ˆì‹œ (flume-weblog.conf)**

```properties
# Agent ì´ë¦„
agent.sources = weblog
agent.channels = memChannel
agent.sinks = hdfsSink

# Source ì„¤ì • (íŒŒì¼ ê°ì‹œ)
agent.sources.weblog.type = exec
agent.sources.weblog.command = tail -F /var/log/apache/access.log
agent.sources.weblog.channels = memChannel

# Channel ì„¤ì • (ë©”ëª¨ë¦¬ ë²„í¼)
agent.channels.memChannel.type = memory
agent.channels.memChannel.capacity = 10000
agent.channels.memChannel.transactionCapacity = 1000

# Sink ì„¤ì • (HDFS ì €ì¥)
agent.sinks.hdfsSink.type = hdfs
agent.sinks.hdfsSink.channel = memChannel
agent.sinks.hdfsSink.hdfs.path = /logs/web/%Y/%m/%d
agent.sinks.hdfsSink.hdfs.filePrefix = access_log
agent.sinks.hdfsSink.hdfs.fileSuffix = .log
agent.sinks.hdfsSink.hdfs.rollInterval = 3600
agent.sinks.hdfsSink.hdfs.rollSize = 134217728
agent.sinks.hdfsSink.hdfs.rollCount = 0
agent.sinks.hdfsSink.hdfs.fileType = DataStream
agent.sinks.hdfsSink.hdfs.writeFormat = Text
agent.sinks.hdfsSink.hdfs.useLocalTimeStamp = true
```

**Flume ì‹¤í–‰**

```bash
# Flume agent ì‹œì‘
flume-ng agent \
  --conf $FLUME_HOME/conf \
  --conf-file flume-weblog.conf \
  --name agent \
  -Dflume.root.logger=INFO,console
```

**ë„¤íŠ¸ì›Œí¬ ì†Œì¼“ ìˆ˜ì‹  ì˜ˆì‹œ (flume-netcat.conf)**

```properties
# Agent ì„¤ì •
agent.sources = netcatSource
agent.channels = memChannel
agent.sinks = loggerSink

# Netcat Source (í¬íŠ¸ 44444ì—ì„œ ìˆ˜ì‹ )
agent.sources.netcatSource.type = netcat
agent.sources.netcatSource.bind = localhost
agent.sources.netcatSource.port = 44444
agent.sources.netcatSource.channels = memChannel

# Memory Channel
agent.channels.memChannel.type = memory
agent.channels.memChannel.capacity = 1000

# Logger Sink (ì½˜ì†” ì¶œë ¥)
agent.sinks.loggerSink.type = logger
agent.sinks.loggerSink.channel = memChannel
```

**í…ŒìŠ¤íŠ¸**

```bash
# Flume ì‹¤í–‰
flume-ng agent \
  --conf $FLUME_HOME/conf \
  --conf-file flume-netcat.conf \
  --name agent

# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ë°ì´í„° ì „ì†¡
telnet localhost 44444
Hello Flume!
```

## 6. Oozie - ì›Œí¬í”Œë¡œìš° ìŠ¤ì¼€ì¤„ëŸ¬

**ê°œë…**

OozieëŠ” Hadoop ì‘ì—…ì„ ì¡°ì •í•˜ê³  ìŠ¤ì¼€ì¤„ë§í•˜ëŠ” ì›Œí¬í”Œë¡œìš° ì—”ì§„ì„. MapReduce, Pig, Hive, Sqoop ë“±ì˜ ì‘ì—…ì„ ìˆœì°¨ì  ë˜ëŠ” ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŒ.

**ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œ (workflow.xml)**

```xml
<workflow-app name="data-pipeline" xmlns="uri:oozie:workflow:0.5">
    <start to="sqoop-import"/>
    
    <!-- Sqoopìœ¼ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° -->
    <action name="sqoop-import">
        <sqoop xmlns="uri:oozie:sqoop-action:0.4">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <command>import --connect jdbc:mysql://localhost/mydb --table users --target-dir /data/users</command>
        </sqoop>
        <ok to="hive-process"/>
        <error to="fail"/>
    </action>
    
    <!-- Hiveë¡œ ë°ì´í„° ì²˜ë¦¬ -->
    <action name="hive-process">
        <hive xmlns="uri:oozie:hive-action:0.5">
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <script>process_users.hql</script>
        </hive>
        <ok to="end"/>
        <error to="fail"/>
    </action>
    
    <kill name="fail">
        <message>Workflow failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    
    <end name="end"/>
</workflow-app>
```

**Coordinator ì„¤ì • (ë§¤ì¼ ì‹¤í–‰)**

```xml
<coordinator-app name="daily-data-pipeline" frequency="${coord:days(1)}"
                 start="2024-01-01T00:00Z" end="2024-12-31T23:59Z"
                 timezone="Asia/Seoul"
                 xmlns="uri:oozie:coordinator:0.4">
    <action>
        <workflow>
            <app-path>${workflowPath}</app-path>
        </workflow>
    </action>
</coordinator-app>
```

## 7. Zookeeper - ë¶„ì‚° ì½”ë””ë„¤ì´ì…˜

**ê°œë…**

ZookeeperëŠ” ë¶„ì‚° ì‹œìŠ¤í…œì„ ìœ„í•œ ì½”ë””ë„¤ì´ì…˜ ì„œë¹„ìŠ¤ì„. ì„¤ì • ê´€ë¦¬, ë„¤ì´ë° ì„œë¹„ìŠ¤, ë¶„ì‚° ë™ê¸°í™”, ê·¸ë£¹ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•¨.

**ì£¼ìš” ê¸°ëŠ¥**

- ì„¤ì • ì •ë³´ ì¤‘ì•™ ê´€ë¦¬
- ë¦¬ë” ì„ ì¶œ
- ë¶„ì‚° ë½
- ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬

**ì„¤ì¹˜ ë° ì‹¤í–‰**

```bash
# Zookeeper ë‹¤ìš´ë¡œë“œ
wget https://downloads.apache.org/zookeeper/zookeeper-3.8.2/apache-zookeeper-3.8.2-bin.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf apache-zookeeper-3.8.2-bin.tar.gz
sudo mv apache-zookeeper-3.8.2-bin /usr/local/zookeeper

# ì„¤ì • íŒŒì¼ ìƒì„±
cp /usr/local/zookeeper/conf/zoo_sample.cfg /usr/local/zookeeper/conf/zoo.cfg

# Zookeeper ì‹œì‘
/usr/local/zookeeper/bin/zkServer.sh start

# CLI ì ‘ì†
/usr/local/zookeeper/bin/zkCli.sh
```

**ê¸°ë³¸ ëª…ë ¹ì–´**

~~~bash
# ë…¸ë“œ ìƒì„±
create /myapp "app_data"

# ë…¸ë“œ ì¡°íšŒ
get /myapp

# ë…¸ë“œ ëª©ë¡
ls /

# ë…¸ë“œ ì—…ë°ì´íŠ¸
set /myapp "new_data"

# ë…¸ë“œ ì‚­ì œ
delete /myapp

# ìƒíƒœ í™•ì¸
stat /myapp
```
~~~

## ì—ì½”ì‹œìŠ¤í…œ í†µí•© ì˜ˆì‹œ

**ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸**
```
[ì›¹ ì„œë²„ ë¡œê·¸]
       â†“
   [Flume] â”€â”€â”€â”€â”€â†’ [Kafka] â”€â”€â”€â”€â”€â†’ [Spark Streaming]
                                         â†“
                                    [HBase] â† ì‹¤ì‹œê°„ ì¡°íšŒ
                                         â†“
                                     [HDFS] â† ì¥ê¸° ë³´ê´€
                                         â†“
                                     [Hive] â† ë°°ì¹˜ ë¶„ì„
                                         â†“
                                     [BI ë„êµ¬]
```

**ë°°ì¹˜ ETL íŒŒì´í”„ë¼ì¸ (Oozie ì¡°ì •)**
```
1. [Sqoop] MySQL â†’ HDFS
2. [Pig] ë°ì´í„° ì •ì œ
3. [Hive] ì§‘ê³„ ë° ë³€í™˜
4. [Sqoop] HDFS â†’ Data Warehouse
