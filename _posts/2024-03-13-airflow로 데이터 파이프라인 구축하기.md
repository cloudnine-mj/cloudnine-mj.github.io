---
key: jekyll-text-theme
title: 'Airflow로 데이터 파이프라인 구축하기'
excerpt: ' DAG 설계의 기본부터 실전까지 😎'
tags: [Airflow]
---

# Airflow로 데이터 파이프라인 구축하기

## 개념

* Apache Airflow는 프로그래밍 방식으로 워크플로우를 작성, 스케줄링, 모니터링할 수 있는 오픈소스 플랫	폼
* DAG(Directed Acyclic Graph)는 Airflow의 핵심 개념으로, 방향성은 있지만 순환하지 않는 그래프 구조로 작업 흐름을 표현함.

## 설치

```bash
# pip를 통한 설치
pip install apache-airflow==2.7.0

# 환경 변수 설정
export AIRFLOW_HOME=~/airflow

# DB 초기화
airflow db init

# 관리자 계정 생성
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# 웹서버 실행
airflow webserver --port 8080

# 스케줄러 실행 (새 터미널)
airflow scheduler
```

## 원리

DAG는 Python 코드로 정의되며, Airflow 스케줄러가 DAG 파일을 주기적으로 스캔하여 실행할 작업을 결정합니다. 각 Task는 Operator로 정의되며, Task 간 의존성은 `>>` 또는 `set_downstream()` 메서드로 설정합니다.

## 코드

* 업무하면서 활용했던 코드


```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

# 기본 인자 설정
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email': ['data-team@company.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# DAG 정의
dag = DAG(
    'daily_data_pipeline',
    default_args=default_args,
    description='일일 데이터 수집 및 처리 파이프라인',
    schedule_interval='0 2 * * *',  # 매일 오전 2시
    catchup=False,
    tags=['production', 'daily'],
)

def extract_data(**context):
    """데이터 추출 함수"""
    execution_date = context['execution_date']
    print(f"Extracting data for {execution_date}")
    # 실제 데이터 추출 로직
    return {'status': 'success', 'rows': 10000}

def transform_data(**context):
    """데이터 변환 함수"""
    ti = context['ti']
    extract_result = ti.xcom_pull(task_ids='extract_task')
    print(f"Transforming {extract_result['rows']} rows")
    # 실제 데이터 변환 로직
    return {'status': 'success', 'transformed_rows': 9500}

def load_data(**context):
    """데이터 적재 함수"""
    ti = context['ti']
    transform_result = ti.xcom_pull(task_ids='transform_task')
    print(f"Loading {transform_result['transformed_rows']} rows")
    # 실제 데이터 적재 로직

# Task 정의
extract_task = PythonOperator(
    task_id='extract_task',
    python_callable=extract_data,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform_task',
    python_callable=transform_data,
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_task',
    python_callable=load_data,
    dag=dag,
)

# 데이터 품질 체크
quality_check = BashOperator(
    task_id='quality_check',
    bash_command='python /scripts/data_quality_check.py',
    dag=dag,
)

# Task 의존성 설정
extract_task >> transform_task >> load_task >> quality_check
```

## 적용할 때 고려했던 점

1. **Task 분리 원칙**: 하나의 Task는 하나의 책임만 가져야 함. 추출, 변환, 적재를 별도 Task로 분리하면 실패 시 재실행이 용이함.
2. **Idempotency 보장**: 같은 입력으로 여러 번 실행해도 같은 결과를 보장하도록 설계해야 함.
3. **태그 활용**: production, dev, critical 등의 태그로 DAG를 분류하면 관리가 편리함.
4. **적절한 retry 설정**: 네트워크 오류 등 일시적 장애 대응을 위해 retry를 설정하되, 무한 재시도는 지양해야함.