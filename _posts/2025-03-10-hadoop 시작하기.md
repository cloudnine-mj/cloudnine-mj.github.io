---
key: jekyll-text-theme
title: 'Hadoop ì‹œì‘í•˜ê¸°?'
excerpt: ' Hadoop ì„¤ì¹˜ ë° ì‹œì‘í•˜ê¸° ğŸ˜'
tags: [Hadoop]
---

# Hadoop ì‹œì‘í•˜ê¸°

## ì„¤ì¹˜ ë°©ë²• 1: Dockerë¡œ ê°„ë‹¨íˆ ì‹œì‘ (ê¶Œì¥)

* Dockerë¥¼ ì‚¬ìš©í•˜ë©´ ë³µì¡í•œ ì„¤ì • ì—†ì´ ë¹ ë¥´ê²Œ Hadoop í™˜ê²½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŒ.

**Docker ì„¤ì¹˜ ë° ì‹¤í–‰**


```bash
# Hadoop Docker ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
docker pull apache/hadoop:3

# Hadoop í´ëŸ¬ìŠ¤í„° ì‹¤í–‰
docker run -d \
  --name hadoop \
  -p 9870:9870 \  # HDFS Web UI
  -p 8088:8088 \  # YARN Web UI
  -p 9000:9000 \  # HDFS
  apache/hadoop:3

# ì»¨í…Œì´ë„ˆ ì ‘ì†
docker exec -it hadoop bash

# Hadoop ë²„ì „ í™•ì¸
hadoop version
```

**ì›¹ UI ì ‘ì†**

- HDFS NameNode: http://localhost:9870
- YARN ResourceManager: http://localhost:8088

## ì„¤ì¹˜ ë°©ë²• 2: Ubuntuì— ì§ì ‘ ì„¤ì¹˜

* ìš´ì˜ í™˜ê²½ì´ë‚˜ í•™ìŠµìš©ìœ¼ë¡œ ì§ì ‘ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„.

**ì‚¬ì „ ìš”êµ¬ì‚¬í•­**

- Ubuntu 20.04 ì´ìƒ
- Java 8 ë˜ëŠ” 11
- SSH ì„¤ì •
- ìµœì†Œ 4GB RAM

**1ë‹¨ê³„: Java ì„¤ì¹˜**


```bash
# Java ì„¤ì¹˜
sudo apt update
sudo apt install openjdk-11-jdk -y

# Java ë²„ì „ í™•ì¸
java -version

# JAVA_HOME í™˜ê²½ë³€ìˆ˜ ì„¤ì •
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
echo 'export PATH=$PATH:$JAVA_HOME/bin' >> ~/.bashrc
source ~/.bashrc
```

**2ë‹¨ê³„: SSH ì„¤ì •**

* Hadoopì€ ë…¸ë“œ ê°„ í†µì‹ ì„ ìœ„í•´ SSHê°€ í•„ìš”í•¨.


```bash
# SSH ì„¤ì¹˜
sudo apt install openssh-server openssh-client -y

# SSH í‚¤ ìƒì„± (íŒ¨ìŠ¤ì›Œë“œ ì—†ì´ Enter)
ssh-keygen -t rsa -P ""

# ê³µê°œí‚¤ë¥¼ authorized_keysì— ì¶”ê°€
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 640 ~/.ssh/authorized_keys

# localhost SSH ì ‘ì† í…ŒìŠ¤íŠ¸
ssh localhost
exit
```

**3ë‹¨ê³„: Hadoop ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜**


```bash
# Hadoop ë‹¤ìš´ë¡œë“œ (3.3.6 ë²„ì „ ê¸°ì¤€)
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf hadoop-3.3.6.tar.gz

# /usr/localë¡œ ì´ë™
sudo mv hadoop-3.3.6 /usr/local/hadoop

# ì†Œìœ ê¶Œ ë³€ê²½
sudo chown -R $USER:$USER /usr/local/hadoop
```

**4ë‹¨ê³„: í™˜ê²½ë³€ìˆ˜ ì„¤ì •**

```bash
# .bashrcì— í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
echo 'export HADOOP_HOME=/usr/local/hadoop' >> ~/.bashrc
echo 'export HADOOP_INSTALL=$HADOOP_HOME' >> ~/.bashrc
echo 'export HADOOP_MAPRED_HOME=$HADOOP_HOME' >> ~/.bashrc
echo 'export HADOOP_COMMON_HOME=$HADOOP_HOME' >> ~/.bashrc
echo 'export HADOOP_HDFS_HOME=$HADOOP_HOME' >> ~/.bashrc
echo 'export YARN_HOME=$HADOOP_HOME' >> ~/.bashrc
echo 'export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native' >> ~/.bashrc
echo 'export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin' >> ~/.bashrc
echo 'export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"' >> ~/.bashrc

# í™˜ê²½ë³€ìˆ˜ ì ìš©
source ~/.bashrc
```

**5ë‹¨ê³„: Hadoop ì„¤ì • íŒŒì¼ í¸ì§‘**

**hadoop-env.sh ì„¤ì •**

```bash
# hadoop-env.sh íŒŒì¼ í¸ì§‘
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# ë‹¤ìŒ ì¤„ ì¶”ê°€ ë˜ëŠ” ìˆ˜ì •
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

**core-site.xml ì„¤ì •**

```bash
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/ì‚¬ìš©ìëª…/hadoop/tmp</value>
    </property>
</configuration>
```

**hdfs-site.xml ì„¤ì •**


```bash
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/ì‚¬ìš©ìëª…/hadoop/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/ì‚¬ìš©ìëª…/hadoop/datanode</value>
    </property>
</configuration>
```

**mapred-site.xml ì„¤ì •**


```bash
nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
```


```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
```

**yarn-site.xml ì„¤ì •**


```bash
nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
```



```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

**6ë‹¨ê³„: HDFS í¬ë§· ë° ì‹œì‘**


```bash
# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/hadoop/tmp
mkdir -p ~/hadoop/namenode
mkdir -p ~/hadoop/datanode

# HDFS í¬ë§· (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)
hdfs namenode -format

# Hadoop ì‹œì‘
start-dfs.sh
start-yarn.sh

# ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ í™•ì¸
jps

# ì¶œë ¥ ì˜ˆì‹œ:
# 12345 NameNode
# 12346 DataNode
# 12347 SecondaryNameNode
# 12348 ResourceManager
# 12349 NodeManager
```

**7ë‹¨ê³„: ì„¤ì¹˜ í™•ì¸**


```bash
# Hadoop ë²„ì „ í™•ì¸
hadoop version

# HDFS ìƒíƒœ í™•ì¸
hdfs dfsadmin -report

# HDFSì— ë””ë ‰í† ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸
hdfs dfs -mkdir /test
hdfs dfs -ls /

# ì›¹ UI ì ‘ì†
# HDFS: http://localhost:9870
# YARN: http://localhost:8088
```

## ì„¤ì¹˜ ë°©ë²• 3: Docker Composeë¡œ ë©€í‹° ë…¸ë“œ í´ëŸ¬ìŠ¤í„° êµ¬ì¶•

* ì‹¤ì œ ë¶„ì‚° í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ë ¤ë©´ Docker Composeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.

**docker-compose.yml íŒŒì¼ ìƒì„±**


```yaml
version: '3'

services:
  namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
      - 9000:9000
    environment:
      - CLUSTER_NAME=hadoop-cluster
    volumes:
      - namenode:/hadoop/dfs/name

  datanode1:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    environment:
      - CLUSTER_NAME=hadoop-cluster
    volumes:
      - datanode1:/hadoop/dfs/data
    depends_on:
      - namenode

  datanode2:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    environment:
      - CLUSTER_NAME=hadoop-cluster
    volumes:
      - datanode2:/hadoop/dfs/data
    depends_on:
      - namenode

  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - 8088:8088
    depends_on:
      - namenode

  nodemanager:
    image: apache/hadoop:3
    command: ["yarn", "nodemanager"]
    depends_on:
      - resourcemanager

volumes:
  namenode:
  datanode1:
  datanode2:
```

**Docker Compose ì‹¤í–‰**


```bash
# í´ëŸ¬ìŠ¤í„° ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# í´ëŸ¬ìŠ¤í„° ì¤‘ì§€
docker-compose down

# ë³¼ë¥¨ê¹Œì§€ ì‚­ì œ (ë°ì´í„° ì™„ì „ ì‚­ì œ)
docker-compose down -v
```

## ì„¤ì¹˜ í›„ ê¸°ë³¸ ëª…ë ¹ì–´



```bash
# HDFS ë””ë ‰í† ë¦¬ ìƒì„±
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/$USER

# íŒŒì¼ ì—…ë¡œë“œ
hdfs dfs -put sample.txt /user/$USER/

# íŒŒì¼ ëª©ë¡ ë³´ê¸°
hdfs dfs -ls /user/$USER

# íŒŒì¼ ë‚´ìš© ë³´ê¸°
hdfs dfs -cat /user/$USER/sample.txt

# íŒŒì¼ ë‹¤ìš´ë¡œë“œ
hdfs dfs -get /user/$USER/sample.txt ./downloaded.txt

# íŒŒì¼ ì‚­ì œ
hdfs dfs -rm /user/$USER/sample.txt
```

## ë¬¸ì œ í•´ê²°

**í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ ê²½ìš°**


```bash
# í¬íŠ¸ ì‚¬ìš© í”„ë¡œì„¸ìŠ¤ í™•ì¸
sudo lsof -i :9870
sudo lsof -i :8088

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo kill -9 [PID]
```

**ê¶Œí•œ ë¬¸ì œ ë°œìƒ ì‹œ**


```bash
# Hadoop ë””ë ‰í† ë¦¬ ì†Œìœ ê¶Œ ë³€ê²½
sudo chown -R $USER:$USER $HADOOP_HOME
sudo chown -R $USER:$USER ~/hadoop
```

**NameNode í¬ë§·ì´ í•„ìš”í•œ ê²½ìš°**

```bash
# Hadoop ì¤‘ì§€
stop-yarn.sh
stop-dfs.sh

# ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
rm -rf ~/hadoop/namenode/*
rm -rf ~/hadoop/datanode/*
rm -rf ~/hadoop/tmp/*

# NameNode ì¬í¬ë§·
hdfs namenode -format

# Hadoop ì¬ì‹œì‘
start-dfs.sh
start-yarn.sh
```