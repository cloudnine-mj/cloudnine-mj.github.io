---
key: jekyll-text-theme
title: 'Apache Spark ì‹œì‘í•˜ê¸°'
excerpt: ' Spark study ğŸ˜'
tags: [Spark]
---

# Apache Spark ì„¤ì¹˜í•˜ê¸°

## Sparkë€?

Apache SparkëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¹ ë¥´ê³  ë²”ìš©ì ì¸ ë¶„ì‚° ì²˜ë¦¬ ì—”ì§„ì„. Hadoop MapReduceë³´ë‹¤ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²˜ë¦¬ë¡œ ìµœëŒ€ 100ë°° ë¹ ë¥¸ ì„±ëŠ¥ì„ ì œê³µí•˜ë©°, Java, Scala, Python, R ë“± ë‹¤ì–‘í•œ ì–¸ì–´ë¥¼ ì§€ì›í•¨.

**Spark vs Hadoop MapReduce**

| íŠ¹ì„±        | Spark                     | Hadoop MapReduce        |
|-------------|---------------------------|--------------------------|
| ì²˜ë¦¬ ë°©ì‹   | ë©”ëª¨ë¦¬ ê¸°ë°˜               | ë””ìŠ¤í¬ ê¸°ë°˜             |
| ì²˜ë¦¬ ì†ë„   | ë§¤ìš° ë¹ ë¦„ (10~100ë°°)      | ëŠë¦¼                     |
| í•™ìŠµ ê³¡ì„    | ì‰¬ì›€                      | ì–´ë ¤ì›€                   |
| ì‹¤ì‹œê°„ ì²˜ë¦¬ | ì§€ì› (Streaming)          | ì§€ì› ì•ˆ í•¨               |
| ë°˜ë³µ ì—°ì‚°   | ìµœì í™”ë¨ (ML)             | ë¹„íš¨ìœ¨ì                  |


## ì„¤ì¹˜ ë°©ë²• 1: Standalone ëª¨ë“œ (ë‹¨ì¼ ì„œë²„)

**ì‚¬ì „ ìš”êµ¬ì‚¬í•­**

- Java 8 ë˜ëŠ” 11
- Scala 2.12 (ì„ íƒì‚¬í•­)
- Python 3.7 ì´ìƒ (PySpark ì‚¬ìš© ì‹œ)

**1ë‹¨ê³„: Java ì„¤ì¹˜ í™•ì¸**

```bash
# Java ë²„ì „ í™•ì¸
java -version

# Javaê°€ ì—†ë‹¤ë©´ ì„¤ì¹˜
sudo apt update
sudo apt install openjdk-11-jdk -y

# JAVA_HOME ì„¤ì •
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
source ~/.bashrc
```

**2ë‹¨ê³„: Spark ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜**

```bash
# Spark ë‹¤ìš´ë¡œë“œ (3.5.0 ë²„ì „ ê¸°ì¤€)
wget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tar.gz

# ì••ì¶• í•´ì œ
tar -xzvf spark-3.5.0-bin-hadoop3.tar.gz

# /usr/localë¡œ ì´ë™
sudo mv spark-3.5.0-bin-hadoop3 /usr/local/spark

# ì†Œìœ ê¶Œ ë³€ê²½
sudo chown -R $USER:$USER /usr/local/spark
```

**3ë‹¨ê³„: í™˜ê²½ë³€ìˆ˜ ì„¤ì •**

```bash
# .bashrcì— í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
echo 'export SPARK_HOME=/usr/local/spark' >> ~/.bashrc
echo 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin' >> ~/.bashrc
echo 'export PYSPARK_PYTHON=python3' >> ~/.bashrc

# í™˜ê²½ë³€ìˆ˜ ì ìš©
source ~/.bashrc
```

**4ë‹¨ê³„: Spark ì„¤ì •**

```bash
# ì„¤ì • íŒŒì¼ ë³µì‚¬
cd $SPARK_HOME/conf
cp spark-env.sh.template spark-env.sh
cp spark-defaults.conf.template spark-defaults.conf

# spark-env.sh í¸ì§‘
nano spark-env.sh
```

**spark-env.sh ë‚´ìš© ì¶”ê°€**

```bash
#!/usr/bin/env bash

# Java ê²½ë¡œ
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Spark ë§ˆìŠ¤í„° í˜¸ìŠ¤íŠ¸
export SPARK_MASTER_HOST=localhost

# ë©”ëª¨ë¦¬ ì„¤ì •
export SPARK_WORKER_MEMORY=4g
export SPARK_DRIVER_MEMORY=2g

# ì½”ì–´ ì„¤ì •
export SPARK_WORKER_CORES=2
```

**spark-defaults.conf í¸ì§‘**

```bash
nano spark-defaults.conf
```

**spark-defaults.conf ë‚´ìš© ì¶”ê°€**

```properties
# ê¸°ë³¸ ì„¤ì •
spark.master                     spark://localhost:7077
spark.eventLog.enabled           true
spark.eventLog.dir               file:///tmp/spark-events
spark.history.fs.logDirectory    file:///tmp/spark-events

# ë©”ëª¨ë¦¬ ì„¤ì •
spark.driver.memory              2g
spark.executor.memory            4g

# íŒŒí‹°ì…˜ ì„¤ì •
spark.sql.shuffle.partitions     200
```

**5ë‹¨ê³„: Spark ì‹œì‘**

```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /tmp/spark-events

# Spark Master ì‹œì‘
start-master.sh

# Spark Worker ì‹œì‘
start-worker.sh spark://localhost:7077

# ì‹¤í–‰ í™•ì¸
jps

# ì¶œë ¥ ì˜ˆì‹œ:
# 12345 Master
# 12346 Worker
```

**6ë‹¨ê³„: ì›¹ UI ì ‘ì†**

- Spark Master UI: http://localhost:8080
- Spark Worker UI: http://localhost:8081
- Spark History Server: http://localhost:18080

## ì„¤ì¹˜ ë°©ë²• 2: YARN ìœ„ì—ì„œ ì‹¤í–‰

Hadoop YARNì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ë©´, Sparkë¥¼ YARN ìœ„ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŒ.

**ì„¤ì • ë°©ë²•**

```bash
# Hadoop ì„¤ì • ê²½ë¡œ ì¶”ê°€
echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> ~/.bashrc
echo 'export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> ~/.bashrc
source ~/.bashrc

# spark-defaults.conf ìˆ˜ì •
nano $SPARK_HOME/conf/spark-defaults.conf
```

**spark-defaults.confì— YARN ì„¤ì • ì¶”ê°€**

```properties
spark.master                     yarn
spark.submit.deployMode          client
spark.yarn.jars                  hdfs:///spark-jars/*
```

**Spark JAR íŒŒì¼ì„ HDFSì— ì—…ë¡œë“œ**

```bash
# HDFS ë””ë ‰í† ë¦¬ ìƒì„±
hdfs dfs -mkdir /spark-jars

# Spark JAR íŒŒì¼ ì—…ë¡œë“œ
hdfs dfs -put $SPARK_HOME/jars/* /spark-jars/
```

## ì„¤ì¹˜ ë°©ë²• 3: Dockerë¡œ ê°„ë‹¨íˆ ì‹œì‘

**ë‹¨ì¼ ì»¨í…Œì´ë„ˆ ì‹¤í–‰**

```bash
# Spark ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
docker pull apache/spark:3.5.0

# Spark Master ì‹¤í–‰
docker run -d \
  --name spark-master \
  -p 8080:8080 \
  -p 7077:7077 \
  apache/spark:3.5.0 \
  /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master

# Spark Worker ì‹¤í–‰
docker run -d \
  --name spark-worker \
  -p 8081:8081 \
  --link spark-master:spark-master \
  apache/spark:3.5.0 \
  /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \
  spark://spark-master:7077

# Spark Shell ì ‘ì†
docker exec -it spark-master /opt/spark/bin/spark-shell
```

**Docker Composeë¡œ í´ëŸ¬ìŠ¤í„° êµ¬ì„±**

**docker-compose.yml íŒŒì¼ ìƒì„±**

```yaml
version: '3'

services:
  spark-master:
    image: apache/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master

  spark-worker-1:
    image: apache/spark:3.5.0
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  spark-worker-2:
    image: apache/spark:3.5.0
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: spark-jupyter
    ports:
      - "8888:8888"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./notebooks:/home/jovyan/work
```

**í´ëŸ¬ìŠ¤í„° ì‹œì‘**

```bash
# í´ëŸ¬ìŠ¤í„° ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# Jupyter í† í° í™•ì¸
docker logs spark-jupyter

# í´ëŸ¬ìŠ¤í„° ì¢…ë£Œ
docker-compose down
```

## ì„¤ì¹˜ ë°©ë²• 4: PySparkë§Œ ì„¤ì¹˜ (Python í™˜ê²½)

Pythonì—ì„œë§Œ Sparkë¥¼ ì‚¬ìš©í•œë‹¤ë©´ pipë¡œ ê°„ë‹¨íˆ ì„¤ì¹˜ ê°€ëŠ¥í•¨.

```bash
# PySpark ì„¤ì¹˜
pip install pyspark

# íŠ¹ì • ë²„ì „ ì„¤ì¹˜
pip install pyspark==3.5.0

# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install pyspark[sql,ml,mllib]
```

**Pythonì—ì„œ PySpark ì‚¬ìš©**

```python
# PySpark ì„¤ì¹˜ í™•ì¸
import pyspark
print(pyspark.__version__)

# SparkSession ìƒì„±
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()

# ê°„ë‹¨í•œ ë°ì´í„° ì²˜ë¦¬
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["name", "age"])
df.show()

# Spark ì¢…ë£Œ
spark.stop()
```

## Spark Shell ì‚¬ìš©ë²•

**Scala Shell ì‹œì‘**

```bash
# Spark Shell ì‹¤í–‰ (Scala)
spark-shell

# Master ì§€ì •í•˜ì—¬ ì‹¤í–‰
spark-shell --master spark://localhost:7077

# ë©”ëª¨ë¦¬ ì„¤ì •
spark-shell --driver-memory 2g --executor-memory 4g
```

**Scalaë¡œ ê°„ë‹¨í•œ ì˜ˆì œ**

```scala
// ë°ì´í„° ìƒì„±
val data = Seq(1, 2, 3, 4, 5)
val rdd = sc.parallelize(data)

// ë³€í™˜ ë° ì•¡ì…˜
val result = rdd.map(_ * 2).collect()
println(result.mkString(", "))

// DataFrame ìƒì„±
val df = Seq(("Alice", 25), ("Bob", 30)).toDF("name", "age")
df.show()

// SQL ì¿¼ë¦¬
df.createOrReplaceTempView("people")
spark.sql("SELECT * FROM people WHERE age > 25").show()
```

**PySpark Shell ì‹œì‘**

```bash
# PySpark Shell ì‹¤í–‰
pyspark

# Master ì§€ì •í•˜ì—¬ ì‹¤í–‰
pyspark --master spark://localhost:7077

# ë©”ëª¨ë¦¬ ì„¤ì •
pyspark --driver-memory 2g --executor-memory 4g
```

**Pythonìœ¼ë¡œ ê°„ë‹¨í•œ ì˜ˆì œ**

```python
# RDD ìƒì„±
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# ë³€í™˜ ë° ì•¡ì…˜
result = rdd.map(lambda x: x * 2).collect()
print(result)

# DataFrame ìƒì„±
from pyspark.sql import Row
data = [Row(name="Alice", age=25), Row(name="Bob", age=30)]
df = spark.createDataFrame(data)
df.show()

# SQL ì¿¼ë¦¬
df.createOrReplaceTempView("people")
spark.sql("SELECT * FROM people WHERE age > 25").show()
```

## Spark Submitìœ¼ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

**Python ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰**

```bash
# ë¡œì»¬ ëª¨ë“œ
spark-submit \
  --master local[4] \
  --driver-memory 2g \
  my_app.py

# Standalone í´ëŸ¬ìŠ¤í„°
spark-submit \
  --master spark://localhost:7077 \
  --executor-memory 4g \
  --total-executor-cores 4 \
  my_app.py

# YARN í´ëŸ¬ìŠ¤í„°
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 4g \
  --num-executors 3 \
  my_app.py
```

**Scala/Java ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰**

```bash
spark-submit \
  --class com.example.MyApp \
  --master spark://localhost:7077 \
  --executor-memory 4g \
  --total-executor-cores 4 \
  my-app.jar
```

## ì„¤ì¹˜ í™•ì¸ ë° í…ŒìŠ¤íŠ¸

**WordCount ì˜ˆì œ ì‹¤í–‰**

**wordcount.py íŒŒì¼ ìƒì„±**

```python
from pyspark.sql import SparkSession

# SparkSession ìƒì„±
spark = SparkSession.builder \
    .appName("WordCount") \
    .getOrCreate()

# í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
lines = spark.read.text("sample.txt").rdd.map(lambda r: r[0])

# ë‹¨ì–´ ë¶„ë¦¬ ë° ì¹´ìš´íŠ¸
words = lines.flatMap(lambda line: line.split(" "))
word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

# ê²°ê³¼ ì¶œë ¥
results = word_counts.collect()
for word, count in results:
    print(f"{word}: {count}")

# Spark ì¢…ë£Œ
spark.stop()
```

**ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì‹¤í–‰**

```bash
# ìƒ˜í”Œ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
echo "spark hadoop spark
hadoop hive spark
spark flink hadoop" > sample.txt

# WordCount ì‹¤í–‰
spark-submit wordcount.py
```

## Spark History Server ì„¤ì •

ì‘ì—… ì‹¤í–‰ ì´ë ¥ì„ ë³´ë ¤ë©´ History Serverë¥¼ ì‹¤í–‰í•´ì•¼ í•¨.

```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /tmp/spark-events

# History Server ì‹œì‘
start-history-server.sh

# History Server ì¤‘ì§€
stop-history-server.sh

# ì›¹ UI ì ‘ì†
# http://localhost:18080
```

## ì„±ëŠ¥ íŠœë‹ ì˜µì…˜

**spark-defaults.confì—ì„œ ì„±ëŠ¥ ìµœì í™”**

```properties
# ë©”ëª¨ë¦¬ íŠœë‹
spark.executor.memory            4g
spark.driver.memory              2g
spark.memory.fraction            0.8
spark.memory.storageFraction     0.3

# ë³‘ë ¬ ì²˜ë¦¬
spark.default.parallelism        200
spark.sql.shuffle.partitions     200

# ì§ë ¬í™”
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max  256m

# ì••ì¶•
spark.rdd.compress               true
spark.shuffle.compress           true
```

### ë¬¸ì œ í•´ê²°

**ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**

```bash
# Executor ë©”ëª¨ë¦¬ ì¦ê°€
spark-submit --executor-memory 8g my_app.py

# Driver ë©”ëª¨ë¦¬ ì¦ê°€
spark-submit --driver-memory 4g my_app.py
```

**í¬íŠ¸ ì¶©ëŒ**

```bash
# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
start-master.sh --port 7078 --webui-port 8090
```

**Python ë²„ì „ ì¶©ëŒ**

```bash
# íŠ¹ì • Python ë²„ì „ ì§€ì •
export PYSPARK_PYTHON=python3.9
export PYSPARK_DRIVER_PYTHON=python3.9
```

## Spark ì¤‘ì§€

```bash
# Worker ì¤‘ì§€
stop-worker.sh

# Master ì¤‘ì§€
stop-master.sh

# ì „ì²´ ì¤‘ì§€
stop-all.sh

# History Server ì¤‘ì§€
stop-history-server.sh
```
