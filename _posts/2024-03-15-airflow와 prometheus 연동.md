---
key: jekyll-text-theme
title: 'Airflowì™€ Prometheus ì—°ë™'
excerpt: ' Airflowì™€ Prometheus ì—°ë™ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ìžë™í™”í•˜ê¸° ðŸ˜Ž'
tags: [Airflow, Prometheus]
---

# Airflowì™€ Prometheus ì—°ë™

## ê°œë…

* PrometheusëŠ” ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ì˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
* Airflowì˜ ë©”íŠ¸ë¦­ì„ Prometheusë¡œ ìˆ˜ì§‘í•˜ê³  Grafanaë¡œ ì‹œê°í™”í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìžˆìŒ.

## ì„¤ì¹˜

```bash
# Airflowì— StatsD Exporter ì„¤ì •
pip install apache-airflow[statsd]

# airflow.cfg ìˆ˜ì •
cat >> ~/airflow/airflow.cfg <<EOF
[metrics]
statsd_on = True
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow
EOF

# Prometheus StatsD Exporter ì„¤ì¹˜ (Docker)
docker run -d \
  --name statsd-exporter \
  -p 9102:9102 \
  -p 8125:9125/udp \
  prom/statsd-exporter:latest \
  --statsd.mapping-config=/tmp/statsd_mapping.yml

# Prometheus ì„¤ì¹˜
cat > prometheus.yml <<EOF
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'airflow'
    static_configs:
      - targets: ['localhost:9102']
EOF

docker run -d \
  --name prometheus \
  -p 9090:9090 \
  -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml \
  prom/prometheus:latest

# Grafana ì„¤ì¹˜
docker run -d \
  --name grafana \
  -p 3000:3000 \
  grafana/grafana:latest
```

## ì›ë¦¬

1. Airflowê°€ **StatsD í˜•ì‹**ìœ¼ë¡œ ë©”íŠ¸ë¦­ì„ ì „ì†¡
2. **StatsD Exporter**ê°€ ë©”íŠ¸ë¦­ì„ Prometheus í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•¨.
3. **Prometheus**ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ë©”íŠ¸ë¦­ì„ ìŠ¤í¬ëž©í•˜ì—¬ ì €ìž¥í•¨.
4. **Grafana**ê°€ Prometheusë¥¼ ë°ì´í„° ì†ŒìŠ¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”í•¨.

## ì½”ë“œ

* ì‹¤ë¬´ì—ì„œ í™œìš©í–ˆë˜ ì½”ë“œ

### StatsD Mapping ì„¤ì •

```yaml
# statsd_mapping.yml
mappings:
  # DAG ì‹¤í–‰ ì‹œê°„
  - match: "airflow.dag_processing.last_duration.*"
    name: "airflow_dag_processing_duration"
    labels:
      dag_file: "$1"
  
  # Task ì„±ê³µ/ì‹¤íŒ¨
  - match: "airflow.task_instance_finished.*.*.*.*"
    name: "airflow_task_finished"
    labels:
      dag_id: "$1"
      task_id: "$2"
      state: "$3"
  
  # Scheduler ë©”íŠ¸ë¦­
  - match: "airflow.scheduler.tasks.running"
    name: "airflow_scheduler_tasks_running"
  
  - match: "airflow.scheduler.tasks.starving"
    name: "airflow_scheduler_tasks_starving"
```

### Custom Metric ì¶”ê°€


```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.stats import Stats
from datetime import datetime
import time

def process_data_with_metrics(**context):
    """ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ëŠ” ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜"""
    
    # ì‹œìž‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    
    try:
        # ë°ì´í„° ì²˜ë¦¬ ë¡œì§
        records_processed = 0
        
        for i in range(1000):
            # ì‹¤ì œ ì²˜ë¦¬ ë¡œì§
            records_processed += 1
            
            # 100ê°œ ë‹¨ìœ„ë¡œ ë©”íŠ¸ë¦­ ì „ì†¡
            if records_processed % 100 == 0:
                Stats.incr('custom.records_processed', 100)
        
        # ì²˜ë¦¬ ì™„ë£Œ
        processing_time = time.time() - start_time
        Stats.timing('custom.processing_duration', processing_time)
        Stats.incr('custom.processing_success')
        
        return {'status': 'success', 'records': records_processed}
        
    except Exception as e:
        Stats.incr('custom.processing_failure')
        Stats.incr(f'custom.error.{type(e).__name__}')
        raise

dag = DAG(
    'monitored_pipeline',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@hourly',
    catchup=False,
)

task = PythonOperator(
    task_id='process_data',
    python_callable=process_data_with_metrics,
    dag=dag,
)
```

### Grafana ëŒ€ì‹œë³´ë“œ JSON


```json
{
  "dashboard": {
    "title": "Airflow Monitoring",
    "panels": [
      {
        "title": "DAG Success Rate",
        "targets": [
          {
            "expr": "sum(rate(airflow_task_finished{state='success'}[5m])) / sum(rate(airflow_task_finished[5m]))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Task Duration by DAG",
        "targets": [
          {
            "expr": "avg(airflow_dag_processing_duration) by (dag_file)"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Running Tasks",
        "targets": [
          {
            "expr": "airflow_scheduler_tasks_running"
          }
        ],
        "type": "stat"
      }
    ]
  }
}
```

## ì ìš©í•  ë•Œ ê³ ë ¤í–ˆë˜ ì 

1. **ì•ŒëžŒ ì„¤ì •**: Prometheus AlertManagerë¥¼ ì‚¬ìš©í•˜ì—¬ DAG ì‹¤íŒ¨, Task ì§€ì—° ë“±ì— ëŒ€í•œ ì•ŒëžŒì„ êµ¬ì„±í•´ì•¼ í•¨.
2. **ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­**: íšŒì‚¬ ë©”íŠ¸ë¦­ ë¡œì§ì— ë§žëŠ” ë©”íŠ¸ë¦­(ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜, API í˜¸ì¶œ ìˆ˜ ë“±)ì„ ì¶”ê°€í•´ì•¼ í•¨.
3. **ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿**: íŒ€ ë³„ë¡œ í‘œì¤€ ëŒ€ì‹œë³´ë“œ í…œí”Œë¦¿ì„ ë§Œë“¤ì–´ ì¼ê´€ì„±ì„ ìœ ì§€í•´ì•¼ í•¨.
4. **ë³´ì¡´ ì •ì±…**: Prometheus ë°ì´í„° ë³´ì¡´ ê¸°ê°„ì„ ì ì ˆížˆ ì„¤ì •í•˜ì—¬ ìŠ¤í† ë¦¬ì§€ë¥¼ ê´€ë¦¬í•´ì•¼ í•¨.
