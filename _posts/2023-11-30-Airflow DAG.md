---
key: jekyll-text-theme
title: 'Airflow DAG'
excerpt: 'DAG ì•Œì•„ë³´ê¸°ğŸ˜'
tags: [Airflow]
---


# DAG (Directed Acyclic Graph)ë€?

* DAGëŠ” ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„ë¥¼ ì˜ë¯¸í•˜ë©°, Airflowì—ì„œëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•˜ëŠ” í•µì‹¬ ê°œë…ì„. ê° ë…¸ë“œëŠ” Taskë¥¼ ë‚˜íƒ€ë‚´ê³ , ì—£ì§€ëŠ” Task ê°„ì˜ ì˜ì¡´ì„±ì„ ë‚˜íƒ€ëƒ„.

## DAGì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œ

| êµ¬ì„±ìš”ì†Œ | ì„¤ëª…                                |
| -------- | ----------------------------------- |
| DAG      | ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•˜ëŠ” ì»¨í…Œì´ë„ˆ |
| Task     | ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¨ìœ„           |
| Operator | Taskë¥¼ ìƒì„±í•˜ëŠ” í…œí”Œë¦¿              |
| Sensor   | íŠ¹ì • ì¡°ê±´ì´ ì¶©ì¡±ë  ë•Œê¹Œì§€ ëŒ€ê¸°      |
| Hook     | ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ì˜ ì—°ê²° ê´€ë¦¬           |

## 1. ê¸°ë³¸ DAG êµ¬ì¡°

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email': ['data@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# DAG ì •ì˜
with DAG(
    'data_pipeline_dag',
    default_args=default_args,
    description='ë°ì´í„° íŒŒì´í”„ë¼ì¸ DAG',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
    catchup=False,
    tags=['data', 'pipeline'],
) as dag:
    
    def extract_data():
        print("ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        return "extracted_data"
    
    def transform_data(**context):
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract')
        print(f"ë°ì´í„° ë³€í™˜ ì¤‘: {data}")
        return "transformed_data"
    
    def load_data(**context):
        ti = context['ti']
        data = ti.xcom_pull(task_ids='transform')
        print(f"ë°ì´í„° ì ì¬ ì¤‘: {data}")
    
    # Task ì •ì˜
    extract_task = PythonOperator(
        task_id='extract',
        python_callable=extract_data,
    )
    
    transform_task = PythonOperator(
        task_id='transform',
        python_callable=transform_data,
        provide_context=True,
    )
    
    load_task = PythonOperator(
        task_id='load',
        python_callable=load_data,
        provide_context=True,
    )
    
    # Task ì˜ì¡´ì„± ì„¤ì •
    extract_task >> transform_task >> load_task
```

## 2. ë‹¤ì–‘í•œ Operator í™œìš©

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime

with DAG(
    'operator_examples',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    # Bash Operator
    bash_task = BashOperator(
        task_id='run_bash_script',
        bash_command='echo "Hello from Bash" && date',
    )
    
    # Python Operator
    def python_function():
        print("Hello from Python")
    
    python_task = PythonOperator(
        task_id='run_python_function',
        python_callable=python_function,
    )
    
    # PostgreSQL Operator
    postgres_task = PostgresOperator(
        task_id='run_postgres_query',
        postgres_conn_id='postgres_default',
        sql="""
            SELECT COUNT(*) FROM users;
        """,
    )
    
    # Email Operator
    email_task = EmailOperator(
        task_id='send_email',
        to='admin@example.com',
        subject='Airflow DAG ì™„ë£Œ',
        html_content='<h3>DAG ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.</h3>',
    )
    
    bash_task >> python_task >> postgres_task >> email_task
```

## 3. Task ì˜ì¡´ì„± íŒ¨í„´

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def dummy_task():
    print("Task executed")

with DAG(
    'dependency_patterns',
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:
    
    task1 = PythonOperator(task_id='task1', python_callable=dummy_task)
    task2 = PythonOperator(task_id='task2', python_callable=dummy_task)
    task3 = PythonOperator(task_id='task3', python_callable=dummy_task)
    task4 = PythonOperator(task_id='task4', python_callable=dummy_task)
    task5 = PythonOperator(task_id='task5', python_callable=dummy_task)
    
    # ìˆœì°¨ ì‹¤í–‰
    task1 >> task2 >> task3
    
    # ë³‘ë ¬ ì‹¤í–‰ í›„ í•©ë¥˜
    task1 >> [task2, task3] >> task4
    
    # ë³µì¡í•œ ì˜ì¡´ì„±
    task1 >> task2
    task1 >> task3
    [task2, task3] >> task4 >> task5
    
    # ë˜ëŠ” set_upstream, set_downstream ì‚¬ìš©
    task1.set_downstream([task2, task3])
    task4.set_upstream([task2, task3])
```

## 4. Dynamic Task ìƒì„±

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def process_data(item):
    print(f"Processing {item}")

with DAG(
    'dynamic_tasks',
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:
    
    items = ['item1', 'item2', 'item3', 'item4', 'item5']
    
    # ë™ì ìœ¼ë¡œ Task ìƒì„±
    tasks = []
    for item in items:
        task = PythonOperator(
            task_id=f'process_{item}',
            python_callable=process_data,
            op_args=[item],
        )
        tasks.append(task)
    
    # ëª¨ë“  Taskë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
    # tasksëŠ” ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ìë™ìœ¼ë¡œ ë³‘ë ¬ ì‹¤í–‰ë¨
```

## 5. TaskGroupì„ í™œìš©í•œ Task ê·¸ë£¹í™”

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime

def dummy_task():
    print("Task executed")

with DAG(
    'taskgroup_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:
    
    start = PythonOperator(
        task_id='start',
        python_callable=dummy_task,
    )
    
    # ë°ì´í„° ì²˜ë¦¬ ê·¸ë£¹
    with TaskGroup('data_processing') as processing_group:
        extract = PythonOperator(
            task_id='extract',
            python_callable=dummy_task,
        )
        transform = PythonOperator(
            task_id='transform',
            python_callable=dummy_task,
        )
        load = PythonOperator(
            task_id='load',
            python_callable=dummy_task,
        )
        
        extract >> transform >> load
    
    # ë°ì´í„° ê²€ì¦ ê·¸ë£¹
    with TaskGroup('data_validation') as validation_group:
        validate_schema = PythonOperator(
            task_id='validate_schema',
            python_callable=dummy_task,
        )
        validate_quality = PythonOperator(
            task_id='validate_quality',
            python_callable=dummy_task,
        )
        
        [validate_schema, validate_quality]
    
    end = PythonOperator(
        task_id='end',
        python_callable=dummy_task,
    )
    
    start >> processing_group >> validation_group >> end
```

## 6. BranchPythonOperatorë¥¼ ì´ìš©í•œ ì¡°ê±´ë¶€ ì‹¤í–‰

```python
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from datetime import datetime

def check_condition(**context):
    """ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ Taskë¡œ ë¶„ê¸°"""
    execution_date = context['execution_date']
    day_of_week = execution_date.weekday()
    
    # ì›”ìš”ì¼ì´ë©´ task_a, ì•„ë‹ˆë©´ task_b
    if day_of_week == 0:
        return 'task_a'
    else:
        return 'task_b'

def task_a():
    print("Task A executed (Monday)")

def task_b():
    print("Task B executed (Not Monday)")

with DAG(
    'branch_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    branch_task = BranchPythonOperator(
        task_id='branch',
        python_callable=check_condition,
        provide_context=True,
    )
    
    task_a_op = PythonOperator(
        task_id='task_a',
        python_callable=task_a,
    )
    
    task_b_op = PythonOperator(
        task_id='task_b',
        python_callable=task_b,
    )
    
    branch_task >> [task_a_op, task_b_op]
```

## 7. Sensorë¥¼ ì´ìš©í•œ ëŒ€ê¸° ì²˜ë¦¬

```python
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.sensors.python import PythonSensor
from airflow.operators.python import PythonOperator
from datetime import datetime
import os

def check_data_ready():
    """ë°ì´í„° ì¤€ë¹„ ì—¬ë¶€ í™•ì¸"""
    # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ APIë¥¼ ì²´í¬
    return os.path.exists('/tmp/data_ready.flag')

def process_data():
    print("ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")

with DAG(
    'sensor_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@hourly',
    catchup=False,
) as dag:
    
    # íŒŒì¼ì´ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    file_sensor = FileSensor(
        task_id='wait_for_file',
        filepath='/tmp/input_data.csv',
        poke_interval=30,  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        timeout=600,  # ìµœëŒ€ 10ë¶„ ëŒ€ê¸°
        mode='poke',
    )
    
    # Python í•¨ìˆ˜ë¡œ ì¡°ê±´ ì²´í¬
    python_sensor = PythonSensor(
        task_id='wait_for_condition',
        python_callable=check_data_ready,
        poke_interval=60,
        timeout=3600,
        mode='poke',
    )
    
    process_task = PythonOperator(
        task_id='process_data',
        python_callable=process_data,
    )
    
    [file_sensor, python_sensor] >> process_task
```

## 8. Jinja í…œí”Œë¦¿ í™œìš©

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from datetime import datetime

def print_context(**context):
    """í…œí”Œë¦¿ ë³€ìˆ˜ ì¶œë ¥"""
    print(f"Execution Date: {context['ds']}")
    print(f"Previous Execution Date: {context['prev_ds']}")
    print(f"Next Execution Date: {context['next_ds']}")

with DAG(
    'template_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    # Bash ëª…ë ¹ì–´ì—ì„œ í…œí”Œë¦¿ ì‚¬ìš©
    bash_task = BashOperator(
        task_id='templated_bash',
        bash_command="""
            echo "Execution date: {{ ds }}"
            echo "Previous date: {{ prev_ds }}"
            echo "DAG: {{ dag.dag_id }}"
            echo "Task: {{ task.task_id }}"
        """,
    )
    
    # Pythonì—ì„œ í…œí”Œë¦¿ ì‚¬ìš©
    python_task = PythonOperator(
        task_id='templated_python',
        python_callable=print_context,
        provide_context=True,
    )
    
    bash_task >> python_task
```

## 9. SubDAG í™œìš©

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.subdag import SubDagOperator
from datetime import datetime

def create_subdag(parent_dag_id, child_dag_id, default_args):
    """SubDAG ìƒì„± í•¨ìˆ˜"""
    with DAG(
        dag_id=f'{parent_dag_id}.{child_dag_id}',
        default_args=default_args,
        schedule_interval=None,
        catchup=False,
    ) as subdag:
        
        def sub_task():
            print("SubDAG task executed")
        
        for i in range(3):
            PythonOperator(
                task_id=f'sub_task_{i}',
                python_callable=sub_task,
            )
    
    return subdag

# Main DAG
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 1, 1),
}

with DAG(
    'main_dag',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    start = PythonOperator(
        task_id='start',
        python_callable=lambda: print("Start"),
    )
    
    subdag_task = SubDagOperator(
        task_id='subdag',
        subdag=create_subdag('main_dag', 'subdag', default_args),
    )
    
    end = PythonOperator(
        task_id='end',
        python_callable=lambda: print("End"),
    )
    
    start >> subdag_task >> end
```

## 10. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ ì½”ë“œ

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import pandas as pd

def extract_from_source(**context):
    """ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    hook = PostgresHook(postgres_conn_id='source_db')
    
    sql = """
        SELECT * FROM sales
        WHERE created_at >= '{{ ds }}'
        AND created_at < '{{ next_ds }}'
    """
    
    df = hook.get_pandas_df(sql)
    
    # ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    filepath = f'/tmp/sales_{context["ds"]}.csv'
    df.to_csv(filepath, index=False)
    
    return filepath

def transform_data(**context):
    """ë°ì´í„° ë³€í™˜"""
    ti = context['ti']
    filepath = ti.xcom_pull(task_ids='extract')
    
    # CSV ì½ê¸°
    df = pd.read_csv(filepath)
    
    # ë°ì´í„° ë³€í™˜ ì‘ì—…
    df['total_amount'] = df['quantity'] * df['price']
    df['created_date'] = pd.to_datetime(df['created_at']).dt.date
    
    # ì§‘ê³„
    result = df.groupby('created_date').agg({
        'total_amount': 'sum',
        'quantity': 'sum'
    }).reset_index()
    
    # ë³€í™˜ëœ ë°ì´í„° ì €ì¥
    output_filepath = f'/tmp/sales_transformed_{context["ds"]}.csv'
    result.to_csv(output_filepath, index=False)
    
    return output_filepath

def load_to_warehouse(**context):
    """ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì ì¬"""
    ti = context['ti']
    filepath = ti.xcom_pull(task_ids='transform')
    
    hook = PostgresHook(postgres_conn_id='warehouse_db')
    
    # CSV ë°ì´í„°ë¥¼ ì½ì–´ì„œ ì‚½ì…
    df = pd.read_csv(filepath)
    
    for _, row in df.iterrows():
        hook.run("""
            INSERT INTO daily_sales (date, total_amount, total_quantity)
            VALUES (%s, %s, %s)
            ON CONFLICT (date) DO UPDATE
            SET total_amount = EXCLUDED.total_amount,
                total_quantity = EXCLUDED.total_quantity
        """, parameters=(row['created_date'], row['total_amount'], row['quantity']))

default_args = {
    'owner': 'data_team',
    'depends_on_past': True,
    'start_date': datetime(2024, 1, 1),
    'email': ['data@example.com'],
    'email_on_failure': True,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'daily_sales_pipeline',
    default_args=default_args,
    description='ì¼ì¼ ë§¤ì¶œ ë°ì´í„° íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 1 * * *',  # ë§¤ì¼ ì˜¤ì „ 1ì‹œ
    catchup=False,
    max_active_runs=1,
    tags=['sales', 'daily', 'etl'],
) as dag:
    
    # í…Œì´ë¸” ì¤€ë¹„
    prepare_table = PostgresOperator(
        task_id='prepare_table',
        postgres_conn_id='warehouse_db',
        sql="""
            CREATE TABLE IF NOT EXISTS daily_sales (
                date DATE PRIMARY KEY,
                total_amount DECIMAL(15, 2),
                total_quantity INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """,
    )
    
    # ETL í”„ë¡œì„¸ìŠ¤
    extract = PythonOperator(
        task_id='extract',
        python_callable=extract_from_source,
        provide_context=True,
    )
    
    transform = PythonOperator(
        task_id='transform',
        python_callable=transform_data,
        provide_context=True,
    )
    
    load = PythonOperator(
        task_id='load',
        python_callable=load_to_warehouse,
        provide_context=True,
    )
    
    # ë°ì´í„° í’ˆì§ˆ ì²´í¬
    quality_check = PostgresOperator(
        task_id='quality_check',
        postgres_conn_id='warehouse_db',
        sql="""
            SELECT COUNT(*) FROM daily_sales
            WHERE date = '{{ ds }}'
            AND total_amount > 0
        """,
    )
    
    # Task ì˜ì¡´ì„±
    prepare_table >> extract >> transform >> load >> quality_check
```

## ì°¸ê³ ì‚¬í•­

* DAGë¥¼ ì‘ì„±í•  ë•ŒëŠ” idempotent(ë©±ë“±ì„±)í•˜ê²Œ ì„¤ê³„í•˜ëŠ” ê²Œ ì¤‘ìš”í•¨. ê°™ì€ ì…ë ¥ì— ëŒ€í•´ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ë„ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼ í•˜ê³ , Task ê°„ ì˜ì¡´ì„±ì„ ëª…í™•íˆ ì •ì˜í•´ì•¼ í•¨. ë˜í•œ ë„ˆë¬´ ë§ì€ Taskë¥¼ í•˜ë‚˜ì˜ DAGì— ë„£ì§€ ë§ê³ , ì ì ˆíˆ ë¶„ë¦¬í•˜ëŠ” ê²Œ ê´€ë¦¬í•˜ê¸° ì¢‹ìŒ.