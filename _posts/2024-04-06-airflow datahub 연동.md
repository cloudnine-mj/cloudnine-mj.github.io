---
key: jekyll-text-theme
title: 'Airflow íŒŒì´í”„ë¼ì¸ê³¼ DataHub ì—°ë™í•˜ê¸°'
excerpt: ' Datahub ì—°ë™!! ğŸ˜'
tags: [Datahub]
---



# Airflow íŒŒì´í”„ë¼ì¸ê³¼ DataHub ì—°ë™í•˜ê¸°

## ê°œë…

* Airflow DAGë¥¼ DataHubì™€ ì—°ë™í•˜ë©´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³ , ë°ì´í„° ê³„ë³´(lineage)ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŒ.
* Airflowì˜ Inlet/Outletê³¼ DataHub Lineage Backendë¥¼ ì‚¬ìš©í•˜ì—¬ Task ê°„ ë°ì´í„° íë¦„ì„ ìë™ìœ¼ë¡œ ì¶”ì í•¨.

## ì„¤ì¹˜

```bash
# Airflowì— DataHub Provider ì„¤ì¹˜
pip install acryl-datahub[airflow]
pip install acryl-datahub-airflow-plugin

# ë˜ëŠ” requirements.txtì— ì¶”ê°€
cat >> requirements.txt <<EOF
acryl-datahub[airflow]==0.12.0.0
acryl-datahub-airflow-plugin==0.12.0.0
EOF

pip install -r requirements.txt
```

## Airflow ì„¤ì •

### airflow.cfg ìˆ˜ì •

```ini
[lineage]
# DataHub Lineage Backend í™œì„±í™”
backend = datahub_provider.lineage.datahub.DatahubLineageBackend

[datahub]
# DataHub ì„œë²„ ì •ë³´ (í™˜ê²½ë³€ìˆ˜ë¡œë„ ê°€ëŠ¥)
enabled = True
conn_id = datahub_rest_default
```

### Connection ì„¤ì •

```python
# Airflow UIì—ì„œ Connection ì¶”ê°€
# Admin â†’ Connections â†’ +

# ë˜ëŠ” CLIë¡œ ì¶”ê°€
airflow connections add datahub_rest_default \
    --conn-type 'datahub-rest' \
    --conn-host 'http://localhost:8080' \
    --conn-extra '{"token": ""}'

# ë˜ëŠ” Pythonìœ¼ë¡œ ì¶”ê°€
from airflow.models import Connection
from airflow.utils.db import create_session

conn = Connection(
    conn_id='datahub_rest_default',
    conn_type='datahub_rest',
    host='http://localhost:8080',
    extra='{"timeout": 30}'
)

with create_session() as session:
    session.add(conn)
    session.commit()
```

## ê¸°ë³¸ Lineage ìˆ˜ì§‘

### ì˜ˆì œ 1: Inlet/Outlet ì‚¬ìš©

~~~python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.lineage.entities import Table, File
from datetime import datetime

def extract_from_db(**context):
    """DBì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    print("Extracting data from MySQL")
    # ì‹¤ì œ ì¶”ì¶œ ë¡œì§
    return {'status': 'success', 'rows': 1000}

def transform_data(**context):
    """ë°ì´í„° ë³€í™˜"""
    print("Transforming data")
    return {'status': 'success', 'rows': 950}

def load_to_warehouse(**context):
    """ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì ì¬"""
    print("Loading to Snowflake")
    return {'status': 'success'}

dag = DAG(
    'datahub_lineage_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['datahub', 'lineage'],
)

# Task with Inlet (ì…ë ¥ ë°ì´í„°)
extract = PythonOperator(
    task_id='extract_data',
    python_callable=extract_from_db,
    # ì…ë ¥: MySQL í…Œì´ë¸”
    inlets=[
        Table(
            database='production',
            cluster='mysql_prod',
            name='raw_events',
        )
    ],
    # ì¶œë ¥: S3 íŒŒì¼
    outlets=[
        File(url='s3://data-lake/raw/events/{{ ds }}.parquet')
    ],
    dag=dag,
)

transform = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    inlets=[
        File(url='s3://data-lake/raw/events/{{ ds }}.parquet')
    ],
    outlets=[
        File(url='s3://data-lake/processed/events/{{ ds }}.parquet')
    ],
    dag=dag,
)

load = PythonOperator(
    task_id='load_to_warehouse',
    python_callable=load_to_warehouse,
    inlets=[
        File(url='s3://data-lake/processed/events/{{ ds }}.parquet')
    ],
    outlets=[
        Table(
            database='analytics',
            cluster='snowflake_prod',
            name='fact_events',
        )
    ],
    dag=dag,
)

extract >> transform >> load
```

#### DataHubì—ì„œ í™•ì¸
```
1. DataHub UI (http://localhost:9002)
2. Pipelines ë©”ë‰´ í´ë¦­
3. "datahub_lineage_example" ê²€ìƒ‰
4. Lineage íƒ­ì—ì„œ ë°ì´í„° íë¦„ í™•ì¸:

production.raw_events (MySQL)
         â†“
s3://data-lake/raw/events/*.parquet
         â†“
s3://data-lake/processed/events/*.parquet
         â†“
analytics.fact_events (Snowflake)
~~~

## ê³ ê¸‰ Lineage ì„¤ì •

### ì˜ˆì œ 2: DataHub Emitter ì§ì ‘ ì‚¬ìš©

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datahub.emitter.mce_builder import make_dataset_urn
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import DatasetLineageTypeClass, UpstreamClass, UpstreamLineageClass
from datetime import datetime

def process_with_lineage(**context):
    """Lineage ì •ë³´ë¥¼ ì§ì ‘ ì „ì†¡"""
    
    # DataHub Emitter
    emitter = DatahubRestEmitter('http://localhost:8080')
    
    # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ì…‹ (ê²°ê³¼)
    downstream_urn = make_dataset_urn(
        platform='snowflake',
        name='analytics.user_summary',
        env='PROD'
    )
    
    # ì—…ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ì…‹ (ì†ŒìŠ¤)
    upstream_urns = [
        make_dataset_urn(
            platform='mysql',
            name='production.users',
            env='PROD'
        ),
        make_dataset_urn(
            platform='mysql',
            name='production.events',
            env='PROD'
        ),
    ]
    
    # Lineage ì •ë³´ êµ¬ì„±
    upstream_lineage = UpstreamLineageClass(
        upstreams=[
            UpstreamClass(
                dataset=urn,
                type=DatasetLineageTypeClass.TRANSFORMED,
            )
            for urn in upstream_urns
        ]
    )
    
    # Lineage ì „ì†¡
    emitter.emit_mcp(
        downstream_urn,
        'upstreamLineage',
        upstream_lineage
    )
    
    print(f"Lineage registered: {len(upstream_urns)} upstreams â†’ {downstream_urn}")

dag = DAG(
    'advanced_lineage_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
)

task = PythonOperator(
    task_id='process_data',
    python_callable=process_with_lineage,
    dag=dag,
)
```

### ì˜ˆì œ 3: SQL ê¸°ë°˜ ì»¬ëŸ¼ ë ˆë²¨ Lineage

```python
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datahub.emitter.mce_builder import make_dataset_urn
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import (
    FineGrainedLineageClass,
    FineGrainedLineageDownstreamTypeClass,
    FineGrainedLineageUpstreamTypeClass,
)
from datetime import datetime

# SQL ì¿¼ë¦¬
CREATE_SUMMARY_SQL = """
CREATE TABLE analytics.user_summary AS
SELECT 
    u.user_id,
    u.name,
    u.email,
    COUNT(o.order_id) as order_count,
    SUM(o.amount) as total_spent
FROM production.users u
LEFT JOIN production.orders o ON u.user_id = o.customer_id
GROUP BY u.user_id, u.name, u.email
"""

def register_column_lineage(**context):
    """ì»¬ëŸ¼ ë ˆë²¨ Lineage ë“±ë¡"""
    
    emitter = DatahubRestEmitter('http://localhost:8080')
    
    downstream_urn = make_dataset_urn(
        platform='postgres',
        name='analytics.user_summary',
        env='PROD'
    )
    
    # ì»¬ëŸ¼ë³„ Lineage ì •ì˜
    column_lineages = [
        # user_id ì»¬ëŸ¼
        FineGrainedLineageClass(
            upstreamType=FineGrainedLineageUpstreamTypeClass.FIELD_SET,
            upstreams=[
                'urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,production.users,PROD),user_id)'
            ],
            downstreamType=FineGrainedLineageDownstreamTypeClass.FIELD,
            downstreams=[
                'urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,analytics.user_summary,PROD),user_id)'
            ],
        ),
        # order_count ì»¬ëŸ¼ (ì§‘ê³„)
        FineGrainedLineageClass(
            upstreamType=FineGrainedLineageUpstreamTypeClass.FIELD_SET,
            upstreams=[
                'urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,production.orders,PROD),order_id)'
            ],
            downstreamType=FineGrainedLineageDownstreamTypeClass.FIELD,
            downstreams=[
                'urn:li:schemaField:(urn:li:dataset:(urn:li:dataPlatform:postgres,analytics.user_summary,PROD),order_count)'
            ],
            transformOperation='COUNT',
        ),
    ]
    
    # Lineage ì „ì†¡
    for lineage in column_lineages:
        emitter.emit_mcp(downstream_urn, 'fineGrainedLineage', lineage)
    
    print("ì»¬ëŸ¼ ë ˆë²¨ Lineage ë“±ë¡ ì™„ë£Œ")

dag = DAG(
    'column_lineage_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
)

create_summary = PostgresOperator(
    task_id='create_user_summary',
    postgres_conn_id='postgres_prod',
    sql=CREATE_SUMMARY_SQL,
    dag=dag,
)

register_lineage = PythonOperator(
    task_id='register_column_lineage',
    python_callable=register_column_lineage,
    dag=dag,
)

create_summary >> register_lineage
```

## DAG ë©”íƒ€ë°ì´í„° ìë™ ìˆ˜ì§‘

### Airflow Ingestion recipe

```yaml
# airflow_recipe.yml
source:
  type: airflow
  config:
    # Airflow ì—°ê²° ì •ë³´
    conn_id: airflow_db  # ë˜ëŠ” ì§ì ‘ ì§€ì •
    # host_port: localhost:5432
    # database: airflow
    # username: airflow
    # password: airflow
    
    # ë©”íƒ€ë°ì´í„° ë² ì´ìŠ¤ URL
    webserver_url: http://localhost:8080
    
    # ìˆ˜ì§‘ ì˜µì…˜
    include_lineage: true
    capture_ownership_info: true
    capture_tags_info: true
    
    # DAG í•„í„°
    dag_pattern:
      allow:
        - "prod_.*"
        - "etl_.*"
      deny:
        - "test_.*"
        - "dev_.*"

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
```


```bash
# Ingestion ì‹¤í–‰
datahub ingest -c airflow_recipe.yml

# ìŠ¤ì¼€ì¤„ë§ (Cron)
0 */6 * * * /usr/local/bin/datahub ingest -c /path/to/airflow_recipe.yml
```

## DataHub Plugin í™œìš©

### DataHub Airflow Plugin ì„¤ì¹˜


```python
# airflow/plugins/datahub_plugin.py
from airflow.plugins_manager import AirflowPlugin
from datahub_provider.entities import Dataset, DataJob, DataFlow

class DataHubPlugin(AirflowPlugin):
    name = "datahub_plugin"
    hooks = []
    operators = []
    sensors = []
    macros = []
    executors = []
    admin_views = []
    flask_blueprints = []
    menu_links = []
```

### Custom Operator with DataHub



```python
from airflow.models import BaseOperator
from datahub.emitter.mce_builder import make_dataset_urn
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import DatasetPropertiesClass

class DataHubAwareOperator(BaseOperator):
    """DataHub ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ë“±ë¡í•˜ëŠ” Operator"""
    
    def __init__(
        self,
        input_datasets,
        output_datasets,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.input_datasets = input_datasets
        self.output_datasets = output_datasets
    
    def execute(self, context):
        # ì‹¤ì œ ì‘ì—… ìˆ˜í–‰
        result = self._do_work(context)
        
        # DataHubì— ë©”íƒ€ë°ì´í„° ë“±ë¡
        self._register_to_datahub(context, result)
        
        return result
    
    def _do_work(self, context):
        """ì‹¤ì œ ì‘ì—… ë¡œì§"""
        pass
    
    def _register_to_datahub(self, context, result):
        """DataHubì— ë©”íƒ€ë°ì´í„° ë“±ë¡"""
        emitter = DatahubRestEmitter('http://localhost:8080')
        
        for dataset_info in self.output_datasets:
            dataset_urn = make_dataset_urn(
                platform=dataset_info['platform'],
                name=dataset_info['name'],
                env='PROD'
            )
            
            properties = DatasetPropertiesClass(
                description=dataset_info.get('description', ''),
                customProperties={
                    'airflow_dag_id': context['dag'].dag_id,
                    'airflow_task_id': context['task'].task_id,
                    'execution_date': str(context['execution_date']),
                    'rows_processed': result.get('rows', 0),
                }
            )
            
            emitter.emit_mcp(dataset_urn, 'datasetProperties', properties)

# ì‚¬ìš© ì˜ˆì œ
task = DataHubAwareOperator(
    task_id='etl_task',
    input_datasets=[
        {'platform': 'mysql', 'name': 'production.orders'}
    ],
    output_datasets=[
        {
            'platform': 'snowflake',
            'name': 'analytics.daily_sales',
            'description': 'ì¼ë³„ ë§¤ì¶œ ì§‘ê³„ í…Œì´ë¸”'
        }
    ],
    dag=dag,
)
```

## ì‹¤ì‹œê°„ Lineage ì—…ë°ì´íŠ¸


```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.kafka.operators.produce import ProduceToTopicOperator
from datahub.emitter.kafka_emitter import DatahubKafkaEmitter
from datahub.metadata.schema_classes import UpstreamLineageClass
from datetime import datetime

def emit_lineage_to_kafka(**context):
    """Kafkaë¡œ Lineage ì´ë²¤íŠ¸ ì „ì†¡"""
    
    # Kafka Emitter
    emitter = DatahubKafkaEmitter(
        bootstrap='localhost:9092',
        topic='MetadataChangeProposal_v1'
    )
    
    downstream_urn = make_dataset_urn(
        platform='snowflake',
        name='analytics.real_time_metrics',
        env='PROD'
    )
    
    upstream_lineage = UpstreamLineageClass(
        upstreams=[
            UpstreamClass(
                dataset=make_dataset_urn(
                    platform='kafka',
                    name='events.user_actions',
                    env='PROD'
                ),
                type=DatasetLineageTypeClass.TRANSFORMED,
            )
        ]
    )
    
    # Kafkaë¡œ ì „ì†¡ (ë¹„ë™ê¸°)
    emitter.emit_mcp(downstream_urn, 'upstreamLineage', upstream_lineage)
    emitter.flush()
    
    print("Lineage event sent to Kafka")

dag = DAG(
    'realtime_lineage_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@hourly',
    catchup=False,
)

task = PythonOperator(
    task_id='emit_lineage',
    python_callable=emit_lineage_to_kafka,
    dag=dag,
)
```

## ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦


```python
# DAGê°€ DataHubì— ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸
from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph

def verify_dag_registration():
    """DAG ë“±ë¡ í™•ì¸"""
    
    graph = DataHubGraph(
        config=DatahubClientConfig(server='http://localhost:8080')
    )
    
    # DAG ê²€ìƒ‰
    results = graph.search(
        entity_types=['dataFlow'],
        query='datahub_lineage_example',
    )
    
    for result in results:
        print(f"Found DAG: {result.entity_urn}")
        
        # Lineage í™•ì¸
        lineage = graph.get_lineage(
            entity_urn=result.entity_urn,
            direction='DOWNSTREAM',
            max_hops=3,
        )
        
        print(f"Downstream datasets: {len(lineage.edges)}")
        for edge in lineage.edges:
            print(f"  - {edge.destinationUrn}")

verify_dag_registration()
```

## ì ìš©í•  ë•Œ ê³ ë ¤í–ˆë˜ ì 

1. **ì ì§„ì  ì ìš©**: ëª¨ë“  DAGë¥¼ í•œ ë²ˆì— ì—°ë™í•˜ì§€ ë§ê³ , ì¤‘ìš”í•œ íŒŒì´í”„ë¼ì¸ë¶€í„° ì‹œì‘
2. **Lineage ê²€ì¦**: ìë™ ìˆ˜ì§‘ëœ Lineageê°€ ì •í™•í•œì§€ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸
3. **ì„±ëŠ¥ ê³ ë ¤**: Lineage ì „ì†¡ì´ Task ì‹¤í–‰ ì‹œê°„ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë¹„ë™ê¸° ì²˜ë¦¬
4. **í‘œì¤€í™”**: ë°ì´í„°ì…‹ ì´ë¦„, í”Œë«í¼ ì´ë¦„ì„ ì¼ê´€ë˜ê²Œ ì‚¬ìš©
5. **ë¬¸ì„œí™”**: ê° Taskì˜ ì…ì¶œë ¥ ë°ì´í„°ë¥¼ ëª…í™•íˆ ë¬¸ì„œí™”
6. **ëª¨ë‹ˆí„°ë§**: DataHub Ingestion ì‹¤íŒ¨ ì‹œ ì•Œë¦¼ ì„¤ì •