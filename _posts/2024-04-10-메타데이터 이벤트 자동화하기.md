---
key: jekyll-text-theme
title: 'DataHub + Actionsë¡œ ë©”íƒ€ë°ì´í„° ì´ë²¤íŠ¸ ìë™í™”í•˜ê¸°'
excerpt: ' Datahub ì‹œì‘, ì ìš©í•´ë³´ê¸°ğŸ˜'
tags: [Datahub]
---



# 5. DataHub + Actionsë¡œ ë©”íƒ€ë°ì´í„° ì´ë²¤íŠ¸ ìë™í™”í•˜ê¸°

## ê°œë…

* DataHub ActionsëŠ” ë©”íƒ€ë°ì´í„° ë³€ê²½ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•˜ê³  ìë™ìœ¼ë¡œ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í”„ë ˆì„ì›Œí¬
* Kafkaë¥¼ í†µí•´ ì „ì†¡ë˜ëŠ” ë©”íƒ€ë°ì´í„° ì´ë²¤íŠ¸ë¥¼ êµ¬ë…í•˜ì—¬ Slack ì•Œë¦¼, Jira ì´ìŠˆ ìƒì„±, ë°ì´í„° í’ˆì§ˆ ì²´í¬ ë“±ì„ ìë™í™”í•  ìˆ˜ ìˆìŒ.

## ì„¤ì¹˜

~~~bash
# DataHub Actions ì„¤ì¹˜
pip install acryl-datahub[actions]

# ë˜ëŠ”
pip install 'acryl-datahub[action-executor]'
```

### Actions ì•„í‚¤í…ì²˜
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DataHub GMS                             â”‚
â”‚         (ë©”íƒ€ë°ì´í„° ë³€ê²½ ë°œìƒ)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Kafka Topic                          â”‚
â”‚        MetadataChangeLog_v1                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             DataHub Actions                          â”‚
â”‚              (Event Processor)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Slack  â”‚    â”‚  Jira  â”‚    â”‚ Custom â”‚
    â”‚ Notif  â”‚    â”‚ Issue  â”‚    â”‚ Action â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

## Action ì„¤ì • íŒŒì¼


```yaml
# actions_config.yml
name: "datahub_actions"

# Kafka ì—°ê²° ì„¤ì •
source:
  type: "kafka"
  config:
    bootstrap: "localhost:9092"
    schema_registry_url: "http://localhost:8081"
    consumer_group_id: "datahub-actions-consumer"

# Action ëª©ë¡
actions:
  # 1. Slack ì•Œë¦¼ - ìƒˆ ë°ì´í„°ì…‹ ìƒì„± ì‹œ
  - name: slack_new_dataset
    type: slack
    config:
      webhook_url: "${SLACK_WEBHOOK_URL}"
    event_type: EntityChangeEvent
    event:
      entity_type: dataset
      operation: CREATE
    template: |
      ğŸ†• New Dataset Created!
      *Dataset*: {{ event.entity_urn }}
      *Platform*: {{ event.platform }}
      *Created by*: {{ event.actor }}
      *Time*: {{ event.timestamp }}
  
  # 2. Slack ì•Œë¦¼ - PII íƒœê·¸ ì¶”ê°€ ì‹œ
  - name: slack_pii_tag_added
    type: slack
    config:
      webhook_url: "${SLACK_WEBHOOK_URL}"
      channel: "#data-governance"
    event_type: EntityChangeEvent
    event:
      entity_type: dataset
      category: TAG
    filter:
      tag: "urn:li:tag:PII"
    template: |
      âš ï¸ PII Tag Added
      *Dataset*: {{ event.entity_urn }}
      *Tagged by*: {{ event.actor }}
      *Action Required*: Review access controls
  
  # 3. ì´ë©”ì¼ ì•Œë¦¼ - ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œ
  - name: email_schema_change
    type: email
    config:
      smtp_host: "smtp.gmail.com"
      smtp_port: 587
      sender: "datahub@company.com"
      recipients:
        - "data-team@company.com"
    event_type: EntityChangeEvent
    event:
      entity_type: dataset
      category: SCHEMA
    template: |
      Subject: Schema Changed - {{ event.entity_urn }}
      
      Dataset: {{ event.entity_urn }}
      Changed by: {{ event.actor }}
      Time: {{ event.timestamp }}
      
      Please review the schema changes and update dependent pipelines.
```

### Actions ì‹¤í–‰

```bash
# Actions ì‹œì‘
datahub actions --config actions_config.yml

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup datahub actions --config actions_config.yml > actions.log 2>&1 &

# Dockerë¡œ ì‹¤í–‰
docker run -d \
  --name datahub-actions \
  --network datahub_network \
  -v $(pwd)/actions_config.yml:/etc/datahub-actions/config.yml \
  -e SLACK_WEBHOOK_URL="https://hooks.slack.com/..." \
  acryldata/datahub-actions:latest
```

## ì»¤ìŠ¤í…€ Action êµ¬í˜„

### ì˜ˆì œ 1: Jira ì´ìŠˆ ìë™ ìƒì„±

```python
# custom_actions/jira_action.py
from datahub.emitter.mce_builder import make_dataset_urn
from datahub_actions.action.action import Action
from datahub_actions.event.event import EventEnvelope
from jira import JIRA
import logging

logger = logging.getLogger(__name__)

class JiraIssueAction(Action):
    """ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë°œìƒ ì‹œ Jira í‹°ì¼“ ìƒì„±"""
    
    def __init__(self, config: dict):
        self.jira_url = config['jira_url']
        self.jira_user = config['jira_user']
        self.jira_token = config['jira_token']
        self.project_key = config['project_key']
        
        # Jira í´ë¼ì´ì–¸íŠ¸
        self.jira = JIRA(
            server=self.jira_url,
            basic_auth=(self.jira_user, self.jira_token)
        )
    
    def act(self, event: EventEnvelope) -> None:
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        
        # ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ í•„í„°ë§
        if not self._is_data_quality_issue(event):
            return
        
        # Jira ì´ìŠˆ ìƒì„±
        issue_dict = {
            'project': {'key': self.project_key},
            'summary': f'Data Quality Issue: {event.entity_urn}',
            'description': self._format_description(event),
            'issuetype': {'name': 'Bug'},
            'priority': {'name': self._get_priority(event)},
            'labels': ['data-quality', 'auto-created'],
        }
        
        try:
            issue = self.jira.create_issue(fields=issue_dict)
            logger.info(f"Created Jira issue: {issue.key}")
            
            # DataHubì— ì´ìŠˆ ë§í¬ ì¶”ê°€
            self._add_issue_link_to_datahub(event.entity_urn, issue.key)
            
        except Exception as e:
            logger.error(f"Failed to create Jira issue: {e}")
    
    def _is_data_quality_issue(self, event: EventEnvelope) -> bool:
        """ë°ì´í„° í’ˆì§ˆ ì´ìŠˆì¸ì§€ í™•ì¸"""
        # customPropertiesì—ì„œ quality_score í™•ì¸
        properties = event.event_data.get('customProperties', {})
        quality_score = float(properties.get('quality_score', 100))
        
        return quality_score < 80  # 80ì  ë¯¸ë§Œì´ë©´ ì´ìŠˆ
    
    def _get_priority(self, event: EventEnvelope) -> str:
        """ìš°ì„ ìˆœìœ„ ê²°ì •"""
        properties = event.event_data.get('customProperties', {})
        quality_score = float(properties.get('quality_score', 100))
        
        if quality_score < 50:
            return 'Critical'
        elif quality_score < 70:
            return 'High'
        else:
            return 'Medium'
    
    def _format_description(self, event: EventEnvelope) -> str:
        """ì´ìŠˆ ì„¤ëª… í¬ë§·"""
        properties = event.event_data.get('customProperties', {})
        
        return f"""
h2. Data Quality Issue Detected

*Dataset*: {event.entity_urn}
*Quality Score*: {properties.get('quality_score')}
*Rules Failed*: {properties.get('rules_failed')}
*Last Check*: {properties.get('last_check')}

h3. Issues
{chr(10).join('* ' + issue for issue in properties.get('issues', '').split(','))}

h3. Action Required
1. Review the data quality rules
2. Investigate root cause
3. Fix data issues or update rules
4. Re-run quality checks

[View in DataHub|http://datahub.company.com/dataset/{event.entity_urn}]
        """
    
    def _add_issue_link_to_datahub(self, dataset_urn: str, issue_key: str):
        """DataHubì— Jira ì´ìŠˆ ë§í¬ ì¶”ê°€"""
        from datahub.emitter.rest_emitter import DatahubRestEmitter
        from datahub.metadata.schema_classes import InstitutionalMemoryClass, InstitutionalMemoryMetadataClass
        
        emitter = DatahubRestEmitter('http://localhost:8080')
        
        memory = InstitutionalMemoryClass(
            elements=[
                InstitutionalMemoryMetadataClass(
                    url=f'{self.jira_url}/browse/{issue_key}',
                    description=f'Data Quality Issue: {issue_key}',
                    createStamp=None,
                )
            ]
        )
        
        emitter.emit_mcp(dataset_urn, 'institutionalMemory', memory)
    
    def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        pass

# Action ë“±ë¡
def get_action():
    return JiraIssueAction
```

## Actions ì„¤ì •ì— ì»¤ìŠ¤í…€ Action ì¶”ê°€


```yaml
# actions_config.yml (ê³„ì†)
actions:
  - name: jira_quality_issue
    type: custom
    module: custom_actions.jira_action
    class: JiraIssueAction
    config:
      jira_url: "https://company.atlassian.net"
      jira_user: "${JIRA_USER}"
      jira_token: "${JIRA_TOKEN}"
      project_key: "DQ"
    event_type: EntityChangeEvent
    event:
      entity_type: dataset
      category: CUSTOM_PROPERTIES
    filter:
      property: quality_score
```

### ì˜ˆì œ 2: ìë™ ë°ì´í„° í’ˆì§ˆ ì²´í¬

```python
# custom_actions/quality_check_action.py
from datahub_actions.action.action import Action
from datahub_actions.event.event import EventEnvelope
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import DatasetPropertiesClass
import logging

logger = logging.getLogger(__name__)

class AutoQualityCheckAction(Action):
    """ìƒˆ ë°ì´í„°ì…‹ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ í’ˆì§ˆ ì²´í¬ ì‹¤í–‰"""
    
    def __init__(self, config: dict):
        self.datahub_url = config.get('datahub_url', 'http://localhost:8080')
        self.emitter = DatahubRestEmitter(self.datahub_url)
    
    def act(self, event: EventEnvelope) -> None:
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        
        if event.event_type != 'EntityChangeEvent':
            return
        
        if event.event_data.get('operation') != 'CREATE':
            return
        
        dataset_urn = event.entity_urn
        
        logger.info(f"Running quality check for new dataset: {dataset_urn}")
        
        # ë°ì´í„° í’ˆì§ˆ ì²´í¬ ì‹¤í–‰
        quality_result = self._run_quality_checks(dataset_urn)
        
        # ê²°ê³¼ë¥¼ DataHubì— ì €ì¥
        self._save_quality_results(dataset_urn, quality_result)
        
        # í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ì†Œìœ ìì—ê²Œ ì•Œë¦¼
        if quality_result['score'] < 80:
            self._notify_owners(dataset_urn, quality_result)
    
    def _run_quality_checks(self, dataset_urn: str) -> dict:
        """ë°ì´í„° í’ˆì§ˆ ì²´í¬ ì‹¤í–‰"""
        
        # ì˜ˆì œ: ì‹¤ì œë¡œëŠ” Great Expectations, Deequ ë“± ì‚¬ìš©
        from sqlalchemy import create_engine, text
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (URNì—ì„œ íŒŒì‹±)
        platform, name, env = self._parse_urn(dataset_urn)
        
        if platform != 'mysql':
            logger.warning(f"Unsupported platform: {platform}")
            return {'score': 100, 'rules_passed': 0, 'rules_failed': 0}
        
        engine = create_engine(f'mysql://user:pass@localhost/{name.split(".")[0]}')
        table_name = name.split('.')[-1]
        
        rules_passed = 0
        rules_failed = 0
        issues = []
        
        # Rule 1: NULL ì²´í¬
        with engine.connect() as conn:
            result = conn.execute(text(f"""
                SELECT 
                    SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as null_count,
                    COUNT(*) as total_count
                FROM {table_name}
            """))
            row = result.fetchone()
            
            null_ratio = row[0] / row[1] if row[1] > 0 else 0
            if null_ratio < 0.01:  # 1% ë¯¸ë§Œ
                rules_passed += 1
            else:
                rules_failed += 1
                issues.append(f"High NULL ratio in email: {null_ratio:.2%}")
        
        # Rule 2: ì¤‘ë³µ ì²´í¬
        with engine.connect() as conn:
            result = conn.execute(text(f"""
                SELECT COUNT(*) - COUNT(DISTINCT customer_id) as duplicates
                FROM {table_name}
            """))
            duplicates = result.scalar()
            
            if duplicates == 0:
                rules_passed += 1
            else:
                rules_failed += 1
                issues.append(f"Found {duplicates} duplicate IDs")
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        total_rules = rules_passed + rules_failed
        score = (rules_passed / total_rules * 100) if total_rules > 0 else 100
        
        return {
            'score': score,
            'rules_passed': rules_passed,
            'rules_failed': rules_failed,
            'issues': issues,
        }
    
    def _save_quality_results(self, dataset_urn: str, quality_result: dict):
        """í’ˆì§ˆ ì²´í¬ ê²°ê³¼ ì €ì¥"""
        from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph
        
        graph = DataHubGraph(config=DatahubClientConfig(server=self.datahub_url))
        
        # ê¸°ì¡´ Properties ê°€ì ¸ì˜¤ê¸°
        properties = graph.get_aspect(
            entity_urn=dataset_urn,
            aspect_type=DatasetPropertiesClass,
        ) or DatasetPropertiesClass()
        
        # í’ˆì§ˆ ì •ë³´ ì¶”ê°€
        properties.customProperties = properties.customProperties or {}
        properties.customProperties.update({
            'quality_score': str(quality_result['score']),
            'rules_passed': str(quality_result['rules_passed']),
            'rules_failed': str(quality_result['rules_failed']),
            'quality_issues': ','.join(quality_result['issues']),
            'last_quality_check': datetime.now().isoformat(),
        })
        
        self.emitter.emit_mcp(dataset_urn, 'datasetProperties', properties)
        logger.info(f"Quality results saved: score={quality_result['score']}")
    
    def _notify_owners(self, dataset_urn: str, quality_result: dict):
        """ì†Œìœ ìì—ê²Œ ì•Œë¦¼"""
        import requests
        
        slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
        if not slack_webhook:
            return
        
        message = {
            'text': f"""
âš ï¸ Low Data Quality Detected

*Dataset*: {dataset_urn}
*Quality Score*: {quality_result['score']:.1f}
*Issues*:
{chr(10).join('â€¢ ' + issue for issue in quality_result['issues'])}

Please review and address these quality issues.
            """
        }
        
        requests.post(slack_webhook, json=message)
    
    def _parse_urn(self, urn: str):
        """URN íŒŒì‹±"""
        # urn:li:dataset:(urn:li:dataPlatform:mysql,db.table,PROD)
        parts = urn.split('(')[1].rstrip(')').split(',')
        platform = parts[0].split(':')[-1]
        name = parts[1]
        env = parts[2]
        return platform, name, env
    
    def close(self) -> None:
        pass

def get_action():
    return AutoQualityCheckAction
```

### ì˜ˆì œ 3: ì†Œìœ ì ìë™ í• ë‹¹

```python
# custom_actions/auto_assign_owner.py
from datahub_actions.action.action import Action
from datahub_actions.event.event import EventEnvelope
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.emitter.mce_builder import make_user_urn
from datahub.metadata.schema_classes import OwnershipClass, OwnerClass, OwnershipTypeClass
import logging

logger = logging.getLogger(__name__)

class AutoAssignOwnerAction(Action):
    """ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì†Œìœ ì ìë™ í• ë‹¹"""
    
    def __init__(self, config: dict):
        self.datahub_url = config.get('datahub_url', 'http://localhost:8080')
        self.emitter = DatahubRestEmitter(self.datahub_url)
        
        # ì†Œìœ ì í• ë‹¹ ê·œì¹™
        self.ownership_rules = config.get('rules', {})
    
    def act(self, event: EventEnvelope) -> None:
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        
        # ìƒˆë¡œ ìƒì„±ëœ ë°ì´í„°ì…‹ë§Œ ì²˜ë¦¬
        if event.event_type != 'EntityChangeEvent':
            return
        
        if event.event_data.get('operation') != 'CREATE':
            return
        
        if event.event_data.get('entity_type') != 'dataset':
            return
        
        dataset_urn = event.entity_urn
        
        # ì´ë¯¸ ì†Œìœ ìê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if self._has_owner(dataset_urn):
            logger.info(f"Dataset already has owner: {dataset_urn}")
            return
        
        # ê·œì¹™ì— ë”°ë¼ ì†Œìœ ì ê²°ì •
        owner = self._determine_owner(dataset_urn)
        
        if owner:
            self._assign_owner(dataset_urn, owner)
            logger.info(f"Auto-assigned owner {owner} to {dataset_urn}")
    
    def _has_owner(self, dataset_urn: str) -> bool:
        """ì†Œìœ ìê°€ ìˆëŠ”ì§€ í™•ì¸"""
        from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph
        
        graph = DataHubGraph(config=DatahubClientConfig(server=self.datahub_url))
        
        ownership = graph.get_aspect(
            entity_urn=dataset_urn,
            aspect_type=OwnershipClass,
        )
        
        return ownership and len(ownership.owners) > 0
    
    def _determine_owner(self, dataset_urn: str) -> str:
        """ê·œì¹™ì— ë”°ë¼ ì†Œìœ ì ê²°ì •"""
        
        # URN íŒŒì‹±
        platform, name, env = self._parse_urn(dataset_urn)
        
        # ê·œì¹™ ì ìš©
        for rule in self.ownership_rules:
            # í”Œë«í¼ ë§¤ì¹­
            if rule.get('platform') and rule['platform'] != platform:
                continue
            
            # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ì¹­
            if rule.get('database'):
                db_name = name.split('.')[0] if '.' in name else None
                if db_name != rule['database']:
                    continue
            
            # í…Œì´ë¸” íŒ¨í„´ ë§¤ì¹­
            if rule.get('table_pattern'):
                import re
                table_name = name.split('.')[-1] if '.' in name else name
                if not re.match(rule['table_pattern'], table_name):
                    continue
            
            # í™˜ê²½ ë§¤ì¹­
            if rule.get('env') and rule['env'] != env:
                continue
            
            # ê·œì¹™ ë§¤ì¹­ ì„±ê³µ
            return rule['owner']
        
        # ê¸°ë³¸ ì†Œìœ ì
        return self.ownership_rules.get('default_owner')
    
    def _assign_owner(self, dataset_urn: str, owner_username: str):
        """ì†Œìœ ì í• ë‹¹"""
        
        ownership = OwnershipClass(
            owners=[
                OwnerClass(
                    owner=make_user_urn(owner_username),
                    type=OwnershipTypeClass.DATAOWNER,
                )
            ]
        )
        
        self.emitter.emit_mcp(dataset_urn, 'ownership', ownership)
    
    def _parse_urn(self, urn: str):
        """URN íŒŒì‹±"""
        parts = urn.split('(')[1].rstrip(')').split(',')
        platform = parts[0].split(':')[-1]
        name = parts[1]
        env = parts[2]
        return platform, name, env
    
    def close(self) -> None:
        pass

def get_action():
    return AutoAssignOwnerAction
```

### ì†Œìœ ì í• ë‹¹ ê·œì¹™ ì„¤ì •

```yaml
# actions_config.yml (ê³„ì†)
actions:
  - name: auto_assign_owner
    type: custom
    module: custom_actions.auto_assign_owner
    class: AutoAssignOwnerAction
    config:
      datahub_url: "http://localhost:8080"
      rules:
        - platform: "mysql"
          database: "production"
          owner: "data-team"
        
        - platform: "snowflake"
          table_pattern: "dim_.*"
          owner: "analytics-team"
        
        - platform: "snowflake"
          table_pattern: "fact_.*"
          owner: "data-engineering"
        
        - platform: "s3"
          table_pattern: "raw/.*"
          owner: "ingestion-team"
        
        default_owner: "data-admin"
    event_type: EntityChangeEvent
    event:
      entity_type: dataset
      operation: CREATE
```

## ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§

```python
# monitor_events.py
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'MetadataChangeLog_v1',
    bootstrap_servers='localhost:9092',
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    group_id='event-monitor',
)

print("Listening for DataHub events...")

for message in consumer:
    event = message.value
    
    print(f"\n{'='*50}")
    print(f"Event Type: {event.get('changeType')}")
    print(f"Entity URN: {event.get('entityUrn')}")
    print(f"Aspect: {event.get('aspectName')}")
    print(f"Timestamp: {event.get('systemMetadata', {}).get('lastObserved')}")
    print(f"Actor: {event.get('systemMetadata', {}).get('runId')}")
    
    # Aspect ë°ì´í„° ì¶œë ¥
    if 'aspect' in event:
        print(f"Aspect Data:")
        print(json.dumps(event['aspect'], indent=2))
```

### ì ìš©í•  ë•Œ ì•Œì•„ì•¼ í•˜ëŠ” point

1. **í•„í„°ë§**: ëª¨ë“  ì´ë²¤íŠ¸ì— ë°˜ì‘í•˜ì§€ ë§ê³  í•„ìš”í•œ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§
2. **ë©±ë“±ì„±**: Actionì´ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ë˜ì–´ë„ ë¬¸ì œì—†ë„ë¡ ì„¤ê³„
3. **ì—ëŸ¬ ì²˜ë¦¬**: Action ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë° ë¡œê¹…
4. **ì„±ëŠ¥**: ë¬´ê±°ìš´ ì‘ì—…ì€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬
5. **í…ŒìŠ¤íŠ¸**: í”„ë¡œë•ì…˜ ì ìš© ì „ ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸
6. **ëª¨ë‹ˆí„°ë§**: Action ì‹¤í–‰ ë¡œê·¸ ë° ì„±ê³µ/ì‹¤íŒ¨ìœ¨ ëª¨ë‹ˆí„°ë§