---
key: jekyll-text-theme
title: '데이터 가공을 위한 Binary 전송방식'
excerpt: 'JSON, AVRO 😎'
tags: [KsqlDB, JSON/AVRO, 데이터 가공]
---

# 데이터 가공을 위한 바이너리 전송방식

* Kafka에서 데이터를 전송할 때 직렬화(serialize)를 통해 바이너리 배열로 변환해야한다. 그 이유는 어떤 값을 참조하는 주소가 담긴 변수를 저장한다고 했을 때, 다시 불러오게 된다고 해도 가리키던 값의 주소가 달라지기 때문에 의미가 없기 때문임. 그래서 참조값은 저장하거나 보낼 수 없고 값만 저장하거나 보낼 수 있다.

## JSON

* 스키마가 따로 없이 생성 가능

* 데이터를 보낼 때 전부 보내야하는 불편함이 존재

### Database의 source 테이블 생성

```
mariaDB> create table udf_test (
id int primary key auto_increment, 
c1 int, 
c2 int, 
c3 int, 
c4 int);
```

### Connector 생성

```
curl -XPOST http://192.168.2.52:30099/connectors/
{
    "name": "simple-udf",
    "config": {
        "connector.class" : "io.confluent.connect.jdbc.JdbcSourceConnector",
        "connection.url" : "jdbc:mariadb://mariadb:3306/connectTest",
        "connection.user" : "root",
        "connection.password" : "secret",
        "topic.prefix" : "simple_udf_",
        "mode" : "incrementing",
        "incrementing.column.name" : "id",
        "table.whitelist": "udf_test",
        "topic.creation.default.replication.factor" : 2,
        "topic.creation.default.partitions" : 3,
        "poll.interval.ms" : 2000,
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter.schemas.enable": "false",
        "key.converter.schemas.enable": "false",
        "max.tasks": 1
    }
}
```

###  데이터를 가져올 stream 생성

```
ksql > create stream test1(
id int key,
c1 int,
c2 int,
c3 int,
c4 int
) with (
kafka_topic='simple_udf_udf_test',
value_format='JSON');

ksql> describe  TEST1;

Name                 : TEST1
 Field | Type                   
--------------------------------
 ID    | INTEGER          (key) 
 C1    | INTEGER                
 C2    | INTEGER                
 C3    | INTEGER                
 C4    | INTEGER                
--------------------------------
```

* 다른 stream을 참조하여 UDF결과를 저장하는 stream 생성

```
#databse에 데이터 insert
MariaDB [connectTest]> insert into udf_test(c1, c2, c3, c4) values(1,2,3,4);
Query OK, 1 row affected (0.00 sec)

MariaDB [connectTest]> insert into udf_test(c1, c2, c3, c4) values(2,3,4,6);
Query OK, 1 row affected (0.02 sec)


#kafka consumer에서 data 확인
[kafka@kafka-cluster-kafka-0 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic simple_udf_udf_test --from-beginning
{"id":17,"c1":1,"c2":2,"c3":3,"c4":4}
{"id":18,"c1":2,"c2":3,"c3":4,"c4":6}

#stream에서 udf 결과 확인
ksql> create stream results AS 
>select TEST_UDF(c1, c2, c3, c4) FROM test2;

 Message                               
---------------------------------------
 Created query with ID CSAS_RESULTS_89 
---------------------------------------
ksql> select * from results EMIT CHANGES;
+-----------------------------------------------------------------------------------------------------------+
|KSQL_COL_0                                                                                                 |
+-----------------------------------------------------------------------------------------------------------+

|2.0                                                                                                        |
|4.0                                                                                                        |

Press CTRL-C to interrupt
```


## AVRO

* schema registry에 schema를 저장하고 데이터를 전송할때 schema의 id만 지정해주면 된다.

### Database의 source 테이블 생성

```
create table udf_test_avro( 
id int auto_increment primary key, 
c1 int, 
c2 int, 
c3 int, 
c4 int);
```

### Connector 생성

```
{
    "name": "simple-udf3",
    "config": {
        "connector.class" : "io.confluent.connect.jdbc.JdbcSourceConnector",
        "connection.url" : "jdbc:mariadb://mariadb:3306/connectTest",
        "connection.user" : "root",
        "connection.password" : "secret",
        "topic.prefix" : "simple_udf2_",
        "mode" : "incrementing",
        "incrementing.column.name" : "id",
        "table.whitelist": "udf_test_avro",
        "topic.creation.default.replication.factor" : 2,
        "topic.creation.default.partitions" : 3,
        "poll.interval.ms" : 2000,
        "key.converter": "io.confluent.connect.avro.AvroConverter",
        "value.converter": "io.confluent.connect.avro.AvroConverter",
        "output.format.value": "schema",
        "output.format.key": "schema",
        "key.converter.schema.registry.url": "http://schema-registry:8081",
        "value.converter.schema.registry.url": "http://schema-registry:8081",
        "value.converter.schemas.enable": "false",
        "key.converter.schemas.enable": "false",
        "max.tasks": 1
    }
}
```

### 데이터를 가져올 stream 생성

* AVRO 형식으로 통신을 하게 되면 stream의 key 가 아닌 schema 자체의 id로 구분을 하기 때문에 key를 설정해줄 필요가 없음.


```
ksql> create stream avro_test (
>id int,
>c1 int,
>c2 int,
>c3 int,
>c4 int
>) WITH (
>KAFKA_TOPIC='simple_udf2_udf_test_avro',
>VALUE_FORMAT='AVRO'
>);

ksql> describe avro_test;

Name                 : avro_test
 Field | Type                   
--------------------------------
 ID    | INTEGER          
 C1    | INTEGER                
 C2    | INTEGER                
 C3    | INTEGER                
 C4    | INTEGER                
--------------------------------
```

### 다른 stream을 참조하여 UDF결과를 저장하는 stream 생성

* Kafka consumer에서 메시지 확인 시 현재 kafka cluster에는 AVRO deserialize가 설치되어 있지않아 공백이나 특수문자로 보이게 된다.

* 위의 문제를 플러그인을 설치하기전엔 볼 순 없지만, kafka topic에서 `_schema`를 확인한다면 제대로 schema가 생성됐는지 확인할 수 있다.


```
#스키마 확인
[kafka@kafka-cluster-kafka-0 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic _schemas --from-beginning
{"subject":"test_udf_udf_test-value","version":1,"id":8,"schema":"{\"type\":\"record\",\"name\":\"udf_test\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"c1\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"c2\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"c3\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"c4\",\"type\":[\"null\",\"int\"],\"default\":null}],\"connect.name\":\"udf_test\"}","deleted":false}
{"subject":"simple_udf2_udf_test_avro-value","version":1,"id":9,"schema":"{\"type\":\"record\",\"name\":\"udf_test_avro\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"c1\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"c2\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"c3\",\"type\":[\"null\",\"int\"],\"default\":null},{\"name\":\"c4\",\"type\":[\"null\",\"int\"],\"default\":null}],\"connect.name\":\"udf_test_avro\"}","deleted":false}
{"subject":"AVRO_RESULTS-value","version":1,"id":10,"schema":"{\"type\":\"record\",\"name\":\"KsqlDataSourceSchema\",\"namespace\":\"io.confluent.ksql.avro_schemas\",\"fields\":[{\"name\":\"KSQL_COL_0\",\"type\":[\"null\",\"double\"],\"default\":null}],\"connect.name\":\"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\"}","deleted":false}
{"subject":"AVRO_RESULTS-value","version":2,"id":11,"schema":"{\"type\":\"record\",\"name\":\"KsqlDataSourceSchema\",\"namespace\":\"io.confluent.ksql.avro_schemas\",\"fields\":[{\"name\":\"KSQL_COL_0\",\"type\":[\"null\",\"double\"],\"default\":null}]}","deleted":false}


#데이터 insert
MariaDB [connectTest]> insert into udf_test_avro(c1, c2, c3, c4) values(213,23,56,78);
Query OK, 1 row affected (0.01 sec)

MariaDB [connectTest]> insert into udf_test_avro(c1, c2, c3, c4) values(213,23,59,78);
Query OK, 1 row affected (0.01 sec)


#UDF를 사용한 stream 조회
ksql> select * from avro_results EMIT CHANGES;
+------------------------------------------------------------------------------------------------------+
|KSQL_COL_0                                                                                            |
+------------------------------------------------------------------------------------------------------+
|117.0                                                                                                 |
|115.0   

```