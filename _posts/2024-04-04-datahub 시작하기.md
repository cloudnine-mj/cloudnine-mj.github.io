---
key: jekyll-text-theme
title: 'DataHub ì‹œìž‘í•˜ê¸°'
excerpt: ' Datahub ì‹œìž‘, ì ìš©í•´ë³´ê¸° ðŸ˜Ž'
tags: [Datahub]
---

# DataHub ì‹œìž‘í•˜ê¸°

## ê°œë…

* DataHubë¥¼ ë¡œì»¬ í™˜ê²½ì—ì„œ ë¹ ë¥´ê²Œ ì‹¤í–‰í•˜ì—¬ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ì„ ê²½í—˜í•  ìˆ˜ ìžˆìŒ.
* Docker Composeë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë“  êµ¬ì„± ìš”ì†Œ(GMS, Frontend, MySQL, Elasticsearch, Kafka)ë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•  ìˆ˜ ìžˆìŒ.

## ì„¤ì¹˜ ê¶Œìž¥ ì‚¬ì–‘

```bash
# í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
- Docker Desktop: 4.0.0+
- Docker Compose: 2.0.0+
- Python: 3.7+
- ìµœì†Œ ì‹œìŠ¤í…œ ì‚¬ì–‘:
  - RAM: 8GB (ê¶Œìž¥ 16GB)
  - Disk: 20GB ì—¬ìœ  ê³µê°„
  - CPU: 2ì½”ì–´ (ê¶Œìž¥ 4ì½”ì–´)
```

## ì„¤ì¹˜ ë‹¨ê³„

#### Step 1: DataHub CLI ì„¤ì¹˜

```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œìž¥)
python3 -m venv datahub-env
source datahub-env/bin/activate  # Windows: datahub-env\Scripts\activate

# DataHub CLI ì„¤ì¹˜
pip install --upgrade pip
pip install --upgrade acryl-datahub

# ì„¤ì¹˜ í™•ì¸
datahub version
# ì˜ˆìƒ ì¶œë ¥: DataHub CLI version: 0.12.0.0
```

### Step 2: DataHub ì‹¤í–‰

```bash
# Docker Composeë¡œ ëª¨ë“  ì„œë¹„ìŠ¤ ì‹œìž‘
datahub docker quickstart

# ì‹¤í–‰ë˜ëŠ” ì»´í¬ë„ŒíŠ¸:
# - datahub-gms: Metadata Service (í¬íŠ¸ 8080)
# - datahub-frontend: UI Server (í¬íŠ¸ 9002)
# - mysql: ë©”íƒ€ë°ì´í„° ì €ìž¥ì†Œ (í¬íŠ¸ 3306)
# - elasticsearch: ê²€ìƒ‰ ì—”ì§„ (í¬íŠ¸ 9200)
# - kafka-broker: ë©”ì‹œì§€ í (í¬íŠ¸ 9092)
# - schema-registry: Kafka ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ (í¬íŠ¸ 8081)

# ì§„í–‰ ìƒí™© í™•ì¸
docker ps

# ë¡œê·¸ í™•ì¸
docker logs -f datahub-gms
```

### Step 3: ì›¹ UI ì ‘ì†

~~~bash
# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
# URL: http://localhost:9002

# ê¸°ë³¸ ë¡œê·¸ì¸ ì •ë³´
# Username: datahub
# Password: datahub
```

### DataHub UI ë‘˜ëŸ¬ë³´ê¸°

**1. í™ˆíŽ˜ì´ì§€**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Search: "customer"        [ê²€ìƒ‰ ë²„íŠ¼]  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Recent Activity                        â”‚
â”‚  - sales.orders (Dataset)               â”‚
â”‚  - etl_pipeline (Data Pipeline)         â”‚
â”‚                                         â”‚
â”‚  Popular Datasets                       â”‚
â”‚  - analytics.user_events                â”‚
â”‚  - warehouse.products                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

**2. íƒìƒ‰ ë©”ë‰´**

- **Datasets**: í…Œì´ë¸”, ë·° ë“± ë°ì´í„°ì…‹ ëª©ë¡
- **Dashboards**: BI ëŒ€ì‹œë³´ë“œ
- **Pipelines**: ë°ì´í„° íŒŒì´í”„ë¼ì¸ (Airflow DAG ë“±)
- **Domains**: ë„ë©”ì¸ë³„ ë°ì´í„° ë¶„ë¥˜
- **Glossary**: ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ ì‚¬ì „


## ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘í•˜ê¸° (í…ŒìŠ¤íŠ¸ìš©)

### ì˜ˆì œ 1: CSV íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘

```bash
# ìƒ˜í”Œ CSV íŒŒì¼ ìƒì„±
cat > users.csv <<EOF
user_id,name,email,created_at
1,Alice,alice@example.com,2024-01-01
2,Bob,bob@example.com,2024-01-02
EOF

# Ingestion Recipe ìž‘ì„±
cat > csv_recipe.yml <<EOF
source:
  type: csv-enricher
  config:
    # CSV íŒŒì¼ ê²½ë¡œ
    filename: users.csv
    # DataHubì—ì„œ í‘œì‹œë  ì´ë¦„
    array_delimiter: ','
    delimiter: ','
    write_semantics: OVERRIDE

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
EOF

# ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
datahub ingest -c csv_recipe.yml

# DataHub UIì—ì„œ í™•ì¸
# 1. http://localhost:9002 ì ‘ì†
# 2. Searchì—ì„œ "users" ê²€ìƒ‰
# 3. ìŠ¤í‚¤ë§ˆ ì •ë³´ í™•ì¸
```

### ì˜ˆì œ 2: MySQL ë°ì´í„°ë² ì´ìŠ¤ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘

```bash
# MySQL ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©)
docker run -d \
  --name mysql-test \
  --network datahub_network \
  -e MYSQL_ROOT_PASSWORD=password \
  -e MYSQL_DATABASE=testdb \
  -p 3307:3306 \
  mysql:8.0

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
docker exec -it mysql-test mysql -ppassword -e "
USE testdb;
CREATE TABLE customers (
  customer_id INT PRIMARY KEY AUTO_INCREMENT,
  name VARCHAR(100) NOT NULL,
  email VARCHAR(100) UNIQUE,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE orders (
  order_id INT PRIMARY KEY AUTO_INCREMENT,
  customer_id INT,
  amount DECIMAL(10,2),
  order_date DATE,
  FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
);

INSERT INTO customers (name, email) VALUES
  ('Alice', 'alice@example.com'),
  ('Bob', 'bob@example.com');

INSERT INTO orders (customer_id, amount, order_date) VALUES
  (1, 100.50, '2024-01-01'),
  (2, 250.00, '2024-01-02');
"

# MySQL Ingestion Recipe ìž‘ì„±
cat > mysql_recipe.yml <<EOF
source:
  type: mysql
  config:
    # MySQL ì—°ê²° ì •ë³´
    host_port: localhost:3307
    database: testdb
    username: root
    password: password
    
    # ìˆ˜ì§‘ ì˜µì…˜
    include_tables: true
    include_views: true
    
    # í”„ë¡œíŒŒì¼ë§ (í†µê³„ ìˆ˜ì§‘)
    profiling:
      enabled: true
      profile_table_level_only: false
    
    # í…Œì´ë¸”ë³„ í–‰ ìˆ˜ ìˆ˜ì§‘
    table_lineage_mode: sql_based

sink:
  type: datahub-rest
  config:
    server: 'http://localhost:8080'
EOF

# ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
datahub ingest -c mysql_recipe.yml

# í™•ì¸ì‚¬í•­:
# 1. testdb.customers, testdb.orders í…Œì´ë¸” ìƒì„± í™•ì¸
# 2. ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì»¬ëŸ¼, íƒ€ìž…, PK, FK) í™•ì¸
# 3. í…Œì´ë¸” í†µê³„ (í–‰ ìˆ˜, ì»¬ëŸ¼ë³„ null ë¹„ìœ¨ ë“±) í™•ì¸
```

### ì˜ˆì œ 3: ìˆ˜ë™ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ê°€

```bash
# Pythonìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì§ì ‘ ë“±ë¡
cat > manual_ingest.py <<EOF
from datahub.emitter.mce_builder import make_dataset_urn
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import (
    DatasetPropertiesClass,
    SchemaMetadataClass,
    SchemaFieldClass,
    SchemaFieldDataTypeClass,
    StringTypeClass,
)

# Emitter ìƒì„±
emitter = DatahubRestEmitter('http://localhost:8080')

# ë°ì´í„°ì…‹ URN
dataset_urn = make_dataset_urn(
    platform='custom',
    name='my_app.user_events',
    env='PROD'
)

# Properties ì„¤ì •
properties = DatasetPropertiesClass(
    description='ì‚¬ìš©ìž ì´ë²¤íŠ¸ ë¡œê·¸ í…Œì´ë¸”',
    customProperties={
        'owner': 'data-team',
        'update_frequency': 'daily',
        'source': 'application_logs',
    }
)

# ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = SchemaMetadataClass(
    schemaName='user_events',
    platform='urn:li:dataPlatform:custom',
    version=0,
    hash='',
    platformSchema=None,
    fields=[
        SchemaFieldClass(
            fieldPath='event_id',
            type=SchemaFieldDataTypeClass(type=StringTypeClass()),
            nativeDataType='VARCHAR(50)',
            description='ì´ë²¤íŠ¸ ê³ ìœ  ID',
        ),
        SchemaFieldClass(
            fieldPath='user_id',
            type=SchemaFieldDataTypeClass(type=StringTypeClass()),
            nativeDataType='VARCHAR(50)',
            description='ì‚¬ìš©ìž ID',
        ),
        SchemaFieldClass(
            fieldPath='event_type',
            type=SchemaFieldDataTypeClass(type=StringTypeClass()),
            nativeDataType='VARCHAR(20)',
            description='ì´ë²¤íŠ¸ ìœ í˜• (click, view, purchase)',
        ),
    ],
)

# ë©”íƒ€ë°ì´í„° ì „ì†¡
emitter.emit_mcp(dataset_urn, 'datasetProperties', properties)
emitter.emit_mcp(dataset_urn, 'schemaMetadata', schema)

print(f"ë©”íƒ€ë°ì´í„° ë“±ë¡ ì™„ë£Œ: {dataset_urn}")
EOF

python manual_ingest.py
```

## DataHub ì£¼ìš” ê¸°ëŠ¥ ì‹¤ìŠµ

### 1. íƒœê·¸ ì¶”ê°€í•˜ê¸°

```bash
# UIì—ì„œ:
# 1. Datasetsì—ì„œ testdb.customers ì„ íƒ
# 2. "Add Tag" ë²„íŠ¼ í´ë¦­
# 3. "PII" íƒœê·¸ ìƒì„± ë° ì¶”ê°€
# 4. email ì»¬ëŸ¼ì—ë„ PII íƒœê·¸ ì¶”ê°€

# CLIë¡œ íƒœê·¸ ì¶”ê°€
datahub tag add --urn "urn:li:dataset:(urn:li:dataPlatform:mysql,testdb.customers,PROD)" \
  --tag PII
```

### 2. ì†Œìœ ìž ì§€ì •í•˜ê¸°


```bash
# UIì—ì„œ:
# 1. Dataset ìƒì„¸ íŽ˜ì´ì§€
# 2. "Add Owners" í´ë¦­
# 3. ì‚¬ìš©ìž ë˜ëŠ” ê·¸ë£¹ ì¶”ê°€

# APIë¡œ ì†Œìœ ìž ì¶”ê°€
cat > add_owner.py <<EOF
from datahub.emitter.mce_builder import make_dataset_urn, make_user_urn
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import (
    OwnerClass,
    OwnershipClass,
    OwnershipTypeClass,
)

emitter = DatahubRestEmitter('http://localhost:8080')

dataset_urn = make_dataset_urn(
    platform='mysql',
    name='testdb.customers',
    env='PROD'
)

ownership = OwnershipClass(
    owners=[
        OwnerClass(
            owner=make_user_urn('data-team'),
            type=OwnershipTypeClass.DATAOWNER,
        )
    ]
)

emitter.emit_mcp(dataset_urn, 'ownership', ownership)
print("ì†Œìœ ìž ì¶”ê°€ ì™„ë£Œ")
EOF

python add_owner.py
```

### 3. Glossary Term ìƒì„±


```bash
# UIì—ì„œ:
# 1. ì™¼ìª½ ë©”ë‰´ì—ì„œ "Glossary" ì„ íƒ
# 2. "Create Glossary Term" í´ë¦­
# 3. Term Name: "Customer"
#    Definition: "ì œí’ˆì„ êµ¬ë§¤í•˜ê±°ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ê°œì¸ ë˜ëŠ” ê¸°ì—…"
# 4. ì €ìž¥ í›„ testdb.customersì— ì—°ê²°
```

## Troubleshooting

**1. í¬íŠ¸ ì¶©ëŒ**

```bash
# ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ í¬íŠ¸ í™•ì¸
lsof -i :9002  # Frontend
lsof -i :8080  # GMS
lsof -i :3306  # MySQL

# í•´ê²°: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ë˜ëŠ” í¬íŠ¸ ë³€ê²½
# docker-compose.yml íŒŒì¼ ìˆ˜ì •
```

**2. ë©”ëª¨ë¦¬ ë¶€ì¡±**

```bash
# Docker Desktop ë©”ëª¨ë¦¬ í• ë‹¹ ì¦ê°€
# Settings â†’ Resources â†’ Memory: 8GB ì´ìƒ

# ë˜ëŠ” ë¶ˆí•„ìš”í•œ ì»¨í…Œì´ë„ˆ ì •ë¦¬
docker system prune -a
```

**3. Ingestion ì‹¤íŒ¨**

```bash
# ìƒì„¸ ë¡œê·¸ í™•ì¸
datahub ingest -c recipe.yml --debug

# ì—°ê²° í…ŒìŠ¤íŠ¸
telnet localhost 3307  # MySQL
curl http://localhost:8080/health  # GMS
```

## ì´í›„ ë‹¨ê³„

```bash
# DataHub ì¤‘ì§€
datahub docker stop

# DataHub ìž¬ì‹œìž‘
datahub docker start

# ì™„ì „ížˆ ì‚­ì œ (ë°ì´í„° í¬í•¨)
datahub docker nuke
```

## í…ŒìŠ¤íŠ¸ ì‹œ ê³ ë ¤í–ˆë˜ ì 

1. **ë¡œì»¬ í…ŒìŠ¤íŠ¸**: í”„ë¡œë•ì…˜ ì ìš© ì „ ë¡œì»¬ì—ì„œ ì¶©ë¶„ížˆ í…ŒìŠ¤íŠ¸
2. **Ingestion ìŠ¤ì¼€ì¤„ë§**: Airflowë‚˜ Cronìœ¼ë¡œ ì£¼ê¸°ì  ìˆ˜ì§‘ ì„¤ì •
3. **ëª¨ë‹ˆí„°ë§**: Elasticsearch, Kafka ìƒíƒœ ì£¼ê¸°ì  í™•ì¸
4. **ë°±ì—…**: MySQL ë°ì´í„° ì •ê¸° ë°±ì—…
5. **ë³´ì•ˆ**: í”„ë¡œë•ì…˜ì—ì„œëŠ” ê¸°ë³¸ ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ í•„ìˆ˜