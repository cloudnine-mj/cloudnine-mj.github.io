---
key: jekyll-text-theme
title: 'DataHubì˜ ë©”íƒ€ë°ì´í„° ëª¨ë¸ êµ¬ì¡° ì´í•´í•˜ê¸°'
excerpt: ' Datahub ì‹œì‘, ì ìš©í•´ë³´ê¸° ğŸ˜'
tags: [Datahub]
---


# DataHubì˜ ë©”íƒ€ë°ì´í„° ëª¨ë¸ êµ¬ì¡° ì´í•´í•˜ê¸°

## ê°œë…

* DataHubëŠ” Entity-Aspect ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¥¼ êµ¬ì¡°í™” í•¨.
* ì´ ëª¨ë¸ì€ ìœ ì—°í•˜ê³  í™•ì¥ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ íƒ€ì…ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³  ì¡°íšŒí•  ìˆ˜ ìˆë„ë¡ í•´ì¤Œ.

## í•µì‹¬ ê°œë… ì •ë¦¬

**1. Entity (ì—”í‹°í‹°)** ë©”íƒ€ë°ì´í„°ì˜ ì£¼ìš” ê°ì²´ë¥¼ ë‚˜íƒ€ëƒ„.

```
ì£¼ìš” Entity íƒ€ì…:
- Dataset: í…Œì´ë¸”, ë·°, íŒŒì¼ ë“±ì˜ ë°ì´í„°ì…‹
- DataJob: Airflow Task, Spark Job ë“±ì˜ ì‘ì—…
- DataFlow: Airflow DAG, Pipeline ë“±ì˜ ì›Œí¬í”Œë¡œìš°
- Dashboard: BI ëŒ€ì‹œë³´ë“œ
- Chart: ëŒ€ì‹œë³´ë“œ ë‚´ ì°¨íŠ¸
- MLModel: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
- MLFeature: í”¼ì²˜
- Container: ë°ì´í„°ë² ì´ìŠ¤, ìŠ¤í‚¤ë§ˆ ë“±ì˜ ì»¨í…Œì´ë„ˆ
- Domain: ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸
- Tag: íƒœê·¸
- GlossaryTerm: ìš©ì–´ ì‚¬ì „
- CorpUser: ì‚¬ìš©ì
- CorpGroup: ê·¸ë£¹
```

**2. Aspect (ì• ìŠ¤í™íŠ¸)** Entityì˜ íŠ¹ì • ì†ì„±ì´ë‚˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë‚˜íƒ€ëƒ„.

```
ì£¼ìš” Aspect íƒ€ì…:
- datasetProperties: ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ (ì„¤ëª…, ìƒì„±ì¼ ë“±)
- schemaMetadata: ìŠ¤í‚¤ë§ˆ ì •ë³´ (ì»¬ëŸ¼, íƒ€ì… ë“±)
- ownership: ì†Œìœ ì ì •ë³´
- globalTags: íƒœê·¸
- glossaryTerms: ìš©ì–´ ì‚¬ì „ ì—°ê²°
- upstreamLineage: ì—…ìŠ¤íŠ¸ë¦¼ ê³„ë³´
- institutionalMemory: ë¬¸ì„œ, ë§í¬
- status: ì‚­ì œ ì—¬ë¶€
- datasetProfile: í†µê³„ ì •ë³´
```

**3. URN (Uniform Resource Name)** Entityë¥¼ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ëŠ” ID

```
URN êµ¬ì¡°:
urn:li:{entityType}:({key1},{key2},...)

ì˜ˆì‹œ:
- Dataset: urn:li:dataset:(urn:li:dataPlatform:mysql,production.users,PROD)
- DataJob: urn:li:dataJob:(urn:li:dataFlow:(airflow,dag_id,PROD),task_id)
- User: urn:li:corpuser:datahub
```

## Entity-Aspect ëª¨ë¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Entity (URN)                      â”‚
â”‚  urn:li:dataset:(platform,name,env)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼             â–¼             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Aspect 1 â”‚  â”‚ Aspect 2 â”‚  â”‚ Aspect 3 â”‚  â”‚ Aspect N â”‚
  â”‚Propertiesâ”‚  â”‚ Schema   â”‚  â”‚Ownership â”‚  â”‚   ...    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Dataset Entity ìƒì„¸ êµ¬ì¡°


```python
from datahub.emitter.mce_builder import make_dataset_urn
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.metadata.schema_classes import (
    # Entity
    DatasetPropertiesClass,
    # Schema
    SchemaMetadataClass,
    SchemaFieldClass,
    SchemaFieldDataTypeClass,
    StringTypeClass,
    NumberTypeClass,
    # Ownership
    OwnershipClass,
    OwnerClass,
    OwnershipTypeClass,
    # Tags
    GlobalTagsClass,
    TagAssociationClass,
    # Lineage
    UpstreamLineageClass,
    UpstreamClass,
    DatasetLineageTypeClass,
    # Status
    StatusClass,
)

# Dataset URN ìƒì„±
dataset_urn = make_dataset_urn(
    platform='mysql',
    name='production.customers',
    env='PROD'
)

emitter = DatahubRestEmitter('http://localhost:8080')

# 1. Dataset Properties (ê¸°ë³¸ ì •ë³´)
properties = DatasetPropertiesClass(
    description='ê³ ê° ì •ë³´ í…Œì´ë¸”',
    customProperties={
        'database': 'production',
        'table_type': 'BASE TABLE',
        'created_date': '2024-01-01',
        'row_count': '1000000',
        'size_bytes': '524288000',
    },
    externalUrl='https://github.com/org/repo/schema.sql',
    tags=['production', 'pii'],
)

emitter.emit_mcp(dataset_urn, 'datasetProperties', properties)

# 2. Schema Metadata (ìŠ¤í‚¤ë§ˆ)
schema = SchemaMetadataClass(
    schemaName='customers',
    platform='urn:li:dataPlatform:mysql',
    version=0,
    hash='',
    platformSchema=None,
    fields=[
        SchemaFieldClass(
            fieldPath='customer_id',
            type=SchemaFieldDataTypeClass(type=NumberTypeClass()),
            nativeDataType='INT',
            description='ê³ ê° ê³ ìœ  ID',
            nullable=False,
            isPartOfKey=True,  # Primary Key
        ),
        SchemaFieldClass(
            fieldPath='name',
            type=SchemaFieldDataTypeClass(type=StringTypeClass()),
            nativeDataType='VARCHAR(100)',
            description='ê³ ê° ì´ë¦„',
            nullable=False,
        ),
        SchemaFieldClass(
            fieldPath='email',
            type=SchemaFieldDataTypeClass(type=StringTypeClass()),
            nativeDataType='VARCHAR(100)',
            description='ì´ë©”ì¼ ì£¼ì†Œ',
            nullable=True,
            globalTags=GlobalTagsClass(
                tags=[
                    TagAssociationClass(tag='urn:li:tag:PII')
                ]
            ),
        ),
    ],
)

emitter.emit_mcp(dataset_urn, 'schemaMetadata', schema)

# 3. Ownership (ì†Œìœ ì)
ownership = OwnershipClass(
    owners=[
        OwnerClass(
            owner='urn:li:corpuser:data-team',
            type=OwnershipTypeClass.DATAOWNER,
        ),
        OwnerClass(
            owner='urn:li:corpuser:john.doe',
            type=OwnershipTypeClass.TECHNICAL_OWNER,
        ),
    ],
    lastModified=None,
)

emitter.emit_mcp(dataset_urn, 'ownership', ownership)

# 4. Global Tags (íƒœê·¸)
tags = GlobalTagsClass(
    tags=[
        TagAssociationClass(tag='urn:li:tag:Production'),
        TagAssociationClass(tag='urn:li:tag:Critical'),
        TagAssociationClass(tag='urn:li:tag:PII'),
    ]
)

emitter.emit_mcp(dataset_urn, 'globalTags', tags)

# 5. Upstream Lineage (ê³„ë³´)
lineage = UpstreamLineageClass(
    upstreams=[
        UpstreamClass(
            dataset='urn:li:dataset:(urn:li:dataPlatform:kafka,user.events,PROD)',
            type=DatasetLineageTypeClass.TRANSFORMED,
        ),
        UpstreamClass(
            dataset='urn:li:dataset:(urn:li:dataPlatform:s3,raw/customers.csv,PROD)',
            type=DatasetLineageTypeClass.COPY,
        ),
    ]
)

emitter.emit_mcp(dataset_urn, 'upstreamLineage', lineage)
```

## DataJob Entity êµ¬ì¡°

```python
from datahub.emitter.mce_builder import make_data_job_urn, make_data_flow_urn
from datahub.metadata.schema_classes import (
    DataJobInfoClass,
    DataJobInputOutputClass,
    DataFlowInfoClass,
)

# DataFlow URN (Airflow DAG)
flow_urn = make_data_flow_urn(
    orchestrator='airflow',
    flow_id='daily_etl_pipeline',
    env='PROD'
)

# DataFlow ì •ë³´
flow_info = DataFlowInfoClass(
    name='daily_etl_pipeline',
    description='ì¼ì¼ ETL íŒŒì´í”„ë¼ì¸',
    customProperties={
        'schedule': '@daily',
        'owner': 'data-team',
        'start_date': '2024-01-01',
    },
    externalUrl='http://airflow.company.com/dags/daily_etl_pipeline',
)

emitter.emit_mcp(flow_urn, 'dataFlowInfo', flow_info)

# DataJob URN (Airflow Task)
job_urn = make_data_job_urn(
    orchestrator='airflow',
    flow_id='daily_etl_pipeline',
    job_id='extract_customers',
    env='PROD'
)

# DataJob ì •ë³´
job_info = DataJobInfoClass(
    name='extract_customers',
    description='MySQLì—ì„œ ê³ ê° ë°ì´í„° ì¶”ì¶œ',
    type='PYTHON',
    customProperties={
        'operator': 'PythonOperator',
        'retries': '3',
        'retry_delay': '300',
    },
    externalUrl='http://airflow.company.com/task/extract_customers',
)

emitter.emit_mcp(job_urn, 'dataJobInfo', job_info)

# DataJob Input/Output
job_io = DataJobInputOutputClass(
    inputDatasets=[
        'urn:li:dataset:(urn:li:dataPlatform:mysql,production.customers,PROD)'
    ],
    outputDatasets=[
        'urn:li:dataset:(urn:li:dataPlatform:s3,processed/customers.parquet,PROD)'
    ],
    inputDatajobs=[],  # ì´ì „ Task
)

emitter.emit_mcp(job_urn, 'dataJobInputOutput', job_io)
```

## ë©”íƒ€ë°ì´í„° ì¡°íšŒ

```python
from datahub.ingestion.graph.client import DatahubClientConfig, DataHubGraph

# GraphQL Client
graph = DataHubGraph(
    config=DatahubClientConfig(server='http://localhost:8080')
)

# 1. Entity ì¡°íšŒ
dataset_urn = 'urn:li:dataset:(urn:li:dataPlatform:mysql,production.customers,PROD)'

# Properties ì¡°íšŒ
properties = graph.get_aspect(
    entity_urn=dataset_urn,
    aspect_type=DatasetPropertiesClass,
)
print(f"Description: {properties.description}")
print(f"Custom Properties: {properties.customProperties}")

# Schema ì¡°íšŒ
schema = graph.get_aspect(
    entity_urn=dataset_urn,
    aspect_type=SchemaMetadataClass,
)
print(f"Schema has {len(schema.fields)} fields:")
for field in schema.fields:
    print(f"  - {field.fieldPath}: {field.nativeDataType}")

# Ownership ì¡°íšŒ
ownership = graph.get_aspect(
    entity_urn=dataset_urn,
    aspect_type=OwnershipClass,
)
print(f"Owners:")
for owner in ownership.owners:
    print(f"  - {owner.owner} ({owner.type})")

# 2. Lineage ì¡°íšŒ
lineage = graph.get_lineage(
    entity_urn=dataset_urn,
    direction='UPSTREAM',  # ë˜ëŠ” 'DOWNSTREAM'
    max_hops=3,
)

print(f"Upstream datasets:")
for edge in lineage.edges:
    print(f"  - {edge.sourceUrn}")

# 3. ê²€ìƒ‰
search_results = graph.search(
    entity_types=['dataset'],
    query='customers',
    start=0,
    count=10,
    filters=[
        {
            'field': 'platform',
            'value': 'urn:li:dataPlatform:mysql',
        }
    ],
)

for result in search_results:
    print(f"Found: {result.entity_urn}")
```

## ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸

```python
# 1. ë¶€ë¶„ ì—…ë°ì´íŠ¸ (Aspect ë‹¨ìœ„)
def update_description(dataset_urn, new_description):
    """ì„¤ëª…ë§Œ ì—…ë°ì´íŠ¸"""
    
    # ê¸°ì¡´ Properties ì¡°íšŒ
    properties = graph.get_aspect(
        entity_urn=dataset_urn,
        aspect_type=DatasetPropertiesClass,
    )
    
    # ì„¤ëª… ìˆ˜ì •
    properties.description = new_description
    
    # ì—…ë°ì´íŠ¸
    emitter.emit_mcp(dataset_urn, 'datasetProperties', properties)

update_description(dataset_urn, 'ê³ ê° ë§ˆìŠ¤í„° ë°ì´í„°')

# 2. í•„ë“œ ì¶”ê°€
def add_schema_field(dataset_urn, field_name, field_type, description):
    """ìŠ¤í‚¤ë§ˆì— ìƒˆ í•„ë“œ ì¶”ê°€"""
    
    schema = graph.get_aspect(
        entity_urn=dataset_urn,
        aspect_type=SchemaMetadataClass,
    )
    
    new_field = SchemaFieldClass(
        fieldPath=field_name,
        type=SchemaFieldDataTypeClass(type=StringTypeClass()),
        nativeDataType=field_type,
        description=description,
    )
    
    schema.fields.append(new_field)
    
    emitter.emit_mcp(dataset_urn, 'schemaMetadata', schema)

add_schema_field(
    dataset_urn,
    'created_at',
    'TIMESTAMP',
    'ë ˆì½”ë“œ ìƒì„± ì‹œê°„'
)

# 3. íƒœê·¸ ì¶”ê°€
def add_tag(dataset_urn, tag_name):
    """íƒœê·¸ ì¶”ê°€"""
    
    tags = graph.get_aspect(
        entity_urn=dataset_urn,
        aspect_type=GlobalTagsClass,
    ) or GlobalTagsClass(tags=[])
    
    tag_urn = f'urn:li:tag:{tag_name}'
    
    # ì¤‘ë³µ í™•ì¸
    existing_tags = {tag.tag for tag in tags.tags}
    if tag_urn not in existing_tags:
        tags.tags.append(TagAssociationClass(tag=tag_urn))
        emitter.emit_mcp(dataset_urn, 'globalTags', tags)

add_tag(dataset_urn, 'Verified')
```

## ë©”íƒ€ë°ì´í„° ì‚­ì œ


```python
# 1. Soft Delete (ê¶Œì¥)
from datahub.metadata.schema_classes import StatusClass

def soft_delete_dataset(dataset_urn):
    """ë°ì´í„°ì…‹ì„ Soft Delete"""
    
    status = StatusClass(removed=True)
    emitter.emit_mcp(dataset_urn, 'status', status)
    
    print(f"Soft deleted: {dataset_urn}")

soft_delete_dataset(dataset_urn)

# 2. Hard Delete (ì£¼ì˜)
def hard_delete_dataset(dataset_urn):
    """ë°ì´í„°ì…‹ì„ ì™„ì „íˆ ì‚­ì œ"""
    
    from datahub.emitter.mce_builder import make_dataset_urn
    from datahub.metadata.com.linkedin.pegasus2avro.mxe import (
        MetadataChangeEvent,
        SystemMetadata,
    )
    
    # Hard deleteëŠ” REST API ì§ì ‘ í˜¸ì¶œ í•„ìš”
    import requests
    
    response = requests.delete(
        f'http://localhost:8080/entities?action=delete',
        json={'urn': dataset_urn},
    )
    
    print(f"Hard deleted: {dataset_urn}")

# ì£¼ì˜: Hard DeleteëŠ” ë³µêµ¬ ë¶ˆê°€ëŠ¥
```

## ì»¤ìŠ¤í…€ Entity ë° Aspect

```python
# ì»¤ìŠ¤í…€ Aspect ì •ì˜ (YAML)
"""
# custom_aspect.pdl
namespace com.company.metadata.aspect

record CustomDataQualityClass {
  score: float
  rules_passed: int
  rules_failed: int
  last_check_time: string
  issues: array[string]
}
"""

# ì‚¬ìš© ì˜ˆì œ
from dataclasses import dataclass

@dataclass
class CustomDataQualityClass:
    score: float
    rules_passed: int
    rules_failed: int
    last_check_time: str
    issues: list

def add_data_quality_metrics(dataset_urn, quality_data):
    """ì»¤ìŠ¤í…€ ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ê°€"""
    
    # ì»¤ìŠ¤í…€ Aspectë¥¼ customPropertiesë¡œ ì €ì¥
    properties = graph.get_aspect(
        entity_urn=dataset_urn,
        aspect_type=DatasetPropertiesClass,
    )
    
    properties.customProperties.update({
        'quality_score': str(quality_data.score),
        'rules_passed': str(quality_data.rules_passed),
        'rules_failed': str(quality_data.rules_failed),
        'last_check': quality_data.last_check_time,
    })
    
    emitter.emit_mcp(dataset_urn, 'datasetProperties', properties)

quality_data = CustomDataQualityClass(
    score=95.5,
    rules_passed=18,
    rules_failed=2,
    last_check_time='2024-11-28T10:00:00Z',
    issues=['NULL values in email column', 'Duplicate IDs found'],
)

add_data_quality_metrics(dataset_urn, quality_data)
```

## ë©”íƒ€ë°ì´í„° ë²„ì „ ê´€ë¦¬


```python
# Aspectì—ëŠ” versionì´ ìˆì–´ ë³€ê²½ ì´ë ¥ ì¶”ì  ê°€ëŠ¥
def get_aspect_history(dataset_urn, aspect_name):
    """Aspect ë³€ê²½ ì´ë ¥ ì¡°íšŒ"""
    
    import requests
    
    response = requests.get(
        f'http://localhost:8080/aspects/{dataset_urn}',
        params={
            'aspect': aspect_name,
            'version': 'all',  # ëª¨ë“  ë²„ì „
        }
    )
    
    versions = response.json()
    
    for i, version in enumerate(versions):
        print(f"Version {i}:")
        print(f"  Created: {version['created']}")
        print(f"  Data: {version['data']}")

# Properties ë³€ê²½ ì´ë ¥ í™•ì¸
get_aspect_history(dataset_urn, 'datasetProperties')
```

## ì—…ë¬´ì— ì ìš©í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ì 

1. **URN í‘œì¤€í™”**: ì¡°ì§ ë‚´ URN ëª…ëª… ê·œì¹™ì„ ì •í•˜ê³  ì¼ê´€ë˜ê²Œ ì‚¬ìš©
2. **Aspect ìµœì†Œí™”**: í•„ìš”í•œ Aspectë§Œ ì‚¬ìš©í•˜ì—¬ ë³µì¡ë„ ê°ì†Œ
3. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ Entity ì—…ë°ì´íŠ¸ ì‹œ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
4. **Soft Delete ìš°ì„ **: Hard DeleteëŠ” ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ì‚¬ìš©
5. **ë²„ì „ ê´€ë¦¬**: ì¤‘ìš”í•œ ë©”íƒ€ë°ì´í„°ëŠ” ë³€ê²½ ì´ë ¥ì„ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸
6. **ì»¤ìŠ¤í…€ í™•ì¥**: í‘œì¤€ Aspectë¡œ ë¶€ì¡±í•˜ë©´ customProperties í™œìš©
