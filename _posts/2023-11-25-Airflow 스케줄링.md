---
key: jekyll-text-theme
title: 'Airflow 스케줄링'
excerpt: 'Airflow 스케줄링 알아보기 😎'
tags: [Airflow]
---


# Airflow 스케줄링

* Airflow는 cron 기반의 스케줄링을 지원하며, 데이터 파이프라인의 실행 시간, 재실행, 의존성 관리를 자동화할 수 있음. 복잡한 데이터 파이프라인도 효율적으로 관리할 수 있음.

## 스케줄링 주요 개념

| 개념              | 설명                                   |
| ----------------- | -------------------------------------- |
| schedule_interval | DAG 실행 주기 정의                     |
| start_date        | DAG 시작 날짜                          |
| end_date          | DAG 종료 날짜 (선택)                   |
| catchup           | 과거 실행 건 자동 실행 여부            |
| execution_date    | 실제 실행 시간이 아닌 데이터 기준 시간 |

## 1. 다양한 스케줄 설정

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def dummy_task():
    print("Task executed")

# 1. Cron 표현식 사용
dag_cron = DAG(
    'cron_schedule',
    start_date=datetime(2024, 1, 1),
    schedule_interval='0 9 * * 1-5',  # 평일 오전 9시
    catchup=False,
)

# 2. timedelta 사용
dag_timedelta = DAG(
    'timedelta_schedule',
    start_date=datetime(2024, 1, 1),
    schedule_interval=timedelta(hours=6),  # 6시간마다
    catchup=False,
)

# 3. Preset 사용
dag_preset = DAG(
    'preset_schedule',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@hourly',  # 매시간
    catchup=False,
)

# 4. 수동 실행만 허용
dag_manual = DAG(
    'manual_schedule',
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,  # 수동 실행만
    catchup=False,
)
```

**[참고 - Schedule Preset]**

| Preset   | 설명         | Cron 표현식 |
| -------- | ------------ | ----------- |
| None     | 수동 실행만  | -           |
| @once    | 한 번만 실행 | -           |
| @hourly  | 매시간       | 0 * * * *   |
| @daily   | 매일 자정    | 0 0 * * *   |
| @weekly  | 매주 일요일  | 0 0 * * 0   |
| @monthly | 매월 1일     | 0 0 1 * *   |
| @yearly  | 매년 1월 1일 | 0 0 1 1 *   |

## 2. Catchup 설정

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def process_data(**context):
    execution_date = context['execution_date']
    print(f"Processing data for {execution_date}")

# Catchup True - 과거 실행 건 모두 실행
dag_with_catchup = DAG(
    'with_catchup',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=True,  # 과거 누락된 실행 건 모두 실행
    max_active_runs=1,  # 동시 실행 제한
)

# Catchup False - 최신 실행 건만 실행
dag_without_catchup = DAG(
    'without_catchup',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,  # 과거 누락 건 무시
)

task = PythonOperator(
    task_id='process',
    python_callable=process_data,
    provide_context=True,
    dag=dag_without_catchup,
)
```

## 3. 의존성 있는 스케줄링

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor
from datetime import datetime, timedelta

# 선행 DAG
with DAG(
    'upstream_dag',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
) as upstream_dag:
    
    def upstream_task():
        print("Upstream task completed")
    
    task = PythonOperator(
        task_id='upstream_task',
        python_callable=upstream_task,
    )

# 후행 DAG - 선행 DAG 완료 후 실행
with DAG(
    'downstream_dag',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
) as downstream_dag:
    
    # 선행 DAG 완료 대기
    wait_for_upstream = ExternalTaskSensor(
        task_id='wait_for_upstream',
        external_dag_id='upstream_dag',
        external_task_id='upstream_task',
        timeout=600,
        poke_interval=60,
        mode='poke',
    )
    
    def downstream_task():
        print("Downstream task executed")
    
    process = PythonOperator(
        task_id='downstream_task',
        python_callable=downstream_task,
    )
    
    wait_for_upstream >> process
```

## 4. 데이터 기준 날짜 활용

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def process_daily_data(**context):
    """데이터 기준 날짜로 처리"""
    # execution_date는 데이터 기준 날짜
    execution_date = context['execution_date']
    ds = context['ds']  # YYYY-MM-DD 형식
    
    print(f"Processing data for date: {ds}")
    print(f"Execution datetime: {execution_date}")
    
    # 이전/다음 날짜 접근
    prev_ds = context['prev_ds']
    next_ds = context['next_ds']
    
    print(f"Previous date: {prev_ds}")
    print(f"Next date: {next_ds}")

with DAG(
    'data_date_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    task = PythonOperator(
        task_id='process_data',
        python_callable=process_daily_data,
        provide_context=True,
    )
```

## 5. 시간대(Timezone) 설정

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import pendulum

# 한국 시간대 설정
kst = pendulum.timezone('Asia/Seoul')

with DAG(
    'timezone_example',
    start_date=datetime(2024, 1, 1, tzinfo=kst),
    schedule_interval='0 9 * * *',  # 한국 시간 오전 9시
    catchup=False,
    default_args={
        'owner': 'airflow',
    }
) as dag:
    
    def print_time(**context):
        execution_date = context['execution_date']
        print(f"Execution time (KST): {execution_date}")
    
    task = PythonOperator(
        task_id='print_time',
        python_callable=print_time,
        provide_context=True,
    )
```

## 6. 모니터링 - Web UI 활용

Airflow Web UI를 통해 다양한 정보를 모니터링할 수 있음.

```python
# DAG에 태그 추가로 필터링 용이
from airflow import DAG
from datetime import datetime

dag = DAG(
    'monitored_dag',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['production', 'critical', 'sales'],  # 태그로 분류
    description='중요한 매출 데이터 파이프라인',
    default_args={
        'owner': 'data_team',
        'email': ['data@example.com'],
        'email_on_failure': True,
        'email_on_retry': False,
    }
)
```

**[주요 모니터링 뷰]**

| 뷰            | 설명                     |
| ------------- | ------------------------ |
| DAGs          | 전체 DAG 목록 및 상태    |
| Grid          | Task별 실행 이력 그리드  |
| Graph         | DAG의 Task 의존성 그래프 |
| Calendar      | 월별 실행 성공/실패 현황 |
| Task Duration | Task별 실행 시간 추이    |
| Gantt         | Task 실행 타임라인       |

## 7. 로그 모니터링 및 관리

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import logging

def task_with_logging(**context):
    """로깅을 포함한 Task"""
    # Airflow 로거 사용
    logger = logging.getLogger(__name__)
    
    logger.info("Task 시작")
    logger.info(f"Execution date: {context['ds']}")
    
    try:
        # 작업 수행
        result = 100 / 10
        logger.info(f"계산 결과: {result}")
        
    except Exception as e:
        logger.error(f"에러 발생: {str(e)}")
        raise
    
    logger.info("Task 완료")

with DAG(
    'logging_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:
    
    task = PythonOperator(
        task_id='task_with_logging',
        python_callable=task_with_logging,
        provide_context=True,
    )
```

## 8. 알림 설정

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from datetime import datetime

def send_slack_notification(context):
    """Slack 알림 전송"""
    from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
    
    slack_hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
    
    task_instance = context['task_instance']
    dag_id = context['dag'].dag_id
    task_id = task_instance.task_id
    execution_date = context['execution_date']
    
    message = f"""
    :red_circle: *Task 실패*
    *DAG*: {dag_id}
    *Task*: {task_id}
    *실행 날짜*: {execution_date}
    """
    
    slack_hook.send(text=message)

def on_success_callback(context):
    """성공 시 콜백"""
    print(f"Task {context['task_instance'].task_id} 성공!")

def on_failure_callback(context):
    """실패 시 콜백"""
    print(f"Task {context['task_instance'].task_id} 실패!")
    send_slack_notification(context)

with DAG(
    'notification_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    default_args={
        'email': ['admin@example.com'],
        'email_on_failure': True,
        'email_on_retry': False,
        'on_success_callback': on_success_callback,
        'on_failure_callback': on_failure_callback,
    }
) as dag:
    
    def task_function():
        print("Task executed")
    
    task = PythonOperator(
        task_id='monitored_task',
        python_callable=task_function,
    )
```

## 9. SLA (Service Level Agreement) 모니터링

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def sla_miss_callback(dag, task_list, blocking_task_list, slas, blocking_tis):
    """SLA 위반 시 콜백"""
    print("SLA 위반 발생!")
    for task in task_list:
        print(f"Task {task.task_id}가 SLA를 위반했습니다.")

def slow_task():
    """느린 Task"""
    import time
    time.sleep(10)
    print("Task completed")

with DAG(
    'sla_example',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    sla_miss_callback=sla_miss_callback,
) as dag:
    
    task = PythonOperator(
        task_id='slow_task',
        python_callable=slow_task,
        sla=timedelta(seconds=5),  # 5초 이내 완료되어야 함
    )
```

## 10. 실전 모니터링 시스템 구축

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import json

def collect_dag_metrics(**context):
    """DAG 실행 메트릭 수집"""
    from airflow.models import DagRun, TaskInstance
    from airflow.utils.session import provide_session
    
    @provide_session
    def get_metrics(session=None):
        execution_date = context['execution_date']
        
        # 최근 24시간 DAG 실행 통계
        start_time = execution_date - timedelta(days=1)
        
        dag_runs = session.query(DagRun).filter(
            DagRun.execution_date >= start_time,
            DagRun.execution_date <= execution_date
        ).all()
        
        metrics = {
            'total_runs': len(dag_runs),
            'success': sum(1 for dr in dag_runs if dr.state == 'success'),
            'failed': sum(1 for dr in dag_runs if dr.state == 'failed'),
            'running': sum(1 for dr in dag_runs if dr.state == 'running'),
        }
        
        return metrics
    
    metrics = get_metrics()
    print(f"DAG Metrics: {json.dumps(metrics, indent=2)}")
    
    return metrics

def store_metrics(**context):
    """메트릭을 데이터베이스에 저장"""
    ti = context['ti']
    metrics = ti.xcom_pull(task_ids='collect_metrics')
    
    hook = PostgresHook(postgres_conn_id='metrics_db')
    
    hook.run("""
        INSERT INTO airflow_metrics 
        (date, total_runs, success, failed, running, created_at)
        VALUES (%s, %s, %s, %s, %s, NOW())
    """, parameters=(
        context['ds'],
        metrics['total_runs'],
        metrics['success'],
        metrics['failed'],
        metrics['running']
    ))

def check_pipeline_health(**context):
    """파이프라인 헬스 체크"""
    ti = context['ti']
    metrics = ti.xcom_pull(task_ids='collect_metrics')
    
    # 실패율 계산
    total = metrics['total_runs']
    failed = metrics['failed']
    
    if total > 0:
        failure_rate = (failed / total) * 100
        print(f"실패율: {failure_rate:.2f}%")
        
        # 실패율이 10%를 초과하면 알림
        if failure_rate > 10:
            print("⚠️ 경고: 실패율이 10%를 초과했습니다!")
            # 여기서 Slack, Email 등 알림 전송
    
    return metrics

def generate_report(**context):
    """일일 리포트 생성"""
    ti = context['ti']
    metrics = ti.xcom_pull(task_ids='health_check')
    
    report = f"""
    ===== Airflow 일일 리포트 =====
    날짜: {context['ds']}
    
    총 실행 건수: {metrics['total_runs']}
    성공: {metrics['success']}
    실패: {metrics['failed']}
    실행 중: {metrics['running']}
    
    성공률: {(metrics['success'] / metrics['total_runs'] * 100) if metrics['total_runs'] > 0 else 0:.2f}%
    ===============================
    """
    
    print(report)
    return report

with DAG(
    'monitoring_system',
    start_date=datetime(2024, 1, 1),
    schedule_interval='0 23 * * *',  # 매일 오후 11시
    catchup=False,
    default_args={
        'owner': 'airflow',
        'retries': 2,
        'retry_delay': timedelta(minutes=5),
    },
    tags=['monitoring', 'system'],
) as dag:
    
    collect = PythonOperator(
        task_id='collect_metrics',
        python_callable=collect_dag_metrics,
        provide_context=True,
    )
    
    store = PythonOperator(
        task_id='store_metrics',
        python_callable=store_metrics,
        provide_context=True,
    )
    
    health_check = PythonOperator(
        task_id='health_check',
        python_callable=check_pipeline_health,
        provide_context=True,
    )
    
    report = PythonOperator(
        task_id='generate_report',
        python_callable=generate_report,
        provide_context=True,
    )
    
    collect >> [store, health_check] >> report
```

## 참고사항

* 스케줄링을 설정할 때는 시스템 리소스와 데이터 처리 시간을 고려해야 함. 너무 빡빡한 스케줄은 Task가 밀리거나 실패할 수 있음. 또한 모니터링은 실시간으로 하는 게 중요하고, Slack이나 Email 같은 알림 시스템과 연동해서 즉각 대응할 수 있도록 구성하는 게 좋음.